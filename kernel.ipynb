{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "6ab092a614aeaf674ef51af6c23badcfcadbac73"
   },
   "source": [
    "This Notebook is a Sequence-to-Sequence Model for Text Summarization task using Attention Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "_uuid": "850b9874d086f145a3d3dfeaeab4100e24bceb84"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Input, Dropout, LSTM, Activation, Bidirectional, Concatenate, Permute, Dot, Multiply\n",
    "from keras.layers import RepeatVector, Lambda, TimeDistributed\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras import optimizers\n",
    "from keras.utils import to_categorical\n",
    "from keras.initializers import glorot_uniform\n",
    "import matplotlib.pyplot as plt\n",
    "from utils import *\n",
    "import pandas as pd\n",
    "#from nltk.corpus import stopwords\n",
    "from pickle import dump, load\n",
    "import re\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "from keras.utils import plot_model\n",
    "from attention import AttentionDecoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "248397c756765fcdf11ee47cb9844adfc5edfb08"
   },
   "outputs": [],
   "source": [
    "reviews = pd.read_csv(\"dataset/Reviews.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b1346c4221e5c02b619265d9788e9168086a4a30"
   },
   "outputs": [],
   "source": [
    "print(reviews.shape)\n",
    "print(reviews.head())\n",
    "print(reviews.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "78067bc72a29493f7fd7e7a8fa56fe8d4e552f22"
   },
   "outputs": [],
   "source": [
    "reviews = reviews.dropna()\n",
    "reviews = reviews.drop(['Id','ProductId','UserId','ProfileName','HelpfulnessNumerator','HelpfulnessDenominator', 'Score','Time'], 1)\n",
    "reviews = reviews.reset_index(drop=True) \n",
    "print(reviews.head())\n",
    "for i in range(5):\n",
    "    print(\"Review #\",i+1)\n",
    "    print(reviews.Summary[i])\n",
    "    print(reviews.Text[i])\n",
    "    print()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "db0331b9fa896a8bd10d3f2f317779336d63f3f6"
   },
   "outputs": [],
   "source": [
    "contractions = {\"ain't\": \"am not\",\n",
    "                \"aren't\": \"are not\",\n",
    "                \"can't\": \"cannot\",\n",
    "                \"can't've\": \"cannot have\",\n",
    "                \"'cause\": \"because\",\n",
    "                \"could've\": \"could have\",\n",
    "                \"couldn't\": \"could not\",\n",
    "                \"couldn't've\": \"could not have\",\n",
    "                \"didn't\": \"did not\",\n",
    "                \"doesn't\": \"does not\",\n",
    "                \"don't\": \"do not\",\n",
    "                \"hadn't\": \"had not\",\n",
    "                \"hadn't've\": \"had not have\",\n",
    "                \"hasn't\": \"has not\",\n",
    "                \"haven't\": \"have not\",\n",
    "                \"he'd\": \"he would\",\n",
    "                \"he'd've\": \"he would have\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "26f764fa33ad7f5587361a8735dcae98729a73ac"
   },
   "outputs": [],
   "source": [
    "def clean_text(text, remove_stopwords = True):\n",
    "    # Convert words to lower case\n",
    "    text = text.lower()\n",
    "    if True:\n",
    "        text = text.split()\n",
    "        new_text = []\n",
    "        for word in text:\n",
    "            if word in contractions:\n",
    "                new_text.append(contractions[word])\n",
    "            else:\n",
    "                new_text.append(word)\n",
    "        text = \" \".join(new_text)\n",
    "        text = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', text, flags=re.MULTILINE)\n",
    "        text = re.sub(r'\\<a href', ' ', text)\n",
    "        text = re.sub(r'</code>&<code>', '', text)\n",
    "        text = re.sub(r'[_\"\\-;%()|+&=*%!?:#$@\\[\\]/]', ' ', text)\n",
    "        text = re.sub(r'</code><br /><code>', ' ', text)\n",
    "        text = re.sub(r'\\'', ' ', text)\n",
    "        if remove_stopwords:\n",
    "            text = text.split()\n",
    "            stops = set(stopwords.words(\"english\"))\n",
    "            text = [w for w in text if not w in stops]\n",
    "            text = \" \".join(text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "414921f6201e0f11526be71e752ff832ffe8aee4"
   },
   "outputs": [],
   "source": [
    "clean_summaries = []\n",
    "for summary in reviews.Summary:\n",
    "    clean_summaries.append(clean_text(summary, remove_stopwords=False))\n",
    "print(\"Summaries are complete.\")\n",
    "clean_texts = []\n",
    "for text in reviews.Text:\n",
    "    clean_texts.append(clean_text(text , remove_stopwords = False))\n",
    "print(\"Texts are complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2bdf20304debc5eb52457092c02813cba3e49fdd"
   },
   "outputs": [],
   "source": [
    "stories = list()\n",
    "for i, text in enumerate(clean_texts):\n",
    "    stories.append({'story': text, 'highlights': clean_summaries[i]})\n",
    "# save to file\n",
    "dump(stories, open('review_dataset2.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "_uuid": "caccdd6968f4e9acf6a309f848881ad82abb02de"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Stories 568411\n",
      "<class 'list'>\n",
      "{'story': 'if you are looking for the secret ingredient in robitussin i believe i have found it. i got this in addition to the root beer extract i ordered  which was good  and made some cherry soda. the flavor is very medicinal.', 'highlights': 'cough medicine'}\n"
     ]
    }
   ],
   "source": [
    "stories = load(open('review_dataset2.pkl', 'rb'))\n",
    "print('Loaded Stories %d' % len(stories))\n",
    "print(type(stories))\n",
    "print(stories[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "_uuid": "73582ce9a5d32f177ad92225046ddb1f0d65bea0"
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "epochs = 110\n",
    "latent_dim = 256\n",
    "num_samples = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "_uuid": "e5446ec6699d41c3c776ec48ff1c95e430ba1a1f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 50\n",
      "Number of unique input tokens: 41\n",
      "Number of unique output tokens: 31\n",
      "Max sequence length for inputs: 1221\n",
      "Max sequence length for outputs: 62\n",
      "50\n",
      "50\n"
     ]
    }
   ],
   "source": [
    "input_texts = []\n",
    "target_texts = []\n",
    "input_characters = set()\n",
    "target_characters = set()\n",
    "for story in stories[:50]:\n",
    "    input_text = story['story']\n",
    "    target_text = story['highlights']\n",
    "    # We use \"tab\" as the \"start sequence\" character\n",
    "    # for the targets, and \"\\n\" as \"end sequence\" character.\n",
    "    target_text = '\\t' + target_text + '\\n'\n",
    "    input_texts.append(input_text)\n",
    "    target_texts.append(target_text)\n",
    "    for char in input_text:\n",
    "        if char not in input_characters:\n",
    "            input_characters.add(char)\n",
    "    for char in target_text:\n",
    "        if char not in target_characters:\n",
    "            target_characters.add(char)\n",
    "input_characters = sorted(list(input_characters))\n",
    "target_characters = sorted(list(target_characters))\n",
    "num_encoder_tokens = len(input_characters)\n",
    "num_decoder_tokens = len(target_characters)\n",
    "max_encoder_seq_length = max([len(txt) for txt in input_texts])\n",
    "max_decoder_seq_length = max([len(txt) for txt in target_texts])\n",
    "print('Number of samples:', len(input_texts))\n",
    "print('Number of unique input tokens:', num_encoder_tokens)\n",
    "print('Number of unique output tokens:', num_decoder_tokens)\n",
    "print('Max sequence length for inputs:', max_encoder_seq_length)\n",
    "print('Max sequence length for outputs:', max_decoder_seq_length)\n",
    "print(len(input_texts))\n",
    "print(len(target_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "_uuid": "8d1f484667466453bf5152c3613e0c0863e81a14"
   },
   "outputs": [],
   "source": [
    "input_token_index = dict([(char, i) for i, char in enumerate(input_characters)])\n",
    "\n",
    "target_token_index = dict([(char, i) for i, char in enumerate(target_characters)])\n",
    "reverse_target_char_index = dict([(i, char) for i, char in enumerate(target_characters)])\n",
    "\n",
    "\n",
    "encoder_input_data = np.zeros((len(input_texts), max_encoder_seq_length, num_encoder_tokens), dtype='float32')\n",
    "\n",
    "decoder_input_data = np.zeros((len(input_texts), max_decoder_seq_length, num_decoder_tokens), dtype='float32')\n",
    "\n",
    "decoder_target_data = np.zeros((len(input_texts), max_decoder_seq_length, num_decoder_tokens), dtype='float32')\n",
    "\n",
    "\n",
    "\n",
    "for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
    "    for t, char in enumerate(input_text):\n",
    "        encoder_input_data[i, t, input_token_index[char]] = 1.\n",
    "        \n",
    "    for t, char in enumerate(target_text):\n",
    "\n",
    "        # decoder_target_data is ahead of decoder_input_data by one timestep\n",
    "        decoder_input_data[i, t, target_token_index[char]] = 1.\n",
    "        if t > 0:\n",
    "            # decoder_target_data will be ahead by one timestep\n",
    "            # and will not include the start character.\n",
    "            decoder_target_data[i, t - 1, target_token_index[char]] = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 1221, 41)\n",
      "(50, 62, 31)\n",
      "(50, 62, 31)\n"
     ]
    }
   ],
   "source": [
    "print(encoder_input_data.shape)\n",
    "print(decoder_input_data.shape)\n",
    "print(decoder_target_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "repeator = RepeatVector(2297)\n",
    "concatenator = Concatenate(axis=-1)\n",
    "densor1 = Dense(10, activation = \"tanh\")\n",
    "densor2 = Dense(1, activation = \"relu\")\n",
    "activator = Activation(\"softmax\", name='attention_weights') # We are using a custom softmax(axis = 1) loaded in this notebook\n",
    "dotor = Dot(axes = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "_uuid": "ca2a387507631fcbda21570948fc77d10884c773"
   },
   "outputs": [],
   "source": [
    "def Character_Model(num_encoder_tokens, num_decoder_tokens, latent_dim):\n",
    "    # Define an input sequence and process it.\n",
    "\n",
    "    encoder_inputs = Input(shape=(None, num_encoder_tokens), name = \"encoder_input\")\n",
    "    #encoder_output = Bidirectional(LSTM(latent_dim,\n",
    "    #                                        return_sequences=True,\n",
    "    #                                        kernel_initializer = glorot_uniform(seed = 0),\n",
    "    #                                        bias_initializer ='zeros',\n",
    "    #                                        name = \"Bi-LSTM_output\"),\n",
    "    #                              name = \"Bidirectional\")(encoder_inputs)\n",
    "    \n",
    "    encoder_output,state_h, state_c = LSTM(256,\n",
    "                                            return_sequences=True,\n",
    "                                            return_state = True,\n",
    "                                            kernel_initializer = glorot_uniform(seed = 0),\n",
    "                                            bias_initializer ='zeros',\n",
    "                                            name = \"encoder_output\")(encoder_inputs)\n",
    "    \n",
    "    # We discard `encoder_outputs` and only keep the states.\n",
    "    encoder_states = [state_h, state_c]\n",
    "\n",
    "    # Set up the decoder, using `encoder_states` as initial state.\n",
    "    decoder_inputs = Input(shape=(None, num_decoder_tokens), name = \"decoder_input\")\n",
    "\n",
    "    # We set up our decoder to return full output sequences,\n",
    "    # and to return internal states as well. We don't use the\n",
    "    # return states in the training model, but we will use them in inference.1\n",
    "    decoder_outputs,_,_ = LSTM(256,return_sequences=True,return_state=True,\n",
    "                               kernel_initializer = glorot_uniform(seed = 0),\n",
    "                               bias_initializer ='zeros',\n",
    "                               name = \"decoder_LSTM_1\")(decoder_inputs,initial_state=encoder_states)\n",
    "    decoder_outputs = Dropout(0.8)(decoder_outputs)\n",
    "    \n",
    "    decoder_outputs,_,_ = LSTM(256,return_sequences=True,return_state=True,\n",
    "                               kernel_initializer = glorot_uniform(seed = 0),\n",
    "                               bias_initializer ='zeros',\n",
    "                               name = \"decoder_LSTM_2\")(decoder_outputs)\n",
    "    decoder_outputs = Dropout(0.8)(decoder_outputs)\n",
    "    \n",
    "    decoder_outputs = Dense(num_decoder_tokens, activation='softmax',\n",
    "    #                        kernel_initializer= glorot_uniform(seed = 0),\n",
    "    #                        bias_initializer='zeros',\n",
    "                            name = \"decoder_Dense_Output\")(decoder_outputs)\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    # Define the model that will turn\n",
    "    # `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
    "    model = Model(inputs = [encoder_inputs, decoder_inputs], outputs = decoder_outputs)\n",
    "    \n",
    "    \n",
    "    # Encoder Model for Inference\n",
    "    encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "    \n",
    "    # Decoder Model for Inference\n",
    "    decoder_state_input_h = Input(shape=(latent_dim,), name = \"Inference_decoder_input_hidden_state\")\n",
    "    decoder_state_input_c = Input(shape=(latent_dim,), name = \"Inference_decoder_input_cell_state\")\n",
    "    decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "    \n",
    "    decoder_outputs_1, state_h, state_c = LSTM(latent_dim,\n",
    "                                               return_sequences=True, \n",
    "                                               return_state=True,\n",
    "                                               name = \"Inference_decoder_LSTM\")(decoder_inputs, initial_state=decoder_states_inputs)\n",
    "    \n",
    "    decoder_states = [state_h, state_c]\n",
    "    \n",
    "    decoder_outputs_1 = Dense(num_decoder_tokens, \n",
    "                              activation='softmax',\n",
    "                              kernel_initializer= glorot_uniform(seed = 0),\n",
    "                              bias_initializer='zeros',\n",
    "                              name = \"decoder_Dense_Output\")(decoder_outputs_1)\n",
    "    decoder_model = Model([decoder_inputs] + decoder_states_inputs,\n",
    "                          [decoder_outputs_1] + decoder_states)\n",
    "\n",
    "    return model, encoder_model, decoder_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "encoder_input (InputLayer)      (None, None, 41)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "decoder_input (InputLayer)      (None, None, 31)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "encoder_output (LSTM)           [(None, None, 256),  305152      encoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "decoder_LSTM_1 (LSTM)           [(None, None, 256),  294912      decoder_input[0][0]              \n",
      "                                                                 encoder_output[0][1]             \n",
      "                                                                 encoder_output[0][2]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_31 (Dropout)            (None, None, 256)    0           decoder_LSTM_1[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "decoder_LSTM_2 (LSTM)           [(None, None, 256),  525312      dropout_31[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_32 (Dropout)            (None, None, 256)    0           decoder_LSTM_2[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "decoder_Dense_Output (Dense)    (None, None, 31)     7967        dropout_32[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 1,133,343\n",
      "Trainable params: 1,133,343\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"483pt\" viewBox=\"0.00 0.00 347.50 483.00\" width=\"348pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g class=\"graph\" id=\"graph0\" transform=\"scale(1 1) rotate(0) translate(4 479)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"white\" points=\"-4,4 -4,-479 343.5,-479 343.5,4 -4,4\" stroke=\"none\"/>\n",
       "<!-- 1711956275152 -->\n",
       "<g class=\"node\" id=\"node1\"><title>1711956275152</title>\n",
       "<polygon fill=\"none\" points=\"0,-438.5 0,-474.5 164,-474.5 164,-438.5 0,-438.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"82\" y=\"-452.8\">encoder_input: InputLayer</text>\n",
       "</g>\n",
       "<!-- 1711956274200 -->\n",
       "<g class=\"node\" id=\"node3\"><title>1711956274200</title>\n",
       "<polygon fill=\"none\" points=\"7.5,-365.5 7.5,-401.5 156.5,-401.5 156.5,-365.5 7.5,-365.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"82\" y=\"-379.8\">encoder_output: LSTM</text>\n",
       "</g>\n",
       "<!-- 1711956275152&#45;&gt;1711956274200 -->\n",
       "<g class=\"edge\" id=\"edge1\"><title>1711956275152-&gt;1711956274200</title>\n",
       "<path d=\"M82,-438.313C82,-430.289 82,-420.547 82,-411.569\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"85.5001,-411.529 82,-401.529 78.5001,-411.529 85.5001,-411.529\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 1711956273640 -->\n",
       "<g class=\"node\" id=\"node2\"><title>1711956273640</title>\n",
       "<polygon fill=\"none\" points=\"174.5,-365.5 174.5,-401.5 339.5,-401.5 339.5,-365.5 174.5,-365.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"257\" y=\"-379.8\">decoder_input: InputLayer</text>\n",
       "</g>\n",
       "<!-- 1711958693592 -->\n",
       "<g class=\"node\" id=\"node4\"><title>1711958693592</title>\n",
       "<polygon fill=\"none\" points=\"86,-292.5 86,-328.5 252,-328.5 252,-292.5 86,-292.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"169\" y=\"-306.8\">decoder_LSTM_1: LSTM</text>\n",
       "</g>\n",
       "<!-- 1711956273640&#45;&gt;1711958693592 -->\n",
       "<g class=\"edge\" id=\"edge2\"><title>1711956273640-&gt;1711958693592</title>\n",
       "<path d=\"M235.698,-365.313C224.348,-356.156 210.224,-344.76 197.899,-334.816\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"200.087,-332.084 190.106,-328.529 195.691,-337.532 200.087,-332.084\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 1711956274200&#45;&gt;1711958693592 -->\n",
       "<g class=\"edge\" id=\"edge3\"><title>1711956274200-&gt;1711958693592</title>\n",
       "<path d=\"M103.06,-365.313C114.174,-356.243 127.979,-344.977 140.081,-335.1\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"142.599,-337.563 148.134,-328.529 138.173,-332.14 142.599,-337.563\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 1711958298576 -->\n",
       "<g class=\"node\" id=\"node5\"><title>1711958298576</title>\n",
       "<polygon fill=\"none\" points=\"100.5,-219.5 100.5,-255.5 237.5,-255.5 237.5,-219.5 100.5,-219.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"169\" y=\"-233.8\">dropout_31: Dropout</text>\n",
       "</g>\n",
       "<!-- 1711958693592&#45;&gt;1711958298576 -->\n",
       "<g class=\"edge\" id=\"edge5\"><title>1711958693592-&gt;1711958298576</title>\n",
       "<path d=\"M169,-292.313C169,-284.289 169,-274.547 169,-265.569\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"172.5,-265.529 169,-255.529 165.5,-265.529 172.5,-265.529\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 1711958436384 -->\n",
       "<g class=\"node\" id=\"node6\"><title>1711958436384</title>\n",
       "<polygon fill=\"none\" points=\"86,-146.5 86,-182.5 252,-182.5 252,-146.5 86,-146.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"169\" y=\"-160.8\">decoder_LSTM_2: LSTM</text>\n",
       "</g>\n",
       "<!-- 1711958298576&#45;&gt;1711958436384 -->\n",
       "<g class=\"edge\" id=\"edge6\"><title>1711958298576-&gt;1711958436384</title>\n",
       "<path d=\"M169,-219.313C169,-211.289 169,-201.547 169,-192.569\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"172.5,-192.529 169,-182.529 165.5,-192.529 172.5,-192.529\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 1711964163880 -->\n",
       "<g class=\"node\" id=\"node7\"><title>1711964163880</title>\n",
       "<polygon fill=\"none\" points=\"100.5,-73.5 100.5,-109.5 237.5,-109.5 237.5,-73.5 100.5,-73.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"169\" y=\"-87.8\">dropout_32: Dropout</text>\n",
       "</g>\n",
       "<!-- 1711958436384&#45;&gt;1711964163880 -->\n",
       "<g class=\"edge\" id=\"edge7\"><title>1711958436384-&gt;1711964163880</title>\n",
       "<path d=\"M169,-146.313C169,-138.289 169,-128.547 169,-119.569\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"172.5,-119.529 169,-109.529 165.5,-119.529 172.5,-119.529\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 1711964161304 -->\n",
       "<g class=\"node\" id=\"node8\"><title>1711964161304</title>\n",
       "<polygon fill=\"none\" points=\"73,-0.5 73,-36.5 265,-36.5 265,-0.5 73,-0.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"169\" y=\"-14.8\">decoder_Dense_Output: Dense</text>\n",
       "</g>\n",
       "<!-- 1711964163880&#45;&gt;1711964161304 -->\n",
       "<g class=\"edge\" id=\"edge8\"><title>1711964163880-&gt;1711964161304</title>\n",
       "<path d=\"M169,-73.3129C169,-65.2895 169,-55.5475 169,-46.5691\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"172.5,-46.5288 169,-36.5288 165.5,-46.5289 172.5,-46.5288\" stroke=\"black\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model, encoder_model, decoder_model = Character_Model(num_encoder_tokens, num_decoder_tokens, latent_dim)\n",
    "\n",
    "rmsprop = optimizers.RMSprop(lr=0.002)\n",
    "\n",
    "# Compile Model\n",
    "model.compile(optimizer=rmsprop, loss='categorical_crossentropy', metrics = ['accuracy'])\n",
    "# Print Model Summary\n",
    "print(model.summary())\n",
    "plot_model(model, to_file='Models/Character_Level_Model.png')\n",
    "SVG(model_to_dot(model).create(prog='dot', format='svg'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Encoder Model\n"
     ]
    },
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"118pt\" viewBox=\"0.00 0.00 172.00 118.00\" width=\"172pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g class=\"graph\" id=\"graph0\" transform=\"scale(1 1) rotate(0) translate(4 114)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"white\" points=\"-4,4 -4,-114 168,-114 168,4 -4,4\" stroke=\"none\"/>\n",
       "<!-- 1707971282200 -->\n",
       "<g class=\"node\" id=\"node1\"><title>1707971282200</title>\n",
       "<polygon fill=\"none\" points=\"0,-73.5 0,-109.5 164,-109.5 164,-73.5 0,-73.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"82\" y=\"-87.8\">encoder_input: InputLayer</text>\n",
       "</g>\n",
       "<!-- 1707971257288 -->\n",
       "<g class=\"node\" id=\"node2\"><title>1707971257288</title>\n",
       "<polygon fill=\"none\" points=\"7.5,-0.5 7.5,-36.5 156.5,-36.5 156.5,-0.5 7.5,-0.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"82\" y=\"-14.8\">encoder_output: LSTM</text>\n",
       "</g>\n",
       "<!-- 1707971282200&#45;&gt;1707971257288 -->\n",
       "<g class=\"edge\" id=\"edge1\"><title>1707971282200-&gt;1707971257288</title>\n",
       "<path d=\"M82,-73.3129C82,-65.2895 82,-55.5475 82,-46.5691\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"85.5001,-46.5288 82,-36.5288 78.5001,-46.5289 85.5001,-46.5288\" stroke=\"black\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print()\n",
    "print(\"Encoder Model\")\n",
    "plot_model(encoder_model, to_file='Models/Inference_Encoder_Character_Level_Model.png')\n",
    "SVG(model_to_dot(encoder_model).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Decoder Model\n"
     ]
    },
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"191pt\" viewBox=\"0.00 0.00 795.50 191.00\" width=\"796pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g class=\"graph\" id=\"graph0\" transform=\"scale(1 1) rotate(0) translate(4 187)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"white\" points=\"-4,4 -4,-187 791.5,-187 791.5,4 -4,4\" stroke=\"none\"/>\n",
       "<!-- 1707971257568 -->\n",
       "<g class=\"node\" id=\"node1\"><title>1707971257568</title>\n",
       "<polygon fill=\"none\" points=\"0,-146.5 0,-182.5 165,-182.5 165,-146.5 0,-146.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"82.5\" y=\"-160.8\">decoder_input: InputLayer</text>\n",
       "</g>\n",
       "<!-- 1707938444736 -->\n",
       "<g class=\"node\" id=\"node4\"><title>1707938444736</title>\n",
       "<polygon fill=\"none\" points=\"229,-73.5 229,-109.5 440,-109.5 440,-73.5 229,-73.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"334.5\" y=\"-87.8\">Inference_decoder_LSTM: LSTM</text>\n",
       "</g>\n",
       "<!-- 1707971257568&#45;&gt;1707938444736 -->\n",
       "<g class=\"edge\" id=\"edge1\"><title>1707971257568-&gt;1707938444736</title>\n",
       "<path d=\"M142.86,-146.494C179.217,-136.25 225.69,-123.157 263.766,-112.429\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"265.083,-115.694 273.759,-109.614 263.184,-108.957 265.083,-115.694\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 1712455688320 -->\n",
       "<g class=\"node\" id=\"node2\"><title>1712455688320</title>\n",
       "<polygon fill=\"none\" points=\"183.5,-146.5 183.5,-182.5 485.5,-182.5 485.5,-146.5 183.5,-146.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"334.5\" y=\"-160.8\">Inference_decoder_input_hidden_state: InputLayer</text>\n",
       "</g>\n",
       "<!-- 1712455688320&#45;&gt;1707938444736 -->\n",
       "<g class=\"edge\" id=\"edge2\"><title>1712455688320-&gt;1707938444736</title>\n",
       "<path d=\"M334.5,-146.313C334.5,-138.289 334.5,-128.547 334.5,-119.569\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"338,-119.529 334.5,-109.529 331,-119.529 338,-119.529\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 1707938446976 -->\n",
       "<g class=\"node\" id=\"node3\"><title>1707938446976</title>\n",
       "<polygon fill=\"none\" points=\"503.5,-146.5 503.5,-182.5 787.5,-182.5 787.5,-146.5 503.5,-146.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"645.5\" y=\"-160.8\">Inference_decoder_input_cell_state: InputLayer</text>\n",
       "</g>\n",
       "<!-- 1707938446976&#45;&gt;1707938444736 -->\n",
       "<g class=\"edge\" id=\"edge3\"><title>1707938446976-&gt;1707938444736</title>\n",
       "<path d=\"M571.008,-146.494C525.291,-136.057 466.614,-122.661 419.143,-111.824\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"419.721,-108.366 409.192,-109.552 418.163,-115.19 419.721,-108.366\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 1707938444064 -->\n",
       "<g class=\"node\" id=\"node5\"><title>1707938444064</title>\n",
       "<polygon fill=\"none\" points=\"238.5,-0.5 238.5,-36.5 430.5,-36.5 430.5,-0.5 238.5,-0.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"334.5\" y=\"-14.8\">decoder_Dense_Output: Dense</text>\n",
       "</g>\n",
       "<!-- 1707938444736&#45;&gt;1707938444064 -->\n",
       "<g class=\"edge\" id=\"edge4\"><title>1707938444736-&gt;1707938444064</title>\n",
       "<path d=\"M334.5,-73.3129C334.5,-65.2895 334.5,-55.5475 334.5,-46.5691\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"338,-46.5288 334.5,-36.5288 331,-46.5289 338,-46.5288\" stroke=\"black\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print()\n",
    "print(\"Decoder Model\")\n",
    "plot_model(decoder_model, to_file='Models/Inference_Decoder_Character_Level_Model.png')\n",
    "SVG(model_to_dot(decoder_model).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 49 samples, validate on 1 samples\n",
      "Epoch 1/50\n",
      "49/49 [==============================] - ETA: 29s - loss: 1.2881 - acc: 0.04 - ETA: 8s - loss: 1.3358 - acc: 0.0479 - ETA: 0s - loss: 1.2914 - acc: 0.048 - 24s 495ms/step - loss: 1.2762 - acc: 0.0477 - val_loss: 0.6025 - val_acc: 0.0161\n",
      "Epoch 2/50\n",
      "49/49 [==============================] - ETA: 4s - loss: 1.4751 - acc: 0.028 - ETA: 2s - loss: 1.2125 - acc: 0.025 - ETA: 0s - loss: 1.1971 - acc: 0.023 - 11s 220ms/step - loss: 1.2054 - acc: 0.0253 - val_loss: 0.5473 - val_acc: 0.0161\n",
      "Epoch 3/50\n",
      "49/49 [==============================] - ETA: 5s - loss: 1.0336 - acc: 0.032 - ETA: 2s - loss: 1.1696 - acc: 0.028 - ETA: 0s - loss: 1.1656 - acc: 0.028 - 11s 222ms/step - loss: 1.1733 - acc: 0.0293 - val_loss: 0.5780 - val_acc: 0.0161\n",
      "Epoch 4/50\n",
      "49/49 [==============================] - ETA: 4s - loss: 1.0708 - acc: 0.041 - ETA: 2s - loss: 1.0365 - acc: 0.041 - ETA: 0s - loss: 1.1209 - acc: 0.047 - 11s 217ms/step - loss: 1.1413 - acc: 0.0477 - val_loss: 0.5448 - val_acc: 0.0161\n",
      "Epoch 5/50\n",
      "49/49 [==============================] - ETA: 4s - loss: 0.9014 - acc: 0.037 - ETA: 2s - loss: 1.1062 - acc: 0.049 - ETA: 0s - loss: 1.1416 - acc: 0.053 - 11s 221ms/step - loss: 1.1271 - acc: 0.0523 - val_loss: 0.5514 - val_acc: 0.0161\n",
      "Epoch 6/50\n",
      "49/49 [==============================] - ETA: 4s - loss: 1.1262 - acc: 0.044 - ETA: 2s - loss: 1.0938 - acc: 0.040 - ETA: 0s - loss: 1.1399 - acc: 0.045 - 11s 217ms/step - loss: 1.1288 - acc: 0.0444 - val_loss: 0.5399 - val_acc: 0.0161\n",
      "Epoch 7/50\n",
      "49/49 [==============================] - ETA: 4s - loss: 1.0088 - acc: 0.036 - ETA: 2s - loss: 1.0840 - acc: 0.040 - ETA: 0s - loss: 1.1231 - acc: 0.046 - 11s 221ms/step - loss: 1.1266 - acc: 0.0461 - val_loss: 0.5146 - val_acc: 0.0161\n",
      "Epoch 8/50\n",
      "49/49 [==============================] - ETA: 4s - loss: 1.3236 - acc: 0.047 - ETA: 2s - loss: 1.1379 - acc: 0.040 - ETA: 0s - loss: 1.1369 - acc: 0.048 - 11s 222ms/step - loss: 1.1236 - acc: 0.0484 - val_loss: 0.5272 - val_acc: 0.0161\n",
      "Epoch 9/50\n",
      "49/49 [==============================] - ETA: 4s - loss: 1.1650 - acc: 0.051 - ETA: 2s - loss: 1.1556 - acc: 0.048 - ETA: 0s - loss: 1.1279 - acc: 0.047 - 12s 236ms/step - loss: 1.1217 - acc: 0.0471 - val_loss: 0.5323 - val_acc: 0.0161\n",
      "Epoch 10/50\n",
      "49/49 [==============================] - ETA: 4s - loss: 1.0742 - acc: 0.054 - ETA: 2s - loss: 1.1550 - acc: 0.059 - ETA: 0s - loss: 1.1090 - acc: 0.052 - 11s 223ms/step - loss: 1.1029 - acc: 0.0527 - val_loss: 0.5403 - val_acc: 0.0161\n",
      "Epoch 11/50\n",
      "49/49 [==============================] - ETA: 4s - loss: 1.0266 - acc: 0.046 - ETA: 2s - loss: 1.0776 - acc: 0.047 - ETA: 0s - loss: 1.1256 - acc: 0.047 - 12s 243ms/step - loss: 1.1141 - acc: 0.0471 - val_loss: 0.5110 - val_acc: 0.0161\n",
      "Epoch 12/50\n",
      "49/49 [==============================] - ETA: 5s - loss: 0.9379 - acc: 0.038 - ETA: 2s - loss: 1.0834 - acc: 0.050 - ETA: 0s - loss: 1.1170 - acc: 0.053 - 12s 247ms/step - loss: 1.1047 - acc: 0.0530 - val_loss: 0.5100 - val_acc: 0.0161\n",
      "Epoch 13/50\n",
      "49/49 [==============================] - ETA: 4s - loss: 0.9767 - acc: 0.034 - ETA: 2s - loss: 1.0933 - acc: 0.042 - ETA: 0s - loss: 1.1024 - acc: 0.044 - 11s 230ms/step - loss: 1.1081 - acc: 0.0451 - val_loss: 0.5334 - val_acc: 0.0161\n",
      "Epoch 14/50\n",
      "49/49 [==============================] - ETA: 4s - loss: 0.9426 - acc: 0.041 - ETA: 2s - loss: 1.1191 - acc: 0.046 - ETA: 0s - loss: 1.1078 - acc: 0.051 - 12s 241ms/step - loss: 1.1033 - acc: 0.0510 - val_loss: 0.5465 - val_acc: 0.0161\n",
      "Epoch 15/50\n",
      "49/49 [==============================] - ETA: 4s - loss: 1.4743 - acc: 0.078 - ETA: 2s - loss: 1.1831 - acc: 0.060 - ETA: 0s - loss: 1.1062 - acc: 0.053 - 11s 221ms/step - loss: 1.0951 - acc: 0.0530 - val_loss: 0.5248 - val_acc: 0.0161\n",
      "Epoch 16/50\n",
      "49/49 [==============================] - ETA: 4s - loss: 1.0569 - acc: 0.051 - ETA: 2s - loss: 1.0602 - acc: 0.047 - ETA: 0s - loss: 1.0834 - acc: 0.056 - 11s 225ms/step - loss: 1.0892 - acc: 0.0579 - val_loss: 0.5382 - val_acc: 0.0161\n",
      "Epoch 17/50\n",
      "49/49 [==============================] - ETA: 4s - loss: 1.3055 - acc: 0.056 - ETA: 2s - loss: 1.1297 - acc: 0.049 - ETA: 0s - loss: 1.1143 - acc: 0.050 - 11s 220ms/step - loss: 1.1108 - acc: 0.0500 - val_loss: 0.5390 - val_acc: 0.0161\n",
      "Epoch 18/50\n",
      "49/49 [==============================] - ETA: 4s - loss: 0.9925 - acc: 0.055 - ETA: 2s - loss: 1.0367 - acc: 0.054 - ETA: 0s - loss: 1.0909 - acc: 0.058 - 11s 220ms/step - loss: 1.0914 - acc: 0.0586 - val_loss: 0.5486 - val_acc: 0.0323\n",
      "Epoch 19/50\n",
      "49/49 [==============================] - ETA: 4s - loss: 1.0909 - acc: 0.052 - ETA: 2s - loss: 1.0347 - acc: 0.046 - ETA: 0s - loss: 1.0947 - acc: 0.051 - 11s 222ms/step - loss: 1.0983 - acc: 0.0517 - val_loss: 0.5048 - val_acc: 0.0161\n",
      "Epoch 20/50\n",
      "49/49 [==============================] - ETA: 4s - loss: 1.2389 - acc: 0.052 - ETA: 2s - loss: 1.0432 - acc: 0.049 - ETA: 0s - loss: 1.0863 - acc: 0.052 - 11s 223ms/step - loss: 1.0943 - acc: 0.0530 - val_loss: 0.5433 - val_acc: 0.0323\n",
      "Epoch 21/50\n",
      "49/49 [==============================] - ETA: 4s - loss: 0.9181 - acc: 0.046 - ETA: 2s - loss: 1.0647 - acc: 0.049 - ETA: 0s - loss: 1.0774 - acc: 0.052 - 11s 222ms/step - loss: 1.0892 - acc: 0.0543 - val_loss: 0.5363 - val_acc: 0.0161\n",
      "Epoch 22/50\n",
      "49/49 [==============================] - ETA: 4s - loss: 1.1195 - acc: 0.057 - ETA: 2s - loss: 1.0365 - acc: 0.052 - ETA: 0s - loss: 1.0751 - acc: 0.053 - 11s 221ms/step - loss: 1.0986 - acc: 0.0543 - val_loss: 0.5085 - val_acc: 0.0161\n",
      "Epoch 23/50\n",
      "49/49 [==============================] - ETA: 4s - loss: 1.0801 - acc: 0.070 - ETA: 2s - loss: 0.9278 - acc: 0.052 - ETA: 0s - loss: 1.0742 - acc: 0.059 - 11s 222ms/step - loss: 1.0832 - acc: 0.0606 - val_loss: 0.5263 - val_acc: 0.0161\n",
      "Epoch 24/50\n",
      "49/49 [==============================] - ETA: 4s - loss: 0.8934 - acc: 0.034 - ETA: 2s - loss: 1.1174 - acc: 0.055 - ETA: 0s - loss: 1.0997 - acc: 0.055 - 11s 226ms/step - loss: 1.0884 - acc: 0.0540 - val_loss: 0.5390 - val_acc: 0.0161\n",
      "Epoch 25/50\n",
      "49/49 [==============================] - ETA: 4s - loss: 1.0350 - acc: 0.054 - ETA: 2s - loss: 1.1272 - acc: 0.064 - ETA: 0s - loss: 1.0879 - acc: 0.059 - 11s 220ms/step - loss: 1.0759 - acc: 0.0592 - val_loss: 0.5137 - val_acc: 0.0161\n",
      "Epoch 26/50\n",
      "49/49 [==============================] - ETA: 4s - loss: 1.0908 - acc: 0.055 - ETA: 2s - loss: 1.1058 - acc: 0.050 - ETA: 0s - loss: 1.0787 - acc: 0.053 - 11s 221ms/step - loss: 1.0745 - acc: 0.0537 - val_loss: 0.5332 - val_acc: 0.0161\n",
      "Epoch 27/50\n",
      "49/49 [==============================] - ETA: 4s - loss: 0.9840 - acc: 0.053 - ETA: 2s - loss: 1.0484 - acc: 0.060 - ETA: 0s - loss: 1.0828 - acc: 0.059 - 11s 218ms/step - loss: 1.0779 - acc: 0.0586 - val_loss: 0.5346 - val_acc: 0.0161\n",
      "Epoch 28/50\n",
      "49/49 [==============================] - ETA: 4s - loss: 1.1815 - acc: 0.067 - ETA: 2s - loss: 1.0579 - acc: 0.060 - ETA: 0s - loss: 1.0810 - acc: 0.061 - 11s 221ms/step - loss: 1.0762 - acc: 0.0619 - val_loss: 0.5285 - val_acc: 0.0161\n",
      "Epoch 29/50\n",
      "49/49 [==============================] - ETA: 4s - loss: 0.9884 - acc: 0.045 - ETA: 2s - loss: 1.0768 - acc: 0.060 - ETA: 0s - loss: 1.0671 - acc: 0.063 - 12s 244ms/step - loss: 1.0624 - acc: 0.0625 - val_loss: 0.5352 - val_acc: 0.0323\n",
      "Epoch 30/50\n",
      "49/49 [==============================] - ETA: 4s - loss: 1.0740 - acc: 0.059 - ETA: 2s - loss: 1.0431 - acc: 0.064 - ETA: 0s - loss: 1.0552 - acc: 0.065 - 11s 222ms/step - loss: 1.0592 - acc: 0.0652 - val_loss: 0.5206 - val_acc: 0.0323\n",
      "Epoch 31/50\n",
      "49/49 [==============================] - ETA: 4s - loss: 0.9732 - acc: 0.052 - ETA: 2s - loss: 1.0529 - acc: 0.055 - ETA: 0s - loss: 1.0284 - acc: 0.056 - 11s 222ms/step - loss: 1.0642 - acc: 0.0592 - val_loss: 0.5478 - val_acc: 0.0161\n",
      "Epoch 32/50\n",
      "49/49 [==============================] - ETA: 4s - loss: 1.1404 - acc: 0.054 - ETA: 2s - loss: 1.1042 - acc: 0.056 - ETA: 0s - loss: 1.0621 - acc: 0.055 - 11s 222ms/step - loss: 1.0681 - acc: 0.0609 - val_loss: 0.5417 - val_acc: 0.0323\n",
      "Epoch 33/50\n",
      "49/49 [==============================] - ETA: 4s - loss: 1.0428 - acc: 0.089 - ETA: 2s - loss: 1.0484 - acc: 0.206 - ETA: 0s - loss: 1.4832 - acc: 0.286 - 11s 221ms/step - loss: 1.4632 - acc: 0.2808 - val_loss: 0.5564 - val_acc: 0.0161\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34/50\n",
      "49/49 [==============================] - ETA: 4s - loss: 1.0487 - acc: 0.054 - ETA: 2s - loss: 1.0187 - acc: 0.049 - ETA: 0s - loss: 1.0711 - acc: 0.055 - 11s 221ms/step - loss: 1.0767 - acc: 0.0546 - val_loss: 0.5417 - val_acc: 0.0161\n",
      "Epoch 35/50\n",
      "49/49 [==============================] - ETA: 5s - loss: 1.0837 - acc: 0.065 - ETA: 2s - loss: 1.0189 - acc: 0.055 - ETA: 0s - loss: 1.0500 - acc: 0.057 - 11s 227ms/step - loss: 1.0628 - acc: 0.0586 - val_loss: 0.5335 - val_acc: 0.0161\n",
      "Epoch 36/50\n",
      "49/49 [==============================] - ETA: 4s - loss: 1.1391 - acc: 0.060 - ETA: 2s - loss: 1.1144 - acc: 0.063 - ETA: 0s - loss: 1.0679 - acc: 0.062 - 11s 219ms/step - loss: 1.0574 - acc: 0.0619 - val_loss: 0.5201 - val_acc: 0.0161\n",
      "Epoch 37/50\n",
      "49/49 [==============================] - ETA: 4s - loss: 0.9879 - acc: 0.060 - ETA: 2s - loss: 1.0017 - acc: 0.061 - ETA: 0s - loss: 1.0648 - acc: 0.064 - 11s 221ms/step - loss: 1.0532 - acc: 0.0639 - val_loss: 0.5117 - val_acc: 0.0161\n",
      "Epoch 38/50\n",
      "49/49 [==============================] - ETA: 4s - loss: 0.8410 - acc: 0.050 - ETA: 2s - loss: 1.0208 - acc: 0.061 - ETA: 0s - loss: 1.0597 - acc: 0.064 - 11s 221ms/step - loss: 1.0549 - acc: 0.0642 - val_loss: 0.5142 - val_acc: 0.0161\n",
      "Epoch 39/50\n",
      "49/49 [==============================] - ETA: 4s - loss: 1.1174 - acc: 0.075 - ETA: 2s - loss: 1.1721 - acc: 0.074 - ETA: 0s - loss: 1.0572 - acc: 0.063 - 11s 217ms/step - loss: 1.0520 - acc: 0.0629 - val_loss: 0.5093 - val_acc: 0.0161\n",
      "Epoch 40/50\n",
      "49/49 [==============================] - ETA: 4s - loss: 1.1389 - acc: 0.072 - ETA: 2s - loss: 1.0473 - acc: 0.066 - ETA: 0s - loss: 1.0452 - acc: 0.069 - 11s 220ms/step - loss: 1.0465 - acc: 0.0688 - val_loss: 0.5223 - val_acc: 0.0161\n",
      "Epoch 41/50\n",
      "49/49 [==============================] - ETA: 4s - loss: 0.9943 - acc: 0.068 - ETA: 2s - loss: 1.0383 - acc: 0.066 - ETA: 0s - loss: 1.0541 - acc: 0.066 - 11s 224ms/step - loss: 1.0423 - acc: 0.0655 - val_loss: 0.5189 - val_acc: 0.0323\n",
      "Epoch 42/50\n",
      "49/49 [==============================] - ETA: 4s - loss: 1.3665 - acc: 0.091 - ETA: 2s - loss: 1.1787 - acc: 0.079 - ETA: 0s - loss: 1.0538 - acc: 0.067 - 11s 220ms/step - loss: 1.0438 - acc: 0.0658 - val_loss: 0.5111 - val_acc: 0.0161\n",
      "Epoch 43/50\n",
      "49/49 [==============================] - ETA: 4s - loss: 1.0150 - acc: 0.070 - ETA: 2s - loss: 1.0829 - acc: 0.075 - ETA: 0s - loss: 1.0376 - acc: 0.070 - 11s 219ms/step - loss: 1.0362 - acc: 0.0698 - val_loss: 0.5045 - val_acc: 0.0161\n",
      "Epoch 44/50\n",
      "49/49 [==============================] - ETA: 4s - loss: 1.1806 - acc: 0.074 - ETA: 2s - loss: 1.0614 - acc: 0.067 - ETA: 0s - loss: 1.0457 - acc: 0.068 - 11s 219ms/step - loss: 1.0447 - acc: 0.0685 - val_loss: 0.5218 - val_acc: 0.0323\n",
      "Epoch 45/50\n",
      "49/49 [==============================] - ETA: 4s - loss: 0.9285 - acc: 0.062 - ETA: 2s - loss: 1.0807 - acc: 0.078 - ETA: 0s - loss: 1.0455 - acc: 0.073 - 11s 222ms/step - loss: 1.0340 - acc: 0.0724 - val_loss: 0.5346 - val_acc: 0.0323\n",
      "Epoch 46/50\n",
      "49/49 [==============================] - ETA: 4s - loss: 0.9741 - acc: 0.065 - ETA: 2s - loss: 0.9441 - acc: 0.064 - ETA: 0s - loss: 1.0502 - acc: 0.072 - 11s 221ms/step - loss: 1.0372 - acc: 0.0718 - val_loss: 0.5248 - val_acc: 0.0323\n",
      "Epoch 47/50\n",
      "49/49 [==============================] - ETA: 4s - loss: 0.8363 - acc: 0.066 - ETA: 2s - loss: 0.9710 - acc: 0.076 - ETA: 0s - loss: 1.0036 - acc: 0.073 - 11s 222ms/step - loss: 1.0200 - acc: 0.0750 - val_loss: 0.5191 - val_acc: 0.0161\n",
      "Epoch 48/50\n",
      "49/49 [==============================] - ETA: 4s - loss: 1.0138 - acc: 0.064 - ETA: 2s - loss: 1.0448 - acc: 0.066 - ETA: 0s - loss: 1.0465 - acc: 0.066 - 11s 227ms/step - loss: 1.0404 - acc: 0.0655 - val_loss: 0.5186 - val_acc: 0.0161\n",
      "Epoch 49/50\n",
      "49/49 [==============================] - ETA: 4s - loss: 0.8883 - acc: 0.060 - ETA: 2s - loss: 1.0043 - acc: 0.067 - ETA: 0s - loss: 1.0292 - acc: 0.070 - 11s 220ms/step - loss: 1.0245 - acc: 0.0704 - val_loss: 0.5292 - val_acc: 0.0323\n",
      "Epoch 50/50\n",
      "49/49 [==============================] - ETA: 4s - loss: 1.0596 - acc: 0.079 - ETA: 2s - loss: 1.0627 - acc: 0.083 - ETA: 0s - loss: 1.0290 - acc: 0.078 - 11s 224ms/step - loss: 1.0161 - acc: 0.0767 - val_loss: 0.5341 - val_acc: 0.0323\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python36\\lib\\site-packages\\keras\\engine\\network.py:888: UserWarning: Layer decoder_LSTM_1 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'encoder_output_33/while/Exit_2:0' shape=(?, 256) dtype=float32>, <tf.Tensor 'encoder_output_33/while/Exit_3:0' shape=(?, 256) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n"
     ]
    }
   ],
   "source": [
    "history = model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
    "                    batch_size=16,\n",
    "                    epochs=50,\n",
    "                    validation_split=0.01)\n",
    "\n",
    "# Save model\n",
    "model.save('s2s.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Do you want to run the Model again for 1 epoch more? (1 - Yes | 0 - No) : 1\n",
      "Train on 40 samples, validate on 10 samples\n",
      "Epoch 1/100\n",
      "40/40 [==============================] - ETA: 4s - loss: 1.2780 - acc: 0.060 - ETA: 1s - loss: 1.1554 - acc: 0.052 - 22s 543ms/step - loss: 1.0949 - acc: 0.0500 - val_loss: 1.1057 - val_acc: 0.0565\n",
      "Epoch 2/100\n",
      "40/40 [==============================] - ETA: 4s - loss: 1.1453 - acc: 0.052 - ETA: 1s - loss: 1.1272 - acc: 0.052 - 21s 526ms/step - loss: 1.0781 - acc: 0.0484 - val_loss: 1.1000 - val_acc: 0.0597\n",
      "Epoch 3/100\n",
      "40/40 [==============================] - ETA: 4s - loss: 1.0062 - acc: 0.048 - ETA: 1s - loss: 1.0850 - acc: 0.050 - 21s 514ms/step - loss: 1.0817 - acc: 0.0516 - val_loss: 1.1030 - val_acc: 0.0597\n",
      "Epoch 4/100\n",
      "40/40 [==============================] - ETA: 4s - loss: 1.1486 - acc: 0.056 - ETA: 1s - loss: 1.1219 - acc: 0.052 - 21s 524ms/step - loss: 1.0769 - acc: 0.0512 - val_loss: 1.0993 - val_acc: 0.0613\n",
      "Epoch 5/100\n",
      "40/40 [==============================] - ETA: 4s - loss: 1.0720 - acc: 0.059 - ETA: 1s - loss: 1.1125 - acc: 0.057 - 23s 577ms/step - loss: 1.0713 - acc: 0.0532 - val_loss: 1.0972 - val_acc: 0.0613\n",
      "Epoch 6/100\n",
      "40/40 [==============================] - ETA: 5s - loss: 1.0198 - acc: 0.053 - ETA: 1s - loss: 1.0940 - acc: 0.048 - 23s 579ms/step - loss: 1.0752 - acc: 0.0468 - val_loss: 1.1133 - val_acc: 0.0597\n",
      "Epoch 7/100\n",
      "40/40 [==============================] - ETA: 4s - loss: 0.9202 - acc: 0.048 - ETA: 1s - loss: 1.0464 - acc: 0.053 - 21s 524ms/step - loss: 1.0594 - acc: 0.0560 - val_loss: 1.0994 - val_acc: 0.0613\n",
      "Epoch 8/100\n",
      "40/40 [==============================] - ETA: 4s - loss: 1.0475 - acc: 0.050 - ETA: 1s - loss: 1.0749 - acc: 0.057 - 21s 520ms/step - loss: 1.0618 - acc: 0.0560 - val_loss: 1.1104 - val_acc: 0.0645\n",
      "Epoch 9/100\n",
      "40/40 [==============================] - ETA: 4s - loss: 1.0784 - acc: 0.047 - ETA: 1s - loss: 1.0832 - acc: 0.051 - 21s 521ms/step - loss: 1.0670 - acc: 0.0540 - val_loss: 1.0994 - val_acc: 0.0597\n",
      "Epoch 10/100\n",
      "40/40 [==============================] - ETA: 4s - loss: 0.8304 - acc: 0.037 - ETA: 1s - loss: 1.0200 - acc: 0.053 - 21s 521ms/step - loss: 1.0542 - acc: 0.0565 - val_loss: 1.0956 - val_acc: 0.0613\n",
      "Epoch 11/100\n",
      "40/40 [==============================] - ETA: 5s - loss: 1.1257 - acc: 0.063 - ETA: 1s - loss: 1.1140 - acc: 0.061 - 21s 526ms/step - loss: 1.0625 - acc: 0.0552 - val_loss: 1.1066 - val_acc: 0.0581\n",
      "Epoch 12/100\n",
      "40/40 [==============================] - ETA: 4s - loss: 1.0439 - acc: 0.057 - ETA: 1s - loss: 1.0351 - acc: 0.057 - 21s 519ms/step - loss: 1.0588 - acc: 0.0569 - val_loss: 1.0924 - val_acc: 0.0613\n",
      "Epoch 13/100\n",
      "40/40 [==============================] - ETA: 4s - loss: 0.8995 - acc: 0.038 - ETA: 1s - loss: 1.0216 - acc: 0.049 - 21s 520ms/step - loss: 1.0510 - acc: 0.0536 - val_loss: 1.1146 - val_acc: 0.0565\n",
      "Epoch 14/100\n",
      "40/40 [==============================] - ETA: 7s - loss: 1.2273 - acc: 0.071 - ETA: 2s - loss: 1.0604 - acc: 0.058 - 25s 636ms/step - loss: 1.0544 - acc: 0.0581 - val_loss: 1.0871 - val_acc: 0.0645\n",
      "Epoch 15/100\n",
      "40/40 [==============================] - ETA: 4s - loss: 1.0836 - acc: 0.075 - ETA: 1s - loss: 1.0950 - acc: 0.065 - 22s 562ms/step - loss: 1.0393 - acc: 0.0706 - val_loss: 1.1367 - val_acc: 0.1565\n",
      "Epoch 16/100\n",
      "40/40 [==============================] - ETA: 5s - loss: 0.9787 - acc: 0.242 - ETA: 1s - loss: 1.0298 - acc: 0.149 - 27s 675ms/step - loss: 1.0577 - acc: 0.1315 - val_loss: 1.0879 - val_acc: 0.0629\n",
      "Epoch 17/100\n",
      "40/40 [==============================] - ETA: 6s - loss: 1.0998 - acc: 0.062 - ETA: 1s - loss: 1.0620 - acc: 0.058 - 24s 602ms/step - loss: 1.0520 - acc: 0.0581 - val_loss: 1.0901 - val_acc: 0.0629\n",
      "Epoch 18/100\n",
      "40/40 [==============================] - ETA: 5s - loss: 1.1525 - acc: 0.071 - ETA: 1s - loss: 1.0525 - acc: 0.061 - 26s 645ms/step - loss: 1.0354 - acc: 0.0629 - val_loss: 1.0911 - val_acc: 0.0645\n",
      "Epoch 19/100\n",
      "40/40 [==============================] - ETA: 5s - loss: 1.0061 - acc: 0.062 - ETA: 1s - loss: 1.0737 - acc: 0.064 - 25s 629ms/step - loss: 1.0302 - acc: 0.0597 - val_loss: 1.1175 - val_acc: 0.0629\n",
      "Epoch 20/100\n",
      "40/40 [==============================] - ETA: 5s - loss: 1.1880 - acc: 0.068 - ETA: 1s - loss: 1.0475 - acc: 0.054 - 24s 590ms/step - loss: 1.0459 - acc: 0.0556 - val_loss: 1.0891 - val_acc: 0.0645\n",
      "Epoch 21/100\n",
      "40/40 [==============================] - ETA: 5s - loss: 1.0130 - acc: 0.052 - ETA: 1s - loss: 1.0451 - acc: 0.063 - 23s 563ms/step - loss: 1.0281 - acc: 0.0617 - val_loss: 1.0848 - val_acc: 0.0661\n",
      "Epoch 22/100\n",
      "40/40 [==============================] - ETA: 4s - loss: 1.0104 - acc: 0.056 - ETA: 1s - loss: 0.9509 - acc: 0.054 - 22s 542ms/step - loss: 1.0242 - acc: 0.0605 - val_loss: 1.1161 - val_acc: 0.0532\n",
      "Epoch 23/100\n",
      "40/40 [==============================] - ETA: 5s - loss: 0.8425 - acc: 0.051 - ETA: 1s - loss: 0.9861 - acc: 0.055 - 24s 610ms/step - loss: 1.0143 - acc: 0.0597 - val_loss: 1.0842 - val_acc: 0.0613\n",
      "Epoch 24/100\n",
      "40/40 [==============================] - ETA: 5s - loss: 1.1893 - acc: 0.076 - ETA: 1s - loss: 1.0344 - acc: 0.068 - 24s 603ms/step - loss: 1.0165 - acc: 0.0698 - val_loss: 1.1141 - val_acc: 0.0613\n",
      "Epoch 25/100\n",
      "40/40 [==============================] - ETA: 5s - loss: 1.0179 - acc: 0.057 - ETA: 1s - loss: 1.0172 - acc: 0.065 - 26s 644ms/step - loss: 1.0091 - acc: 0.0637 - val_loss: 1.1570 - val_acc: 0.0532\n",
      "Epoch 26/100\n",
      "40/40 [==============================] - ETA: 5s - loss: 0.8783 - acc: 0.058 - ETA: 1s - loss: 0.9665 - acc: 0.064 - 27s 663ms/step - loss: 1.0111 - acc: 0.0694 - val_loss: 1.0951 - val_acc: 0.0597\n",
      "Epoch 27/100\n",
      "40/40 [==============================] - ETA: 6s - loss: 1.0247 - acc: 0.064 - ETA: 2s - loss: 1.0524 - acc: 0.073 - 27s 663ms/step - loss: 0.9958 - acc: 0.0669 - val_loss: 1.1209 - val_acc: 0.0548\n",
      "Epoch 28/100\n",
      "40/40 [==============================] - ETA: 4s - loss: 0.9750 - acc: 0.069 - ETA: 1s - loss: 0.9760 - acc: 0.067 - 24s 612ms/step - loss: 0.9901 - acc: 0.0702 - val_loss: 1.0957 - val_acc: 0.0565\n",
      "Epoch 29/100\n",
      "40/40 [==============================] - ETA: 5s - loss: 1.1899 - acc: 0.082 - ETA: 1s - loss: 1.0435 - acc: 0.068 - 22s 543ms/step - loss: 1.0076 - acc: 0.0637 - val_loss: 1.0818 - val_acc: 0.0645\n",
      "Epoch 30/100\n",
      "40/40 [==============================] - ETA: 4s - loss: 1.0928 - acc: 0.080 - ETA: 1s - loss: 1.0308 - acc: 0.076 - 22s 558ms/step - loss: 0.9873 - acc: 0.0730 - val_loss: 1.0892 - val_acc: 0.0565\n",
      "Epoch 31/100\n",
      "40/40 [==============================] - ETA: 5s - loss: 1.0015 - acc: 0.084 - ETA: 1s - loss: 0.9313 - acc: 0.068 - 23s 579ms/step - loss: 0.9717 - acc: 0.0734 - val_loss: 1.3976 - val_acc: 0.0484\n",
      "Epoch 32/100\n",
      "40/40 [==============================] - ETA: 4s - loss: 0.9758 - acc: 0.058 - ETA: 1s - loss: 1.0165 - acc: 0.068 - 23s 583ms/step - loss: 1.0275 - acc: 0.0661 - val_loss: 1.0649 - val_acc: 0.0677\n",
      "Epoch 33/100\n",
      "40/40 [==============================] - ETA: 5s - loss: 1.0474 - acc: 0.077 - ETA: 1s - loss: 0.9715 - acc: 0.071 - 26s 644ms/step - loss: 0.9804 - acc: 0.0762 - val_loss: 1.0723 - val_acc: 0.0710\n",
      "Epoch 34/100\n",
      "40/40 [==============================] - ETA: 5s - loss: 0.9508 - acc: 0.069 - ETA: 1s - loss: 0.9060 - acc: 0.066 - 25s 613ms/step - loss: 0.9696 - acc: 0.0726 - val_loss: 1.0707 - val_acc: 0.0629\n",
      "Epoch 35/100\n",
      "40/40 [==============================] - ETA: 5s - loss: 0.9547 - acc: 0.081 - ETA: 1s - loss: 0.9908 - acc: 0.082 - 25s 627ms/step - loss: 0.9592 - acc: 0.0778 - val_loss: 1.0724 - val_acc: 0.0661\n",
      "Epoch 36/100\n",
      "40/40 [==============================] - ETA: 5s - loss: 0.9317 - acc: 0.068 - ETA: 1s - loss: 0.9956 - acc: 0.073 - 26s 640ms/step - loss: 0.9733 - acc: 0.0706 - val_loss: 1.0671 - val_acc: 0.0677\n",
      "Epoch 37/100\n",
      "40/40 [==============================] - ETA: 5s - loss: 1.0081 - acc: 0.086 - ETA: 1s - loss: 0.9630 - acc: 0.084 - 26s 655ms/step - loss: 0.9458 - acc: 0.0815 - val_loss: 1.0999 - val_acc: 0.0629\n",
      "Epoch 38/100\n",
      "40/40 [==============================] - ETA: 6s - loss: 0.9046 - acc: 0.069 - ETA: 2s - loss: 0.9510 - acc: 0.074 - 29s 715ms/step - loss: 0.9512 - acc: 0.0726 - val_loss: 1.0789 - val_acc: 0.0613\n",
      "Epoch 39/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40/40 [==============================] - ETA: 5s - loss: 0.9976 - acc: 0.101 - ETA: 1s - loss: 0.9589 - acc: 0.086 - 24s 597ms/step - loss: 0.9329 - acc: 0.0831 - val_loss: 1.0877 - val_acc: 0.0694\n",
      "Epoch 40/100\n",
      "40/40 [==============================] - ETA: 5s - loss: 0.8890 - acc: 0.080 - ETA: 1s - loss: 0.9412 - acc: 0.081 - 24s 593ms/step - loss: 0.9343 - acc: 0.0770 - val_loss: 1.0806 - val_acc: 0.0597\n",
      "Epoch 41/100\n",
      "40/40 [==============================] - ETA: 5s - loss: 0.8885 - acc: 0.069 - ETA: 1s - loss: 0.9356 - acc: 0.077 - 28s 706ms/step - loss: 0.9511 - acc: 0.0794 - val_loss: 1.0674 - val_acc: 0.0694\n",
      "Epoch 42/100\n",
      "40/40 [==============================] - ETA: 6s - loss: 0.9107 - acc: 0.083 - ETA: 2s - loss: 0.9024 - acc: 0.078 - 28s 708ms/step - loss: 0.9271 - acc: 0.0815 - val_loss: 1.0687 - val_acc: 0.0726\n",
      "Epoch 43/100\n",
      "40/40 [==============================] - ETA: 7s - loss: 0.8367 - acc: 0.079 - ETA: 2s - loss: 0.9290 - acc: 0.090 - 30s 754ms/step - loss: 0.9229 - acc: 0.0847 - val_loss: 1.1017 - val_acc: 0.0710\n",
      "Epoch 44/100\n",
      "40/40 [==============================] - ETA: 6s - loss: 1.0757 - acc: 0.102 - ETA: 2s - loss: 0.9823 - acc: 0.091 - 27s 671ms/step - loss: 0.9116 - acc: 0.0883 - val_loss: 1.1110 - val_acc: 0.0758\n",
      "Epoch 45/100\n",
      "40/40 [==============================] - ETA: 5s - loss: 0.8437 - acc: 0.089 - ETA: 1s - loss: 0.9085 - acc: 0.090 - 24s 589ms/step - loss: 0.9072 - acc: 0.0855 - val_loss: 1.0807 - val_acc: 0.0710\n",
      "Epoch 46/100\n",
      "40/40 [==============================] - ETA: 4s - loss: 0.8457 - acc: 0.071 - ETA: 1s - loss: 0.9618 - acc: 0.086 - 22s 559ms/step - loss: 0.9207 - acc: 0.0847 - val_loss: 1.0881 - val_acc: 0.0694\n",
      "Epoch 47/100\n",
      "40/40 [==============================] - ETA: 4s - loss: 1.0994 - acc: 0.101 - ETA: 1s - loss: 0.9555 - acc: 0.085 - 24s 609ms/step - loss: 0.9118 - acc: 0.0871 - val_loss: 1.0677 - val_acc: 0.0855\n",
      "Epoch 48/100\n",
      "40/40 [==============================] - ETA: 5s - loss: 0.9335 - acc: 0.084 - ETA: 1s - loss: 0.9218 - acc: 0.090 - 27s 666ms/step - loss: 0.8897 - acc: 0.0952 - val_loss: 1.1006 - val_acc: 0.0661\n",
      "Epoch 49/100\n",
      "40/40 [==============================] - ETA: 7s - loss: 0.6869 - acc: 0.075 - ETA: 2s - loss: 0.8239 - acc: 0.083 - 28s 698ms/step - loss: 0.8992 - acc: 0.0952 - val_loss: 1.1290 - val_acc: 0.0742\n",
      "Epoch 50/100\n",
      "40/40 [==============================] - ETA: 5s - loss: 0.8636 - acc: 0.074 - ETA: 1s - loss: 0.8650 - acc: 0.087 - 26s 658ms/step - loss: 0.9006 - acc: 0.0895 - val_loss: 1.1087 - val_acc: 0.0758\n",
      "Epoch 51/100\n",
      "40/40 [==============================] - ETA: 5s - loss: 0.8847 - acc: 0.090 - ETA: 1s - loss: 0.8746 - acc: 0.095 - 23s 576ms/step - loss: 0.8787 - acc: 0.0952 - val_loss: 1.1603 - val_acc: 0.0613\n",
      "Epoch 52/100\n",
      "40/40 [==============================] - ETA: 5s - loss: 0.9145 - acc: 0.086 - ETA: 1s - loss: 0.8821 - acc: 0.092 - 25s 622ms/step - loss: 0.8912 - acc: 0.0952 - val_loss: 1.0769 - val_acc: 0.0758\n",
      "Epoch 53/100\n",
      "40/40 [==============================] - ETA: 6s - loss: 0.8383 - acc: 0.094 - ETA: 1s - loss: 0.8880 - acc: 0.101 - 26s 653ms/step - loss: 0.8765 - acc: 0.1000 - val_loss: 1.1680 - val_acc: 0.0823\n",
      "Epoch 54/100\n",
      "40/40 [==============================] - ETA: 5s - loss: 0.8623 - acc: 0.088 - ETA: 1s - loss: 0.8501 - acc: 0.090 - 23s 582ms/step - loss: 0.8808 - acc: 0.0956 - val_loss: 1.1574 - val_acc: 0.0806\n",
      "Epoch 55/100\n",
      "40/40 [==============================] - ETA: 5s - loss: 0.8426 - acc: 0.103 - ETA: 1s - loss: 0.8743 - acc: 0.108 - 27s 680ms/step - loss: 0.8577 - acc: 0.1040 - val_loss: 1.1747 - val_acc: 0.0806\n",
      "Epoch 56/100\n",
      "40/40 [==============================] - ETA: 6s - loss: 0.7789 - acc: 0.098 - ETA: 1s - loss: 0.8376 - acc: 0.104 - 24s 594ms/step - loss: 0.8546 - acc: 0.1048 - val_loss: 1.0676 - val_acc: 0.0823\n",
      "Epoch 57/100\n",
      "40/40 [==============================] - ETA: 5s - loss: 1.0624 - acc: 0.121 - ETA: 1s - loss: 0.9005 - acc: 0.107 - 24s 589ms/step - loss: 0.8603 - acc: 0.1060 - val_loss: 1.0848 - val_acc: 0.0823\n",
      "Epoch 58/100\n",
      "40/40 [==============================] - ETA: 4s - loss: 0.9985 - acc: 0.110 - ETA: 1s - loss: 0.8769 - acc: 0.102 - 24s 612ms/step - loss: 0.8547 - acc: 0.0980 - val_loss: 1.0606 - val_acc: 0.0871\n",
      "Epoch 59/100\n",
      "40/40 [==============================] - ETA: 5s - loss: 0.8453 - acc: 0.095 - ETA: 1s - loss: 0.8543 - acc: 0.098 - 30s 744ms/step - loss: 0.8563 - acc: 0.1016 - val_loss: 1.0989 - val_acc: 0.0871\n",
      "Epoch 60/100\n",
      "40/40 [==============================] - ETA: 6s - loss: 0.8290 - acc: 0.107 - ETA: 2s - loss: 0.8167 - acc: 0.100 - 32s 796ms/step - loss: 0.8537 - acc: 0.0984 - val_loss: 1.0709 - val_acc: 0.0758\n",
      "Epoch 61/100\n",
      "40/40 [==============================] - ETA: 6s - loss: 0.9177 - acc: 0.110 - ETA: 2s - loss: 0.8501 - acc: 0.108 - 31s 787ms/step - loss: 0.8449 - acc: 0.1113 - val_loss: 1.1382 - val_acc: 0.0806\n",
      "Epoch 62/100\n",
      "40/40 [==============================] - ETA: 5s - loss: 0.7608 - acc: 0.093 - ETA: 1s - loss: 0.8211 - acc: 0.106 - 28s 711ms/step - loss: 0.8326 - acc: 0.1085 - val_loss: 1.1227 - val_acc: 0.0758\n",
      "Epoch 63/100\n",
      "40/40 [==============================] - ETA: 6s - loss: 0.8281 - acc: 0.086 - ETA: 2s - loss: 0.8639 - acc: 0.100 - 26s 645ms/step - loss: 0.8398 - acc: 0.1012 - val_loss: 1.0797 - val_acc: 0.0774\n",
      "Epoch 64/100\n",
      "40/40 [==============================] - ETA: 5s - loss: 0.9456 - acc: 0.131 - ETA: 1s - loss: 0.8808 - acc: 0.120 - 25s 621ms/step - loss: 0.8161 - acc: 0.1113 - val_loss: 1.1862 - val_acc: 0.0806\n",
      "Epoch 65/100\n",
      "40/40 [==============================] - ETA: 5s - loss: 0.7658 - acc: 0.107 - ETA: 1s - loss: 0.7776 - acc: 0.099 - 25s 634ms/step - loss: 0.8363 - acc: 0.1089 - val_loss: 1.0567 - val_acc: 0.0887\n",
      "Epoch 66/100\n",
      "40/40 [==============================] - ETA: 5s - loss: 0.7115 - acc: 0.098 - ETA: 1s - loss: 0.7850 - acc: 0.101 - 25s 623ms/step - loss: 0.8310 - acc: 0.1060 - val_loss: 1.1208 - val_acc: 0.0806\n",
      "Epoch 67/100\n",
      "40/40 [==============================] - ETA: 5s - loss: 0.9010 - acc: 0.132 - ETA: 1s - loss: 0.8975 - acc: 0.123 - 28s 688ms/step - loss: 0.8179 - acc: 0.1077 - val_loss: 1.1070 - val_acc: 0.0806\n",
      "Epoch 68/100\n",
      "40/40 [==============================] - ETA: 5s - loss: 0.9054 - acc: 0.119 - ETA: 1s - loss: 0.8515 - acc: 0.115 - 24s 590ms/step - loss: 0.8157 - acc: 0.1137 - val_loss: 1.0977 - val_acc: 0.0790\n",
      "Epoch 69/100\n",
      "40/40 [==============================] - ETA: 6s - loss: 0.8290 - acc: 0.119 - ETA: 2s - loss: 0.7940 - acc: 0.111 - 42s 1s/step - loss: 0.8236 - acc: 0.1073 - val_loss: 1.1513 - val_acc: 0.0823\n",
      "Epoch 70/100\n",
      "40/40 [==============================] - ETA: 8s - loss: 0.9538 - acc: 0.128 - ETA: 2s - loss: 0.8198 - acc: 0.114 - 42s 1s/step - loss: 0.8092 - acc: 0.1085 - val_loss: 1.1831 - val_acc: 0.0790\n",
      "Epoch 71/100\n",
      "40/40 [==============================] - ETA: 8s - loss: 0.8963 - acc: 0.125 - ETA: 2s - loss: 0.8530 - acc: 0.107 - 42s 1s/step - loss: 0.8206 - acc: 0.1065 - val_loss: 1.1266 - val_acc: 0.0790\n",
      "Epoch 72/100\n",
      "40/40 [==============================] - ETA: 8s - loss: 0.6770 - acc: 0.115 - ETA: 2s - loss: 0.7450 - acc: 0.121 - 42s 1s/step - loss: 0.7868 - acc: 0.1234 - val_loss: 1.2232 - val_acc: 0.0871\n",
      "Epoch 73/100\n",
      "40/40 [==============================] - ETA: 8s - loss: 0.6949 - acc: 0.100 - ETA: 2s - loss: 0.8640 - acc: 0.118 - 40s 1s/step - loss: 0.8181 - acc: 0.1105 - val_loss: 1.0936 - val_acc: 0.0774\n",
      "Epoch 74/100\n",
      "40/40 [==============================] - ETA: 8s - loss: 0.7563 - acc: 0.126 - ETA: 2s - loss: 0.7511 - acc: 0.119 - 41s 1s/step - loss: 0.7986 - acc: 0.1230 - val_loss: 1.1242 - val_acc: 0.0839\n",
      "Epoch 75/100\n",
      "40/40 [==============================] - ETA: 8s - loss: 0.7274 - acc: 0.125 - ETA: 2s - loss: 0.7323 - acc: 0.116 - 42s 1s/step - loss: 0.7758 - acc: 0.1226 - val_loss: 1.2076 - val_acc: 0.0823\n",
      "Epoch 76/100\n",
      "40/40 [==============================] - ETA: 9s - loss: 0.7936 - acc: 0.124 - ETA: 3s - loss: 0.7897 - acc: 0.119 - 47s 1s/step - loss: 0.7884 - acc: 0.1165 - val_loss: 1.1289 - val_acc: 0.0871\n",
      "Epoch 77/100\n",
      "40/40 [==============================] - ETA: 8s - loss: 0.9690 - acc: 0.156 - ETA: 2s - loss: 0.7909 - acc: 0.127 - 42s 1s/step - loss: 0.7811 - acc: 0.1198 - val_loss: 1.1061 - val_acc: 0.0823\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78/100\n",
      "40/40 [==============================] - ETA: 8s - loss: 0.6342 - acc: 0.100 - ETA: 2s - loss: 0.7486 - acc: 0.127 - 39s 964ms/step - loss: 0.7679 - acc: 0.1290 - val_loss: 1.2029 - val_acc: 0.0855\n",
      "Epoch 79/100\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.9333 - acc: 0.132 - ETA: 1s - loss: 0.8372 - acc: 0.119 - 17s 417ms/step - loss: 0.7861 - acc: 0.1157 - val_loss: 1.1570 - val_acc: 0.0839\n",
      "Epoch 80/100\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.6976 - acc: 0.109 - ETA: 1s - loss: 0.7609 - acc: 0.129 - 17s 418ms/step - loss: 0.7674 - acc: 0.1246 - val_loss: 1.2365 - val_acc: 0.0742\n",
      "Epoch 81/100\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.9802 - acc: 0.155 - ETA: 1s - loss: 0.8128 - acc: 0.141 - 16s 392ms/step - loss: 0.7736 - acc: 0.1379 - val_loss: 1.1507 - val_acc: 0.0790\n",
      "Epoch 82/100\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.9531 - acc: 0.133 - ETA: 1s - loss: 0.7869 - acc: 0.107 - 16s 408ms/step - loss: 0.7950 - acc: 0.1161 - val_loss: 1.1875 - val_acc: 0.0806\n",
      "Epoch 83/100\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.8016 - acc: 0.137 - ETA: 1s - loss: 0.7603 - acc: 0.126 - 16s 404ms/step - loss: 0.7651 - acc: 0.1246 - val_loss: 1.2491 - val_acc: 0.0839\n",
      "Epoch 84/100\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.8968 - acc: 0.142 - ETA: 1s - loss: 0.8367 - acc: 0.132 - 16s 400ms/step - loss: 0.7689 - acc: 0.1226 - val_loss: 1.0888 - val_acc: 0.0919\n",
      "Epoch 85/100\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.6898 - acc: 0.110 - ETA: 1s - loss: 0.7783 - acc: 0.127 - 16s 403ms/step - loss: 0.7753 - acc: 0.1278 - val_loss: 1.0824 - val_acc: 0.0790\n",
      "Epoch 86/100\n",
      "40/40 [==============================] - ETA: 2s - loss: 0.8196 - acc: 0.143 - ETA: 1s - loss: 0.7814 - acc: 0.121 - 17s 430ms/step - loss: 0.7562 - acc: 0.1250 - val_loss: 1.1481 - val_acc: 0.0871\n",
      "Epoch 87/100\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.7311 - acc: 0.129 - ETA: 1s - loss: 0.7200 - acc: 0.114 - 17s 430ms/step - loss: 0.7585 - acc: 0.1262 - val_loss: 1.0886 - val_acc: 0.0790\n",
      "Epoch 88/100\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.6046 - acc: 0.121 - ETA: 1s - loss: 0.6906 - acc: 0.137 - 17s 418ms/step - loss: 0.7332 - acc: 0.1403 - val_loss: 1.1215 - val_acc: 0.0871\n",
      "Epoch 89/100\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.6265 - acc: 0.132 - ETA: 1s - loss: 0.6840 - acc: 0.125 - 18s 441ms/step - loss: 0.7377 - acc: 0.1419 - val_loss: 1.1347 - val_acc: 0.0919\n",
      "Epoch 90/100\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.7426 - acc: 0.122 - ETA: 1s - loss: 0.7675 - acc: 0.133 - 17s 436ms/step - loss: 0.7442 - acc: 0.1290 - val_loss: 1.1208 - val_acc: 0.0871\n",
      "Epoch 91/100\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.8120 - acc: 0.155 - ETA: 1s - loss: 0.7627 - acc: 0.144 - 18s 444ms/step - loss: 0.7201 - acc: 0.1371 - val_loss: 1.1171 - val_acc: 0.0871\n",
      "Epoch 92/100\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.8341 - acc: 0.175 - ETA: 1s - loss: 0.7451 - acc: 0.142 - 18s 447ms/step - loss: 0.7220 - acc: 0.1411 - val_loss: 1.1126 - val_acc: 0.0871\n",
      "Epoch 93/100\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.7311 - acc: 0.126 - ETA: 1s - loss: 0.7361 - acc: 0.143 - 19s 480ms/step - loss: 0.7225 - acc: 0.1411 - val_loss: 1.1166 - val_acc: 0.0774\n",
      "Epoch 94/100\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.6113 - acc: 0.132 - ETA: 1s - loss: 0.7145 - acc: 0.145 - 18s 460ms/step - loss: 0.7057 - acc: 0.1448 - val_loss: 1.1618 - val_acc: 0.0823\n",
      "Epoch 95/100\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.7398 - acc: 0.138 - ETA: 1s - loss: 0.7152 - acc: 0.135 - 18s 455ms/step - loss: 0.7178 - acc: 0.1407 - val_loss: 1.1403 - val_acc: 0.0903\n",
      "Epoch 96/100\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.6907 - acc: 0.152 - ETA: 1s - loss: 0.6802 - acc: 0.146 - 20s 497ms/step - loss: 0.6825 - acc: 0.1500 - val_loss: 1.1852 - val_acc: 0.0855\n",
      "Epoch 97/100\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.6301 - acc: 0.129 - ETA: 1s - loss: 0.7403 - acc: 0.156 - 18s 459ms/step - loss: 0.6953 - acc: 0.1423 - val_loss: 1.1584 - val_acc: 0.0823\n",
      "Epoch 98/100\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.6073 - acc: 0.106 - ETA: 1s - loss: 0.7132 - acc: 0.129 - 17s 436ms/step - loss: 0.7238 - acc: 0.1367 - val_loss: 1.1646 - val_acc: 0.0823\n",
      "Epoch 99/100\n",
      "40/40 [==============================] - ETA: 4s - loss: 0.8455 - acc: 0.150 - ETA: 1s - loss: 0.7248 - acc: 0.136 - 19s 478ms/step - loss: 0.7139 - acc: 0.1359 - val_loss: 1.2297 - val_acc: 0.0806\n",
      "Epoch 100/100\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.6654 - acc: 0.141 - ETA: 1s - loss: 0.6824 - acc: 0.152 - 18s 461ms/step - loss: 0.6901 - acc: 0.1500 - val_loss: 1.1145 - val_acc: 0.0935\n",
      "Do you want to run the Model again for 1 epoch more? (1 - Yes | 0 - No) : 0\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    try:\n",
    "        run_again = int(input(\"Do you want to run the Model again for 1 epoch more? (1 - Yes | 0 - No) : \"))\n",
    "        if run_again == 1:\n",
    "            model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
    "                    batch_size=16,\n",
    "                    epochs=100,\n",
    "                    validation_split=0.2)\n",
    "        else:\n",
    "            break\n",
    "    except Exception as e:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Do you want to run the Model again for 1 epoch more? (1 - Yes | 0 - No) : 1\n",
      "Train on 40 samples, validate on 10 samples\n",
      "Epoch 1/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.6921 - acc: 0.122 - ETA: 1s - loss: 0.6983 - acc: 0.151 - 20s 494ms/step - loss: 0.6957 - acc: 0.1452 - val_loss: 1.1281 - val_acc: 0.0790\n",
      "Epoch 2/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.7279 - acc: 0.129 - ETA: 1s - loss: 0.6723 - acc: 0.139 - 17s 420ms/step - loss: 0.6947 - acc: 0.1427 - val_loss: 1.2058 - val_acc: 0.0855\n",
      "Epoch 3/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.7233 - acc: 0.148 - ETA: 1s - loss: 0.6817 - acc: 0.146 - 16s 409ms/step - loss: 0.6864 - acc: 0.1496 - val_loss: 1.1574 - val_acc: 0.0887\n",
      "Epoch 4/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.5748 - acc: 0.128 - ETA: 1s - loss: 0.6294 - acc: 0.128 - 16s 394ms/step - loss: 0.7106 - acc: 0.1347 - val_loss: 1.1828 - val_acc: 0.0839\n",
      "Epoch 5/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.8630 - acc: 0.191 - ETA: 1s - loss: 0.7354 - acc: 0.164 - 17s 413ms/step - loss: 0.7011 - acc: 0.1492 - val_loss: 1.1130 - val_acc: 0.0952\n",
      "Epoch 6/1000\n",
      "40/40 [==============================] - ETA: 2s - loss: 0.6416 - acc: 0.145 - ETA: 1s - loss: 0.6914 - acc: 0.153 - 17s 413ms/step - loss: 0.6762 - acc: 0.1460 - val_loss: 1.1974 - val_acc: 0.0919\n",
      "Epoch 7/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.7043 - acc: 0.161 - ETA: 1s - loss: 0.7176 - acc: 0.149 - 16s 393ms/step - loss: 0.6781 - acc: 0.1444 - val_loss: 1.1404 - val_acc: 0.0871\n",
      "Epoch 8/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.6783 - acc: 0.158 - ETA: 1s - loss: 0.6896 - acc: 0.146 - 18s 442ms/step - loss: 0.6780 - acc: 0.1452 - val_loss: 1.1934 - val_acc: 0.0790\n",
      "Epoch 9/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.6120 - acc: 0.141 - ETA: 1s - loss: 0.6849 - acc: 0.167 - 15s 384ms/step - loss: 0.6570 - acc: 0.1565 - val_loss: 1.1805 - val_acc: 0.0855\n",
      "Epoch 10/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.6820 - acc: 0.170 - ETA: 1s - loss: 0.6722 - acc: 0.157 - 15s 382ms/step - loss: 0.6393 - acc: 0.1585 - val_loss: 1.1781 - val_acc: 0.0887\n",
      "Epoch 11/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.6197 - acc: 0.149 - ETA: 1s - loss: 0.6308 - acc: 0.160 - 15s 387ms/step - loss: 0.6411 - acc: 0.1577 - val_loss: 1.2438 - val_acc: 0.0887\n",
      "Epoch 12/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.6418 - acc: 0.156 - ETA: 1s - loss: 0.6410 - acc: 0.148 - 16s 400ms/step - loss: 0.6632 - acc: 0.1496 - val_loss: 1.1728 - val_acc: 0.0919\n",
      "Epoch 13/1000\n",
      "40/40 [==============================] - ETA: 2s - loss: 0.6679 - acc: 0.176 - ETA: 1s - loss: 0.6665 - acc: 0.159 - 16s 401ms/step - loss: 0.6583 - acc: 0.1552 - val_loss: 1.1781 - val_acc: 0.0871\n",
      "Epoch 14/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.7544 - acc: 0.198 - ETA: 1s - loss: 0.6295 - acc: 0.170 - 15s 380ms/step - loss: 0.6314 - acc: 0.1625 - val_loss: 1.1431 - val_acc: 0.0887\n",
      "Epoch 15/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.5956 - acc: 0.154 - ETA: 1s - loss: 0.6382 - acc: 0.161 - 16s 396ms/step - loss: 0.6252 - acc: 0.1601 - val_loss: 1.2064 - val_acc: 0.0919\n",
      "Epoch 16/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.6958 - acc: 0.163 - ETA: 1s - loss: 0.6783 - acc: 0.176 - 15s 385ms/step - loss: 0.6245 - acc: 0.1581 - val_loss: 1.1940 - val_acc: 0.0855\n",
      "Epoch 17/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.6654 - acc: 0.194 - ETA: 1s - loss: 0.6487 - acc: 0.159 - 16s 402ms/step - loss: 0.6300 - acc: 0.1681 - val_loss: 1.2452 - val_acc: 0.0871\n",
      "Epoch 18/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.5929 - acc: 0.189 - ETA: 1s - loss: 0.5804 - acc: 0.166 - 16s 395ms/step - loss: 0.6242 - acc: 0.1657 - val_loss: 1.1646 - val_acc: 0.0935\n",
      "Epoch 19/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.5849 - acc: 0.140 - ETA: 1s - loss: 0.6096 - acc: 0.152 - 16s 388ms/step - loss: 0.6269 - acc: 0.1621 - val_loss: 1.2031 - val_acc: 0.0839\n",
      "Epoch 20/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.6982 - acc: 0.186 - ETA: 1s - loss: 0.6730 - acc: 0.180 - 16s 390ms/step - loss: 0.6185 - acc: 0.1621 - val_loss: 1.1814 - val_acc: 0.0903\n",
      "Epoch 21/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.5845 - acc: 0.157 - ETA: 1s - loss: 0.5926 - acc: 0.172 - 15s 386ms/step - loss: 0.6002 - acc: 0.1770 - val_loss: 1.3427 - val_acc: 0.0871\n",
      "Epoch 22/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.6694 - acc: 0.183 - ETA: 1s - loss: 0.6132 - acc: 0.171 - 16s 395ms/step - loss: 0.6185 - acc: 0.1685 - val_loss: 1.1788 - val_acc: 0.0887\n",
      "Epoch 23/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.5510 - acc: 0.161 - ETA: 1s - loss: 0.6324 - acc: 0.180 - 15s 385ms/step - loss: 0.6019 - acc: 0.1706 - val_loss: 1.1720 - val_acc: 0.0984\n",
      "Epoch 24/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.5939 - acc: 0.153 - ETA: 1s - loss: 0.6008 - acc: 0.161 - 16s 392ms/step - loss: 0.6135 - acc: 0.1710 - val_loss: 1.3072 - val_acc: 0.0919\n",
      "Epoch 25/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.6225 - acc: 0.181 - ETA: 1s - loss: 0.6071 - acc: 0.172 - 16s 406ms/step - loss: 0.6073 - acc: 0.1706 - val_loss: 1.2252 - val_acc: 0.0952\n",
      "Epoch 26/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.6469 - acc: 0.146 - ETA: 1s - loss: 0.6185 - acc: 0.160 - 16s 399ms/step - loss: 0.5999 - acc: 0.1669 - val_loss: 1.4097 - val_acc: 0.0823\n",
      "Epoch 27/1000\n",
      "40/40 [==============================] - ETA: 2s - loss: 0.5441 - acc: 0.148 - ETA: 0s - loss: 0.5609 - acc: 0.167 - 15s 387ms/step - loss: 0.5931 - acc: 0.1754 - val_loss: 1.2304 - val_acc: 0.1000\n",
      "Epoch 28/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.5412 - acc: 0.175 - ETA: 1s - loss: 0.6175 - acc: 0.191 - 15s 387ms/step - loss: 0.5959 - acc: 0.1746 - val_loss: 1.2082 - val_acc: 0.1016\n",
      "Epoch 29/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.5538 - acc: 0.168 - ETA: 1s - loss: 0.5728 - acc: 0.168 - 15s 385ms/step - loss: 0.5889 - acc: 0.1750 - val_loss: 1.2580 - val_acc: 0.0952\n",
      "Epoch 30/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.5756 - acc: 0.179 - ETA: 1s - loss: 0.5837 - acc: 0.176 - 15s 382ms/step - loss: 0.5786 - acc: 0.1766 - val_loss: 1.2704 - val_acc: 0.0919\n",
      "Epoch 31/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.5247 - acc: 0.177 - ETA: 1s - loss: 0.5850 - acc: 0.189 - 16s 389ms/step - loss: 0.5762 - acc: 0.1879 - val_loss: 1.2031 - val_acc: 0.0823\n",
      "Epoch 32/1000\n",
      "40/40 [==============================] - ETA: 2s - loss: 0.6040 - acc: 0.214 - ETA: 1s - loss: 0.5704 - acc: 0.191 - 15s 384ms/step - loss: 0.5752 - acc: 0.1847 - val_loss: 1.2125 - val_acc: 0.1000\n",
      "Epoch 33/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.5612 - acc: 0.172 - ETA: 1s - loss: 0.5610 - acc: 0.177 - 16s 395ms/step - loss: 0.5647 - acc: 0.1839 - val_loss: 1.2535 - val_acc: 0.0968\n",
      "Epoch 34/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.6000 - acc: 0.205 - ETA: 1s - loss: 0.5777 - acc: 0.187 - 16s 391ms/step - loss: 0.5598 - acc: 0.1802 - val_loss: 1.1902 - val_acc: 0.0984\n",
      "Epoch 35/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.4948 - acc: 0.166 - ETA: 1s - loss: 0.5556 - acc: 0.188 - 16s 388ms/step - loss: 0.5588 - acc: 0.1903 - val_loss: 1.3249 - val_acc: 0.0984\n",
      "Epoch 36/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.5520 - acc: 0.177 - ETA: 1s - loss: 0.5243 - acc: 0.162 - 16s 399ms/step - loss: 0.5483 - acc: 0.1806 - val_loss: 1.3234 - val_acc: 0.0855\n",
      "Epoch 37/1000\n",
      "40/40 [==============================] - ETA: 2s - loss: 0.5659 - acc: 0.166 - ETA: 1s - loss: 0.5689 - acc: 0.179 - 16s 389ms/step - loss: 0.5686 - acc: 0.1782 - val_loss: 1.2534 - val_acc: 0.0903\n",
      "Epoch 38/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.6135 - acc: 0.196 - ETA: 1s - loss: 0.5693 - acc: 0.176 - 16s 391ms/step - loss: 0.5601 - acc: 0.1831 - val_loss: 1.2661 - val_acc: 0.0968\n",
      "Epoch 39/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40/40 [==============================] - ETA: 2s - loss: 0.6393 - acc: 0.226 - ETA: 0s - loss: 0.5662 - acc: 0.186 - 16s 389ms/step - loss: 0.5562 - acc: 0.1859 - val_loss: 1.2161 - val_acc: 0.1000\n",
      "Epoch 40/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.5451 - acc: 0.164 - ETA: 1s - loss: 0.5739 - acc: 0.179 - 16s 398ms/step - loss: 0.5654 - acc: 0.1823 - val_loss: 1.2972 - val_acc: 0.0984\n",
      "Epoch 41/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.5797 - acc: 0.206 - ETA: 1s - loss: 0.5846 - acc: 0.205 - 15s 386ms/step - loss: 0.5464 - acc: 0.1871 - val_loss: 1.2656 - val_acc: 0.1000\n",
      "Epoch 42/1000\n",
      "40/40 [==============================] - ETA: 2s - loss: 0.4163 - acc: 0.126 - ETA: 1s - loss: 0.5370 - acc: 0.174 - 16s 398ms/step - loss: 0.5418 - acc: 0.1859 - val_loss: 1.3117 - val_acc: 0.1000\n",
      "Epoch 43/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.5192 - acc: 0.157 - ETA: 1s - loss: 0.5322 - acc: 0.183 - 15s 385ms/step - loss: 0.5379 - acc: 0.1911 - val_loss: 1.1888 - val_acc: 0.1000\n",
      "Epoch 44/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.5596 - acc: 0.212 - ETA: 1s - loss: 0.5333 - acc: 0.186 - 16s 394ms/step - loss: 0.5462 - acc: 0.1847 - val_loss: 1.3012 - val_acc: 0.0903\n",
      "Epoch 45/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.6110 - acc: 0.208 - ETA: 1s - loss: 0.5319 - acc: 0.182 - 15s 387ms/step - loss: 0.5347 - acc: 0.1948 - val_loss: 1.2234 - val_acc: 0.0952\n",
      "Epoch 46/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.4695 - acc: 0.153 - ETA: 1s - loss: 0.4941 - acc: 0.185 - 15s 382ms/step - loss: 0.5322 - acc: 0.1919 - val_loss: 1.2259 - val_acc: 0.0968\n",
      "Epoch 47/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.5996 - acc: 0.245 - ETA: 0s - loss: 0.5163 - acc: 0.204 - 15s 381ms/step - loss: 0.5208 - acc: 0.1948 - val_loss: 1.1962 - val_acc: 0.0968\n",
      "Epoch 48/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.4557 - acc: 0.174 - ETA: 1s - loss: 0.4722 - acc: 0.185 - 16s 396ms/step - loss: 0.5048 - acc: 0.2036 - val_loss: 1.3305 - val_acc: 0.0806\n",
      "Epoch 49/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.6351 - acc: 0.201 - ETA: 1s - loss: 0.5384 - acc: 0.180 - 16s 397ms/step - loss: 0.5293 - acc: 0.1960 - val_loss: 1.1967 - val_acc: 0.0871\n",
      "Epoch 50/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.5626 - acc: 0.178 - ETA: 1s - loss: 0.5121 - acc: 0.189 - 16s 392ms/step - loss: 0.5107 - acc: 0.1976 - val_loss: 1.2352 - val_acc: 0.0984\n",
      "Epoch 51/1000\n",
      "40/40 [==============================] - ETA: 2s - loss: 0.4947 - acc: 0.190 - ETA: 0s - loss: 0.4900 - acc: 0.191 - 15s 387ms/step - loss: 0.4982 - acc: 0.2004 - val_loss: 1.1842 - val_acc: 0.0968\n",
      "Epoch 52/1000\n",
      "40/40 [==============================] - ETA: 2s - loss: 0.6079 - acc: 0.226 - ETA: 1s - loss: 0.5420 - acc: 0.197 - 15s 386ms/step - loss: 0.5234 - acc: 0.1895 - val_loss: 1.2689 - val_acc: 0.0919\n",
      "Epoch 53/1000\n",
      "40/40 [==============================] - ETA: 2s - loss: 0.4691 - acc: 0.165 - ETA: 0s - loss: 0.5153 - acc: 0.191 - 16s 389ms/step - loss: 0.5000 - acc: 0.1964 - val_loss: 1.3086 - val_acc: 0.0887\n",
      "Epoch 54/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.5539 - acc: 0.209 - ETA: 0s - loss: 0.5310 - acc: 0.193 - 15s 386ms/step - loss: 0.5157 - acc: 0.1984 - val_loss: 1.3213 - val_acc: 0.0855\n",
      "Epoch 55/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.5073 - acc: 0.199 - ETA: 0s - loss: 0.5059 - acc: 0.200 - 16s 389ms/step - loss: 0.4978 - acc: 0.1992 - val_loss: 1.3313 - val_acc: 0.0903\n",
      "Epoch 56/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.4720 - acc: 0.170 - ETA: 0s - loss: 0.5018 - acc: 0.202 - 15s 387ms/step - loss: 0.4976 - acc: 0.1952 - val_loss: 1.1833 - val_acc: 0.0984\n",
      "Epoch 57/1000\n",
      "40/40 [==============================] - ETA: 2s - loss: 0.4930 - acc: 0.225 - ETA: 0s - loss: 0.4770 - acc: 0.202 - 16s 391ms/step - loss: 0.4926 - acc: 0.2081 - val_loss: 1.2149 - val_acc: 0.1000\n",
      "Epoch 58/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.4804 - acc: 0.196 - ETA: 1s - loss: 0.5087 - acc: 0.217 - 15s 385ms/step - loss: 0.4902 - acc: 0.2077 - val_loss: 1.3169 - val_acc: 0.0903\n",
      "Epoch 59/1000\n",
      "40/40 [==============================] - ETA: 2s - loss: 0.3883 - acc: 0.143 - ETA: 0s - loss: 0.4737 - acc: 0.206 - 16s 389ms/step - loss: 0.4793 - acc: 0.2069 - val_loss: 1.2679 - val_acc: 0.0968\n",
      "Epoch 60/1000\n",
      "40/40 [==============================] - ETA: 2s - loss: 0.5120 - acc: 0.242 - ETA: 0s - loss: 0.4443 - acc: 0.208 - 15s 384ms/step - loss: 0.4630 - acc: 0.2161 - val_loss: 1.2892 - val_acc: 0.0952\n",
      "Epoch 61/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.4327 - acc: 0.169 - ETA: 1s - loss: 0.4460 - acc: 0.195 - 16s 401ms/step - loss: 0.4646 - acc: 0.2125 - val_loss: 1.2625 - val_acc: 0.0887\n",
      "Epoch 62/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.5113 - acc: 0.198 - ETA: 1s - loss: 0.5579 - acc: 0.200 - 15s 385ms/step - loss: 0.5162 - acc: 0.1964 - val_loss: 1.2484 - val_acc: 0.1048\n",
      "Epoch 63/1000\n",
      "40/40 [==============================] - ETA: 2s - loss: 0.3816 - acc: 0.220 - ETA: 1s - loss: 0.4200 - acc: 0.208 - 16s 394ms/step - loss: 0.4569 - acc: 0.2169 - val_loss: 1.3828 - val_acc: 0.0984\n",
      "Epoch 64/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.5119 - acc: 0.251 - ETA: 1s - loss: 0.4691 - acc: 0.218 - 16s 393ms/step - loss: 0.4670 - acc: 0.2173 - val_loss: 1.2755 - val_acc: 0.0952\n",
      "Epoch 65/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.5610 - acc: 0.244 - ETA: 1s - loss: 0.5071 - acc: 0.215 - 16s 395ms/step - loss: 0.4773 - acc: 0.2121 - val_loss: 1.2501 - val_acc: 0.1048\n",
      "Epoch 66/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.5314 - acc: 0.246 - ETA: 1s - loss: 0.4804 - acc: 0.211 - 15s 384ms/step - loss: 0.4725 - acc: 0.2113 - val_loss: 1.3526 - val_acc: 0.1016\n",
      "Epoch 67/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.3849 - acc: 0.176 - ETA: 1s - loss: 0.4413 - acc: 0.211 - 15s 380ms/step - loss: 0.4450 - acc: 0.2165 - val_loss: 1.3369 - val_acc: 0.0935\n",
      "Epoch 68/1000\n",
      "40/40 [==============================] - ETA: 2s - loss: 0.4191 - acc: 0.231 - ETA: 0s - loss: 0.4512 - acc: 0.225 - 15s 383ms/step - loss: 0.4379 - acc: 0.2226 - val_loss: 1.2737 - val_acc: 0.1065\n",
      "Epoch 69/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.4538 - acc: 0.190 - ETA: 0s - loss: 0.4855 - acc: 0.226 - 15s 379ms/step - loss: 0.4617 - acc: 0.2137 - val_loss: 1.5000 - val_acc: 0.0903\n",
      "Epoch 70/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.4762 - acc: 0.195 - ETA: 1s - loss: 0.5050 - acc: 0.227 - 16s 392ms/step - loss: 0.4782 - acc: 0.2097 - val_loss: 1.2782 - val_acc: 0.0968\n",
      "Epoch 71/1000\n",
      "40/40 [==============================] - ETA: 2s - loss: 0.3815 - acc: 0.234 - ETA: 1s - loss: 0.4267 - acc: 0.213 - 16s 388ms/step - loss: 0.4346 - acc: 0.2210 - val_loss: 1.3523 - val_acc: 0.1000\n",
      "Epoch 72/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.4826 - acc: 0.197 - ETA: 1s - loss: 0.4773 - acc: 0.214 - 16s 390ms/step - loss: 0.4655 - acc: 0.2141 - val_loss: 1.3278 - val_acc: 0.0984\n",
      "Epoch 73/1000\n",
      "40/40 [==============================] - ETA: 2s - loss: 0.4392 - acc: 0.178 - ETA: 1s - loss: 0.4669 - acc: 0.218 - 16s 394ms/step - loss: 0.4444 - acc: 0.2141 - val_loss: 1.3135 - val_acc: 0.0968\n",
      "Epoch 74/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.4263 - acc: 0.228 - ETA: 1s - loss: 0.4197 - acc: 0.219 - 16s 401ms/step - loss: 0.4273 - acc: 0.2198 - val_loss: 1.3890 - val_acc: 0.0984\n",
      "Epoch 75/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.4528 - acc: 0.191 - ETA: 1s - loss: 0.4843 - acc: 0.221 - 15s 386ms/step - loss: 0.4539 - acc: 0.2129 - val_loss: 1.3641 - val_acc: 0.1000\n",
      "Epoch 76/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.3722 - acc: 0.213 - ETA: 1s - loss: 0.4352 - acc: 0.232 - 16s 393ms/step - loss: 0.4360 - acc: 0.2266 - val_loss: 1.3314 - val_acc: 0.0952\n",
      "Epoch 77/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.4267 - acc: 0.229 - ETA: 1s - loss: 0.4269 - acc: 0.236 - 16s 393ms/step - loss: 0.4237 - acc: 0.2262 - val_loss: 1.3890 - val_acc: 0.0887\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.3761 - acc: 0.179 - ETA: 0s - loss: 0.4224 - acc: 0.208 - 15s 386ms/step - loss: 0.4361 - acc: 0.2226 - val_loss: 1.3255 - val_acc: 0.1000\n",
      "Epoch 79/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.3423 - acc: 0.180 - ETA: 1s - loss: 0.4039 - acc: 0.221 - 16s 389ms/step - loss: 0.4154 - acc: 0.2290 - val_loss: 1.3037 - val_acc: 0.1000\n",
      "Epoch 80/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.4296 - acc: 0.227 - ETA: 1s - loss: 0.4079 - acc: 0.240 - 15s 386ms/step - loss: 0.4011 - acc: 0.2347 - val_loss: 1.3582 - val_acc: 0.1000\n",
      "Epoch 81/1000\n",
      "40/40 [==============================] - ETA: 2s - loss: 0.3997 - acc: 0.207 - ETA: 0s - loss: 0.3967 - acc: 0.229 - 16s 392ms/step - loss: 0.4180 - acc: 0.2262 - val_loss: 1.3810 - val_acc: 0.1032\n",
      "Epoch 82/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.4688 - acc: 0.262 - ETA: 1s - loss: 0.4191 - acc: 0.238 - 16s 398ms/step - loss: 0.4110 - acc: 0.2278 - val_loss: 1.3224 - val_acc: 0.1000\n",
      "Epoch 83/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.3592 - acc: 0.246 - ETA: 1s - loss: 0.3846 - acc: 0.216 - 16s 392ms/step - loss: 0.4075 - acc: 0.2298 - val_loss: 1.4088 - val_acc: 0.0935\n",
      "Epoch 84/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.5173 - acc: 0.245 - ETA: 1s - loss: 0.4383 - acc: 0.230 - 15s 386ms/step - loss: 0.4144 - acc: 0.2242 - val_loss: 1.3709 - val_acc: 0.0968\n",
      "Epoch 85/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.4228 - acc: 0.222 - ETA: 1s - loss: 0.3987 - acc: 0.225 - 16s 391ms/step - loss: 0.4137 - acc: 0.2290 - val_loss: 1.3263 - val_acc: 0.0984\n",
      "Epoch 86/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.3835 - acc: 0.192 - ETA: 1s - loss: 0.3653 - acc: 0.214 - 16s 390ms/step - loss: 0.3981 - acc: 0.2355 - val_loss: 1.3576 - val_acc: 0.1032\n",
      "Epoch 87/1000\n",
      "40/40 [==============================] - ETA: 2s - loss: 0.4781 - acc: 0.246 - ETA: 0s - loss: 0.3833 - acc: 0.219 - 16s 392ms/step - loss: 0.3934 - acc: 0.2383 - val_loss: 1.3953 - val_acc: 0.0968\n",
      "Epoch 88/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.3508 - acc: 0.214 - ETA: 1s - loss: 0.3709 - acc: 0.239 - 16s 411ms/step - loss: 0.3852 - acc: 0.2339 - val_loss: 1.3128 - val_acc: 0.0984\n",
      "Epoch 89/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.4019 - acc: 0.244 - ETA: 1s - loss: 0.3866 - acc: 0.224 - 15s 387ms/step - loss: 0.4012 - acc: 0.2359 - val_loss: 1.3075 - val_acc: 0.0968\n",
      "Epoch 90/1000\n",
      "40/40 [==============================] - ETA: 2s - loss: 0.3621 - acc: 0.181 - ETA: 1s - loss: 0.3949 - acc: 0.221 - 16s 398ms/step - loss: 0.3934 - acc: 0.2351 - val_loss: 1.2737 - val_acc: 0.1081\n",
      "Epoch 91/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.3851 - acc: 0.224 - ETA: 1s - loss: 0.3600 - acc: 0.222 - 16s 400ms/step - loss: 0.3925 - acc: 0.2331 - val_loss: 1.3542 - val_acc: 0.1016\n",
      "Epoch 92/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.3905 - acc: 0.259 - ETA: 1s - loss: 0.3818 - acc: 0.257 - 15s 384ms/step - loss: 0.3711 - acc: 0.2448 - val_loss: 1.3083 - val_acc: 0.1016\n",
      "Epoch 93/1000\n",
      "40/40 [==============================] - ETA: 2s - loss: 0.3019 - acc: 0.216 - ETA: 1s - loss: 0.3531 - acc: 0.251 - 16s 394ms/step - loss: 0.3610 - acc: 0.2480 - val_loss: 1.3419 - val_acc: 0.1016\n",
      "Epoch 94/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.3992 - acc: 0.202 - ETA: 1s - loss: 0.4186 - acc: 0.246 - 16s 398ms/step - loss: 0.4075 - acc: 0.2290 - val_loss: 1.4155 - val_acc: 0.0935\n",
      "Epoch 95/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.4142 - acc: 0.224 - ETA: 1s - loss: 0.3828 - acc: 0.235 - 16s 393ms/step - loss: 0.3841 - acc: 0.2347 - val_loss: 1.3273 - val_acc: 0.1032\n",
      "Epoch 96/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.3583 - acc: 0.189 - ETA: 1s - loss: 0.3909 - acc: 0.239 - 16s 396ms/step - loss: 0.3747 - acc: 0.2435 - val_loss: 1.3827 - val_acc: 0.1032\n",
      "Epoch 97/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.3563 - acc: 0.207 - ETA: 1s - loss: 0.3380 - acc: 0.232 - 15s 385ms/step - loss: 0.3551 - acc: 0.2468 - val_loss: 1.3141 - val_acc: 0.1065\n",
      "Epoch 98/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.4112 - acc: 0.257 - ETA: 1s - loss: 0.3854 - acc: 0.238 - 16s 394ms/step - loss: 0.3800 - acc: 0.2363 - val_loss: 1.3630 - val_acc: 0.1000\n",
      "Epoch 99/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.4053 - acc: 0.210 - ETA: 1s - loss: 0.3638 - acc: 0.226 - 15s 377ms/step - loss: 0.3803 - acc: 0.2383 - val_loss: 1.4425 - val_acc: 0.1032\n",
      "Epoch 100/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.3412 - acc: 0.251 - ETA: 0s - loss: 0.3515 - acc: 0.252 - 15s 381ms/step - loss: 0.3683 - acc: 0.2492 - val_loss: 1.3548 - val_acc: 0.1081\n",
      "Epoch 101/1000\n",
      "40/40 [==============================] - ETA: 2s - loss: 0.4275 - acc: 0.284 - ETA: 0s - loss: 0.3784 - acc: 0.260 - 15s 374ms/step - loss: 0.3613 - acc: 0.2492 - val_loss: 1.3596 - val_acc: 0.1000\n",
      "Epoch 102/1000\n",
      "40/40 [==============================] - ETA: 2s - loss: 0.3965 - acc: 0.302 - ETA: 1s - loss: 0.3508 - acc: 0.248 - 16s 390ms/step - loss: 0.3501 - acc: 0.2440 - val_loss: 1.4665 - val_acc: 0.0968\n",
      "Epoch 103/1000\n",
      "40/40 [==============================] - ETA: 2s - loss: 0.3544 - acc: 0.231 - ETA: 1s - loss: 0.3670 - acc: 0.246 - 15s 387ms/step - loss: 0.3613 - acc: 0.2427 - val_loss: 1.3756 - val_acc: 0.0968\n",
      "Epoch 104/1000\n",
      "40/40 [==============================] - ETA: 2s - loss: 0.3868 - acc: 0.274 - ETA: 1s - loss: 0.3561 - acc: 0.252 - 15s 380ms/step - loss: 0.3584 - acc: 0.2464 - val_loss: 1.4237 - val_acc: 0.0919\n",
      "Epoch 105/1000\n",
      "40/40 [==============================] - ETA: 2s - loss: 0.3556 - acc: 0.255 - ETA: 0s - loss: 0.3681 - acc: 0.255 - 15s 378ms/step - loss: 0.3540 - acc: 0.2468 - val_loss: 1.3926 - val_acc: 0.1016\n",
      "Epoch 106/1000\n",
      "40/40 [==============================] - ETA: 2s - loss: 0.3305 - acc: 0.209 - ETA: 0s - loss: 0.3344 - acc: 0.242 - 15s 381ms/step - loss: 0.3590 - acc: 0.2540 - val_loss: 1.2927 - val_acc: 0.0952\n",
      "Epoch 107/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.3969 - acc: 0.259 - ETA: 1s - loss: 0.3855 - acc: 0.272 - 15s 384ms/step - loss: 0.3620 - acc: 0.2472 - val_loss: 1.3359 - val_acc: 0.0952\n",
      "Epoch 108/1000\n",
      "40/40 [==============================] - ETA: 2s - loss: 0.3599 - acc: 0.255 - ETA: 1s - loss: 0.3479 - acc: 0.250 - 15s 385ms/step - loss: 0.3437 - acc: 0.2532 - val_loss: 1.3763 - val_acc: 0.1048\n",
      "Epoch 109/1000\n",
      "40/40 [==============================] - ETA: 2s - loss: 0.2770 - acc: 0.216 - ETA: 0s - loss: 0.3103 - acc: 0.225 - 15s 384ms/step - loss: 0.3418 - acc: 0.2536 - val_loss: 1.3085 - val_acc: 0.1032\n",
      "Epoch 110/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.3405 - acc: 0.195 - ETA: 1s - loss: 0.3465 - acc: 0.236 - 16s 401ms/step - loss: 0.3491 - acc: 0.2476 - val_loss: 1.3597 - val_acc: 0.0984\n",
      "Epoch 111/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.3647 - acc: 0.270 - ETA: 0s - loss: 0.3545 - acc: 0.263 - 16s 391ms/step - loss: 0.3446 - acc: 0.2472 - val_loss: 1.3829 - val_acc: 0.1016\n",
      "Epoch 112/1000\n",
      "40/40 [==============================] - ETA: 2s - loss: 0.3287 - acc: 0.259 - ETA: 0s - loss: 0.3318 - acc: 0.266 - 16s 388ms/step - loss: 0.3281 - acc: 0.2556 - val_loss: 1.4885 - val_acc: 0.0968\n",
      "Epoch 113/1000\n",
      "40/40 [==============================] - ETA: 2s - loss: 0.3204 - acc: 0.226 - ETA: 0s - loss: 0.3243 - acc: 0.231 - 15s 386ms/step - loss: 0.3369 - acc: 0.2468 - val_loss: 1.3671 - val_acc: 0.0984\n",
      "Epoch 114/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.3645 - acc: 0.255 - ETA: 1s - loss: 0.3364 - acc: 0.242 - 15s 386ms/step - loss: 0.3475 - acc: 0.2472 - val_loss: 1.4022 - val_acc: 0.0968\n",
      "Epoch 115/1000\n",
      "40/40 [==============================] - ETA: 2s - loss: 0.3217 - acc: 0.211 - ETA: 1s - loss: 0.3347 - acc: 0.265 - 15s 385ms/step - loss: 0.3222 - acc: 0.2528 - val_loss: 1.3920 - val_acc: 0.0903\n",
      "Epoch 116/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40/40 [==============================] - ETA: 2s - loss: 0.3494 - acc: 0.279 - ETA: 0s - loss: 0.3305 - acc: 0.265 - 15s 378ms/step - loss: 0.3211 - acc: 0.2597 - val_loss: 1.3437 - val_acc: 0.1097\n",
      "Epoch 117/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.3404 - acc: 0.282 - ETA: 0s - loss: 0.3252 - acc: 0.274 - 16s 389ms/step - loss: 0.3134 - acc: 0.2597 - val_loss: 1.3924 - val_acc: 0.1048\n",
      "Epoch 118/1000\n",
      "40/40 [==============================] - ETA: 2s - loss: 0.2658 - acc: 0.263 - ETA: 0s - loss: 0.2876 - acc: 0.258 - 16s 390ms/step - loss: 0.3146 - acc: 0.2629 - val_loss: 1.3417 - val_acc: 0.0952\n",
      "Epoch 119/1000\n",
      "40/40 [==============================] - ETA: 2s - loss: 0.3319 - acc: 0.255 - ETA: 0s - loss: 0.3330 - acc: 0.249 - 15s 387ms/step - loss: 0.3439 - acc: 0.2488 - val_loss: 1.4393 - val_acc: 0.0887\n",
      "Epoch 120/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.3870 - acc: 0.285 - ETA: 1s - loss: 0.3443 - acc: 0.260 - 15s 381ms/step - loss: 0.3274 - acc: 0.2512 - val_loss: 1.4381 - val_acc: 0.0984\n",
      "Epoch 121/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.2983 - acc: 0.274 - ETA: 1s - loss: 0.3065 - acc: 0.275 - 16s 400ms/step - loss: 0.3028 - acc: 0.2645 - val_loss: 1.4434 - val_acc: 0.0935\n",
      "Epoch 122/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.2760 - acc: 0.257 - ETA: 0s - loss: 0.3098 - acc: 0.269 - 15s 387ms/step - loss: 0.3158 - acc: 0.2597 - val_loss: 1.4196 - val_acc: 0.0968\n",
      "Epoch 123/1000\n",
      "40/40 [==============================] - ETA: 2s - loss: 0.3543 - acc: 0.267 - ETA: 1s - loss: 0.3161 - acc: 0.245 - 16s 391ms/step - loss: 0.3248 - acc: 0.2637 - val_loss: 1.5684 - val_acc: 0.0935\n",
      "Epoch 124/1000\n",
      "40/40 [==============================] - ETA: 2s - loss: 0.2911 - acc: 0.295 - ETA: 1s - loss: 0.2858 - acc: 0.267 - 16s 391ms/step - loss: 0.2983 - acc: 0.2677 - val_loss: 1.4911 - val_acc: 0.0952\n",
      "Epoch 125/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.2994 - acc: 0.238 - ETA: 1s - loss: 0.3141 - acc: 0.247 - 16s 396ms/step - loss: 0.3300 - acc: 0.2484 - val_loss: 1.3959 - val_acc: 0.1000\n",
      "Epoch 126/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.3254 - acc: 0.259 - ETA: 1s - loss: 0.3403 - acc: 0.273 - 16s 404ms/step - loss: 0.3248 - acc: 0.2605 - val_loss: 1.4624 - val_acc: 0.0919\n",
      "Epoch 127/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.2800 - acc: 0.236 - ETA: 1s - loss: 0.3057 - acc: 0.251 - 15s 385ms/step - loss: 0.3080 - acc: 0.2589 - val_loss: 1.5288 - val_acc: 0.0919\n",
      "Epoch 128/1000\n",
      "40/40 [==============================] - ETA: 2s - loss: 0.3111 - acc: 0.270 - ETA: 0s - loss: 0.2903 - acc: 0.290 - 16s 394ms/step - loss: 0.2842 - acc: 0.2710 - val_loss: 1.4470 - val_acc: 0.0919\n",
      "Epoch 129/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.2709 - acc: 0.248 - ETA: 1s - loss: 0.2898 - acc: 0.264 - 16s 388ms/step - loss: 0.3014 - acc: 0.2653 - val_loss: 1.4561 - val_acc: 0.0919\n",
      "Epoch 130/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.3254 - acc: 0.277 - ETA: 1s - loss: 0.3180 - acc: 0.248 - 15s 387ms/step - loss: 0.3207 - acc: 0.2589 - val_loss: 1.4627 - val_acc: 0.1000\n",
      "Epoch 131/1000\n",
      "40/40 [==============================] - ETA: 2s - loss: 0.3116 - acc: 0.223 - ETA: 0s - loss: 0.3111 - acc: 0.234 - 16s 390ms/step - loss: 0.3122 - acc: 0.2556 - val_loss: 1.4800 - val_acc: 0.0968\n",
      "Epoch 132/1000\n",
      "40/40 [==============================] - ETA: 2s - loss: 0.3139 - acc: 0.283 - ETA: 1s - loss: 0.2832 - acc: 0.257 - 15s 377ms/step - loss: 0.2987 - acc: 0.2649 - val_loss: 1.4563 - val_acc: 0.1016\n",
      "Epoch 133/1000\n",
      "40/40 [==============================] - ETA: 2s - loss: 0.2535 - acc: 0.274 - ETA: 0s - loss: 0.2798 - acc: 0.284 - 15s 376ms/step - loss: 0.2881 - acc: 0.2649 - val_loss: 1.3569 - val_acc: 0.0887\n",
      "Epoch 134/1000\n",
      "40/40 [==============================] - ETA: 2s - loss: 0.3048 - acc: 0.293 - ETA: 1s - loss: 0.3161 - acc: 0.280 - 15s 381ms/step - loss: 0.3142 - acc: 0.2645 - val_loss: 1.4715 - val_acc: 0.0871\n",
      "Epoch 135/1000\n",
      "40/40 [==============================] - ETA: 2s - loss: 0.2956 - acc: 0.203 - ETA: 1s - loss: 0.2939 - acc: 0.250 - 15s 381ms/step - loss: 0.2992 - acc: 0.2577 - val_loss: 1.4533 - val_acc: 0.0839\n",
      "Epoch 136/1000\n",
      "40/40 [==============================] - ETA: 2s - loss: 0.3257 - acc: 0.344 - ETA: 0s - loss: 0.2827 - acc: 0.261 - 15s 377ms/step - loss: 0.2893 - acc: 0.2673 - val_loss: 1.4686 - val_acc: 0.0919\n",
      "Epoch 137/1000\n",
      "40/40 [==============================] - ETA: 2s - loss: 0.3153 - acc: 0.266 - ETA: 0s - loss: 0.2995 - acc: 0.255 - 15s 378ms/step - loss: 0.3046 - acc: 0.2577 - val_loss: 1.4731 - val_acc: 0.0984\n",
      "Epoch 138/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.2873 - acc: 0.309 - ETA: 1s - loss: 0.2742 - acc: 0.261 - 16s 393ms/step - loss: 0.2802 - acc: 0.2726 - val_loss: 1.4231 - val_acc: 0.1032\n",
      "Epoch 139/1000\n",
      "40/40 [==============================] - ETA: 2s - loss: 0.2855 - acc: 0.310 - ETA: 0s - loss: 0.2617 - acc: 0.265 - 15s 386ms/step - loss: 0.2682 - acc: 0.2742 - val_loss: 1.4936 - val_acc: 0.0919\n",
      "Epoch 140/1000\n",
      "40/40 [==============================] - ETA: 2s - loss: 0.3125 - acc: 0.309 - ETA: 1s - loss: 0.2905 - acc: 0.278 - 15s 383ms/step - loss: 0.2869 - acc: 0.2706 - val_loss: 1.5074 - val_acc: 0.1048\n",
      "Epoch 141/1000\n",
      "40/40 [==============================] - ETA: 2s - loss: 0.2679 - acc: 0.264 - ETA: 0s - loss: 0.2598 - acc: 0.263 - 15s 379ms/step - loss: 0.2740 - acc: 0.2754 - val_loss: 1.4749 - val_acc: 0.0903\n",
      "Epoch 142/1000\n",
      "40/40 [==============================] - ETA: 2s - loss: 0.3434 - acc: 0.278 - ETA: 1s - loss: 0.3064 - acc: 0.269 - 15s 386ms/step - loss: 0.3110 - acc: 0.2649 - val_loss: 1.4700 - val_acc: 0.0968\n",
      "Epoch 143/1000\n",
      "40/40 [==============================] - ETA: 2s - loss: 0.3049 - acc: 0.285 - ETA: 0s - loss: 0.2831 - acc: 0.283 - 15s 376ms/step - loss: 0.2790 - acc: 0.2706 - val_loss: 1.5809 - val_acc: 0.0871\n",
      "Epoch 144/1000\n",
      "40/40 [==============================] - ETA: 2s - loss: 0.2585 - acc: 0.206 - ETA: 1s - loss: 0.2918 - acc: 0.254 - 15s 386ms/step - loss: 0.2834 - acc: 0.2673 - val_loss: 1.5932 - val_acc: 0.0903\n",
      "Epoch 145/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.2612 - acc: 0.239 - ETA: 1s - loss: 0.2735 - acc: 0.274 - 15s 375ms/step - loss: 0.2762 - acc: 0.2754 - val_loss: 1.5089 - val_acc: 0.1048\n",
      "Epoch 146/1000\n",
      "40/40 [==============================] - ETA: 2s - loss: 0.2695 - acc: 0.327 - ETA: 1s - loss: 0.2650 - acc: 0.270 - 15s 387ms/step - loss: 0.2775 - acc: 0.2754 - val_loss: 1.5449 - val_acc: 0.0968\n",
      "Epoch 147/1000\n",
      "40/40 [==============================] - ETA: 2s - loss: 0.3332 - acc: 0.317 - ETA: 0s - loss: 0.2773 - acc: 0.279 - 15s 378ms/step - loss: 0.2763 - acc: 0.2730 - val_loss: 1.6071 - val_acc: 0.1000\n",
      "Epoch 148/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.2801 - acc: 0.335 - ETA: 0s - loss: 0.2644 - acc: 0.299 - 15s 375ms/step - loss: 0.2640 - acc: 0.2770 - val_loss: 1.5609 - val_acc: 0.0968\n",
      "Epoch 149/1000\n",
      "40/40 [==============================] - ETA: 2s - loss: 0.2556 - acc: 0.270 - ETA: 0s - loss: 0.2659 - acc: 0.294 - 15s 383ms/step - loss: 0.2641 - acc: 0.2774 - val_loss: 1.5215 - val_acc: 0.1113\n",
      "Epoch 150/1000\n",
      "40/40 [==============================] - ETA: 2s - loss: 0.2507 - acc: 0.248 - ETA: 0s - loss: 0.2498 - acc: 0.276 - 15s 381ms/step - loss: 0.2511 - acc: 0.2819 - val_loss: 1.5834 - val_acc: 0.1032\n",
      "Epoch 151/1000\n",
      "40/40 [==============================] - ETA: 2s - loss: 0.2601 - acc: 0.324 - ETA: 0s - loss: 0.2792 - acc: 0.307 - 15s 380ms/step - loss: 0.2676 - acc: 0.2827 - val_loss: 1.4810 - val_acc: 0.0984\n",
      "Epoch 152/1000\n",
      "40/40 [==============================] - ETA: 2s - loss: 0.2233 - acc: 0.261 - ETA: 0s - loss: 0.2505 - acc: 0.291 - 15s 384ms/step - loss: 0.2529 - acc: 0.2843 - val_loss: 1.4959 - val_acc: 0.1032\n",
      "Epoch 153/1000\n",
      "40/40 [==============================] - ETA: 2s - loss: 0.2901 - acc: 0.298 - ETA: 0s - loss: 0.2583 - acc: 0.278 - 15s 385ms/step - loss: 0.2602 - acc: 0.2794 - val_loss: 1.5819 - val_acc: 0.0935\n",
      "Epoch 154/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40/40 [==============================] - ETA: 2s - loss: 0.2625 - acc: 0.248 - ETA: 1s - loss: 0.2652 - acc: 0.264 - 16s 407ms/step - loss: 0.2613 - acc: 0.2774 - val_loss: 1.5514 - val_acc: 0.0919\n",
      "Epoch 155/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.2466 - acc: 0.268 - ETA: 1s - loss: 0.2582 - acc: 0.273 - 16s 404ms/step - loss: 0.2538 - acc: 0.2819 - val_loss: 1.5071 - val_acc: 0.0935\n",
      "Epoch 156/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.2435 - acc: 0.284 - ETA: 1s - loss: 0.2527 - acc: 0.287 - 18s 450ms/step - loss: 0.2538 - acc: 0.2823 - val_loss: 1.6060 - val_acc: 0.0919\n",
      "Epoch 157/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.3292 - acc: 0.348 - ETA: 1s - loss: 0.2909 - acc: 0.284 - 18s 444ms/step - loss: 0.2691 - acc: 0.2710 - val_loss: 1.6272 - val_acc: 0.0887\n",
      "Epoch 158/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.2600 - acc: 0.278 - ETA: 1s - loss: 0.2844 - acc: 0.286 - 17s 437ms/step - loss: 0.2712 - acc: 0.2782 - val_loss: 1.5345 - val_acc: 0.0919\n",
      "Epoch 159/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.2363 - acc: 0.261 - ETA: 1s - loss: 0.2432 - acc: 0.262 - 17s 433ms/step - loss: 0.2557 - acc: 0.2810 - val_loss: 1.5009 - val_acc: 0.0952\n",
      "Epoch 160/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.2187 - acc: 0.216 - ETA: 1s - loss: 0.2403 - acc: 0.257 - 16s 389ms/step - loss: 0.2531 - acc: 0.2738 - val_loss: 1.5357 - val_acc: 0.0903\n",
      "Epoch 161/1000\n",
      "40/40 [==============================] - ETA: 2s - loss: 0.2363 - acc: 0.229 - ETA: 0s - loss: 0.2752 - acc: 0.286 - 16s 392ms/step - loss: 0.2729 - acc: 0.2726 - val_loss: 1.5093 - val_acc: 0.0952\n",
      "Epoch 162/1000\n",
      "40/40 [==============================] - ETA: 2s - loss: 0.2770 - acc: 0.316 - ETA: 0s - loss: 0.2696 - acc: 0.289 - 17s 417ms/step - loss: 0.2676 - acc: 0.2754 - val_loss: 1.6174 - val_acc: 0.1000\n",
      "Epoch 163/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.2536 - acc: 0.281 - ETA: 1s - loss: 0.2694 - acc: 0.297 - 18s 455ms/step - loss: 0.2552 - acc: 0.2827 - val_loss: 1.5334 - val_acc: 0.1000\n",
      "Epoch 164/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.2622 - acc: 0.294 - ETA: 1s - loss: 0.2562 - acc: 0.295 - 16s 392ms/step - loss: 0.2494 - acc: 0.2847 - val_loss: 1.5599 - val_acc: 0.1081\n",
      "Epoch 165/1000\n",
      "40/40 [==============================] - ETA: 2s - loss: 0.2298 - acc: 0.289 - ETA: 1s - loss: 0.2260 - acc: 0.288 - 15s 385ms/step - loss: 0.2354 - acc: 0.2895 - val_loss: 1.5057 - val_acc: 0.1016\n",
      "Epoch 166/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.2590 - acc: 0.316 - ETA: 1s - loss: 0.2393 - acc: 0.279 - 16s 403ms/step - loss: 0.2403 - acc: 0.2863 - val_loss: 1.4778 - val_acc: 0.1000\n",
      "Epoch 167/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.2146 - acc: 0.265 - ETA: 0s - loss: 0.2222 - acc: 0.265 - 16s 390ms/step - loss: 0.2408 - acc: 0.2903 - val_loss: 1.5488 - val_acc: 0.0984\n",
      "Epoch 168/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.2832 - acc: 0.247 - ETA: 1s - loss: 0.2696 - acc: 0.300 - 16s 402ms/step - loss: 0.2515 - acc: 0.2867 - val_loss: 1.5786 - val_acc: 0.1000\n",
      "Epoch 169/1000\n",
      "40/40 [==============================] - ETA: 2s - loss: 0.2567 - acc: 0.344 - ETA: 0s - loss: 0.2454 - acc: 0.290 - 15s 384ms/step - loss: 0.2381 - acc: 0.2903 - val_loss: 1.5793 - val_acc: 0.0952\n",
      "Epoch 170/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.2755 - acc: 0.279 - ETA: 1s - loss: 0.2523 - acc: 0.271 - 16s 398ms/step - loss: 0.2552 - acc: 0.2770 - val_loss: 1.5262 - val_acc: 0.0919\n",
      "Epoch 171/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.2943 - acc: 0.214 - ETA: 1s - loss: 0.2963 - acc: 0.286 - 18s 462ms/step - loss: 0.2723 - acc: 0.2762 - val_loss: 1.5403 - val_acc: 0.0968\n",
      "Epoch 172/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.2520 - acc: 0.273 - ETA: 1s - loss: 0.2514 - acc: 0.299 - 16s 396ms/step - loss: 0.2518 - acc: 0.2798 - val_loss: 1.5794 - val_acc: 0.1016\n",
      "Epoch 173/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.2232 - acc: 0.300 - ETA: 1s - loss: 0.2352 - acc: 0.284 - 16s 401ms/step - loss: 0.2260 - acc: 0.2851 - val_loss: 1.5975 - val_acc: 0.1000\n",
      "Epoch 174/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.2851 - acc: 0.283 - ETA: 1s - loss: 0.2625 - acc: 0.279 - 16s 391ms/step - loss: 0.2612 - acc: 0.2794 - val_loss: 1.4664 - val_acc: 0.1016\n",
      "Epoch 175/1000\n",
      "40/40 [==============================] - ETA: 2s - loss: 0.2529 - acc: 0.306 - ETA: 1s - loss: 0.2429 - acc: 0.289 - 16s 389ms/step - loss: 0.2311 - acc: 0.2843 - val_loss: 1.5685 - val_acc: 0.0984\n",
      "Epoch 176/1000\n",
      "40/40 [==============================] - ETA: 2s - loss: 0.2432 - acc: 0.303 - ETA: 1s - loss: 0.2318 - acc: 0.287 - 16s 406ms/step - loss: 0.2267 - acc: 0.2907 - val_loss: 1.5973 - val_acc: 0.0968\n",
      "Epoch 177/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.2194 - acc: 0.201 - ETA: 1s - loss: 0.2225 - acc: 0.277 - 18s 459ms/step - loss: 0.2252 - acc: 0.2839 - val_loss: 1.6434 - val_acc: 0.0952\n",
      "Epoch 178/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.2193 - acc: 0.272 - ETA: 1s - loss: 0.2204 - acc: 0.290 - 17s 413ms/step - loss: 0.2294 - acc: 0.2810 - val_loss: 1.6435 - val_acc: 0.0952\n",
      "Epoch 179/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.2584 - acc: 0.316 - ETA: 1s - loss: 0.2472 - acc: 0.277 - 18s 440ms/step - loss: 0.2444 - acc: 0.2835 - val_loss: 1.6804 - val_acc: 0.0919\n",
      "Epoch 180/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.2612 - acc: 0.319 - ETA: 1s - loss: 0.2545 - acc: 0.278 - 16s 408ms/step - loss: 0.2550 - acc: 0.2758 - val_loss: 1.6742 - val_acc: 0.1000\n",
      "Epoch 181/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.2542 - acc: 0.372 - ETA: 1s - loss: 0.2274 - acc: 0.283 - 16s 400ms/step - loss: 0.2270 - acc: 0.2883 - val_loss: 1.6467 - val_acc: 0.1032\n",
      "Epoch 182/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.2108 - acc: 0.281 - ETA: 1s - loss: 0.2334 - acc: 0.260 - 16s 398ms/step - loss: 0.2359 - acc: 0.2839 - val_loss: 1.5901 - val_acc: 0.0984\n",
      "Epoch 183/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.2447 - acc: 0.268 - ETA: 1s - loss: 0.2440 - acc: 0.278 - 16s 408ms/step - loss: 0.2415 - acc: 0.2806 - val_loss: 1.6338 - val_acc: 0.0968\n",
      "Epoch 184/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.2151 - acc: 0.259 - ETA: 1s - loss: 0.2182 - acc: 0.268 - 16s 395ms/step - loss: 0.2195 - acc: 0.2899 - val_loss: 1.6567 - val_acc: 0.1016\n",
      "Epoch 185/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.2152 - acc: 0.262 - ETA: 1s - loss: 0.2197 - acc: 0.280 - 16s 405ms/step - loss: 0.2198 - acc: 0.2859 - val_loss: 1.6085 - val_acc: 0.0984\n",
      "Epoch 186/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.2307 - acc: 0.253 - ETA: 1s - loss: 0.2591 - acc: 0.283 - 15s 387ms/step - loss: 0.2547 - acc: 0.2835 - val_loss: 1.5517 - val_acc: 0.0984\n",
      "Epoch 187/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.2315 - acc: 0.237 - ETA: 1s - loss: 0.2514 - acc: 0.274 - 16s 403ms/step - loss: 0.2537 - acc: 0.2794 - val_loss: 1.4847 - val_acc: 0.0968\n",
      "Epoch 188/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.2248 - acc: 0.311 - ETA: 1s - loss: 0.2146 - acc: 0.293 - 19s 475ms/step - loss: 0.2170 - acc: 0.2931 - val_loss: 1.5443 - val_acc: 0.0984\n",
      "Epoch 189/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.1998 - acc: 0.270 - ETA: 1s - loss: 0.2021 - acc: 0.260 - 19s 477ms/step - loss: 0.2164 - acc: 0.2899 - val_loss: 1.5953 - val_acc: 0.1048\n",
      "Epoch 190/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.2176 - acc: 0.289 - ETA: 1s - loss: 0.2204 - acc: 0.294 - 18s 439ms/step - loss: 0.2197 - acc: 0.2903 - val_loss: 1.5729 - val_acc: 0.0968\n",
      "Epoch 191/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.2372 - acc: 0.345 - ETA: 1s - loss: 0.2203 - acc: 0.296 - 18s 450ms/step - loss: 0.2168 - acc: 0.2907 - val_loss: 1.5864 - val_acc: 0.1000\n",
      "Epoch 192/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40/40 [==============================] - ETA: 3s - loss: 0.1970 - acc: 0.279 - ETA: 1s - loss: 0.2101 - acc: 0.297 - 18s 438ms/step - loss: 0.2100 - acc: 0.2907 - val_loss: 1.5525 - val_acc: 0.1032\n",
      "Epoch 193/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.2387 - acc: 0.283 - ETA: 1s - loss: 0.2194 - acc: 0.298 - 18s 450ms/step - loss: 0.2206 - acc: 0.2899 - val_loss: 1.7148 - val_acc: 0.0871\n",
      "Epoch 194/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.2042 - acc: 0.290 - ETA: 1s - loss: 0.2154 - acc: 0.290 - 18s 454ms/step - loss: 0.2184 - acc: 0.2919 - val_loss: 1.6146 - val_acc: 0.0968\n",
      "Epoch 195/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.1960 - acc: 0.263 - ETA: 1s - loss: 0.1960 - acc: 0.270 - 18s 457ms/step - loss: 0.1959 - acc: 0.2992 - val_loss: 1.6598 - val_acc: 0.0968\n",
      "Epoch 196/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.1984 - acc: 0.217 - ETA: 1s - loss: 0.1994 - acc: 0.248 - 18s 440ms/step - loss: 0.2195 - acc: 0.2863 - val_loss: 1.6717 - val_acc: 0.1000\n",
      "Epoch 197/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.2365 - acc: 0.274 - ETA: 1s - loss: 0.2240 - acc: 0.296 - 18s 444ms/step - loss: 0.2165 - acc: 0.2851 - val_loss: 1.6338 - val_acc: 0.1032\n",
      "Epoch 198/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.2116 - acc: 0.282 - ETA: 1s - loss: 0.2071 - acc: 0.294 - 17s 434ms/step - loss: 0.2067 - acc: 0.2923 - val_loss: 1.6840 - val_acc: 0.0839\n",
      "Epoch 199/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.2032 - acc: 0.273 - ETA: 1s - loss: 0.2056 - acc: 0.308 - 18s 444ms/step - loss: 0.2046 - acc: 0.2956 - val_loss: 1.6828 - val_acc: 0.0984\n",
      "Epoch 200/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.2004 - acc: 0.258 - ETA: 1s - loss: 0.2008 - acc: 0.294 - 18s 442ms/step - loss: 0.2078 - acc: 0.2976 - val_loss: 1.6318 - val_acc: 0.0984\n",
      "Epoch 201/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.1929 - acc: 0.305 - ETA: 1s - loss: 0.1972 - acc: 0.314 - 18s 447ms/step - loss: 0.2002 - acc: 0.2992 - val_loss: 1.6811 - val_acc: 0.0919\n",
      "Epoch 202/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.1991 - acc: 0.240 - ETA: 1s - loss: 0.2023 - acc: 0.295 - 18s 445ms/step - loss: 0.2041 - acc: 0.2964 - val_loss: 1.6587 - val_acc: 0.0903\n",
      "Epoch 203/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.2201 - acc: 0.292 - ETA: 1s - loss: 0.2197 - acc: 0.275 - 18s 445ms/step - loss: 0.2170 - acc: 0.2891 - val_loss: 1.6249 - val_acc: 0.0919\n",
      "Epoch 204/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.2026 - acc: 0.277 - ETA: 1s - loss: 0.2055 - acc: 0.291 - 18s 458ms/step - loss: 0.2134 - acc: 0.2923 - val_loss: 1.5762 - val_acc: 0.1000\n",
      "Epoch 205/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.1835 - acc: 0.334 - ETA: 1s - loss: 0.1933 - acc: 0.320 - 18s 442ms/step - loss: 0.1901 - acc: 0.2976 - val_loss: 1.6438 - val_acc: 0.1032\n",
      "Epoch 206/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.2014 - acc: 0.329 - ETA: 1s - loss: 0.1972 - acc: 0.312 - 16s 407ms/step - loss: 0.1925 - acc: 0.2988 - val_loss: 1.6104 - val_acc: 0.0968\n",
      "Epoch 207/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.2055 - acc: 0.294 - ETA: 1s - loss: 0.2037 - acc: 0.281 - 15s 383ms/step - loss: 0.1984 - acc: 0.2944 - val_loss: 1.6024 - val_acc: 0.0919\n",
      "Epoch 208/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.1750 - acc: 0.270 - ETA: 0s - loss: 0.1906 - acc: 0.294 - 15s 387ms/step - loss: 0.1895 - acc: 0.3020 - val_loss: 1.6736 - val_acc: 0.1000\n",
      "Epoch 209/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.1982 - acc: 0.318 - ETA: 1s - loss: 0.1942 - acc: 0.302 - 16s 393ms/step - loss: 0.1976 - acc: 0.2996 - val_loss: 1.6655 - val_acc: 0.0855\n",
      "Epoch 210/1000\n",
      "40/40 [==============================] - ETA: 2s - loss: 0.1955 - acc: 0.281 - ETA: 1s - loss: 0.2049 - acc: 0.310 - 16s 392ms/step - loss: 0.1968 - acc: 0.3000 - val_loss: 1.7444 - val_acc: 0.0919\n",
      "Epoch 211/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.2008 - acc: 0.266 - ETA: 1s - loss: 0.1897 - acc: 0.277 - 16s 390ms/step - loss: 0.1987 - acc: 0.2972 - val_loss: 1.6977 - val_acc: 0.0871\n",
      "Epoch 212/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.2033 - acc: 0.271 - ETA: 1s - loss: 0.2058 - acc: 0.280 - 15s 384ms/step - loss: 0.2173 - acc: 0.2891 - val_loss: 1.6237 - val_acc: 0.0952\n",
      "Epoch 213/1000\n",
      "40/40 [==============================] - ETA: 2s - loss: 0.2272 - acc: 0.332 - ETA: 1s - loss: 0.2127 - acc: 0.295 - 16s 410ms/step - loss: 0.2126 - acc: 0.2948 - val_loss: 1.9055 - val_acc: 0.0758\n",
      "Epoch 214/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.2419 - acc: 0.313 - ETA: 1s - loss: 0.2236 - acc: 0.291 - 16s 392ms/step - loss: 0.2271 - acc: 0.2855 - val_loss: 1.6847 - val_acc: 0.1016\n",
      "Epoch 215/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.1897 - acc: 0.293 - ETA: 1s - loss: 0.1911 - acc: 0.299 - 16s 391ms/step - loss: 0.1950 - acc: 0.3004 - val_loss: 1.6491 - val_acc: 0.0968\n",
      "Epoch 216/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.1914 - acc: 0.277 - ETA: 1s - loss: 0.1804 - acc: 0.320 - 17s 414ms/step - loss: 0.1840 - acc: 0.3036 - val_loss: 1.6312 - val_acc: 0.0919\n",
      "Epoch 217/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.1926 - acc: 0.341 - ETA: 1s - loss: 0.1882 - acc: 0.316 - 16s 406ms/step - loss: 0.1874 - acc: 0.3008 - val_loss: 1.6413 - val_acc: 0.0903\n",
      "Epoch 218/1000\n",
      "40/40 [==============================] - ETA: 2s - loss: 0.1857 - acc: 0.280 - ETA: 1s - loss: 0.1858 - acc: 0.294 - 15s 378ms/step - loss: 0.1869 - acc: 0.3024 - val_loss: 1.6479 - val_acc: 0.0839\n",
      "Epoch 219/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.2350 - acc: 0.328 - ETA: 1s - loss: 0.2166 - acc: 0.309 - 15s 380ms/step - loss: 0.2130 - acc: 0.2927 - val_loss: 1.6678 - val_acc: 0.0968\n",
      "Epoch 220/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.2060 - acc: 0.353 - ETA: 1s - loss: 0.1951 - acc: 0.300 - 15s 387ms/step - loss: 0.1926 - acc: 0.3000 - val_loss: 1.6137 - val_acc: 0.1000\n",
      "Epoch 221/1000\n",
      "40/40 [==============================] - ETA: 2s - loss: 0.1697 - acc: 0.290 - ETA: 0s - loss: 0.1766 - acc: 0.300 - 16s 388ms/step - loss: 0.1829 - acc: 0.3060 - val_loss: 1.7047 - val_acc: 0.0968\n",
      "Epoch 222/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.2082 - acc: 0.342 - ETA: 1s - loss: 0.1897 - acc: 0.301 - 16s 392ms/step - loss: 0.1935 - acc: 0.3000 - val_loss: 1.6741 - val_acc: 0.0952\n",
      "Epoch 223/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.2181 - acc: 0.303 - ETA: 1s - loss: 0.1931 - acc: 0.295 - 16s 402ms/step - loss: 0.1870 - acc: 0.3048 - val_loss: 1.6729 - val_acc: 0.0935\n",
      "Epoch 224/1000\n",
      "40/40 [==============================] - ETA: 2s - loss: 0.1782 - acc: 0.264 - ETA: 0s - loss: 0.1837 - acc: 0.299 - 15s 382ms/step - loss: 0.1918 - acc: 0.3008 - val_loss: 1.7058 - val_acc: 0.0871\n",
      "Epoch 225/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.2162 - acc: 0.333 - ETA: 1s - loss: 0.1934 - acc: 0.308 - 15s 378ms/step - loss: 0.1865 - acc: 0.3020 - val_loss: 1.7192 - val_acc: 0.0919\n",
      "Epoch 226/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.2149 - acc: 0.340 - ETA: 1s - loss: 0.1911 - acc: 0.313 - 15s 382ms/step - loss: 0.1892 - acc: 0.3000 - val_loss: 1.7723 - val_acc: 0.0871\n",
      "Epoch 227/1000\n",
      "40/40 [==============================] - ETA: 2s - loss: 0.1935 - acc: 0.238 - ETA: 0s - loss: 0.1970 - acc: 0.296 - 15s 387ms/step - loss: 0.2013 - acc: 0.2980 - val_loss: 1.6243 - val_acc: 0.0903\n",
      "Epoch 228/1000\n",
      "40/40 [==============================] - ETA: 2s - loss: 0.1777 - acc: 0.330 - ETA: 0s - loss: 0.1709 - acc: 0.305 - 15s 387ms/step - loss: 0.1729 - acc: 0.3073 - val_loss: 1.6805 - val_acc: 0.0952\n",
      "Epoch 229/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.2014 - acc: 0.279 - ETA: 1s - loss: 0.1927 - acc: 0.320 - 15s 382ms/step - loss: 0.1825 - acc: 0.3020 - val_loss: 1.6439 - val_acc: 0.0935\n",
      "Epoch 230/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40/40 [==============================] - ETA: 3s - loss: 0.1679 - acc: 0.349 - ETA: 1s - loss: 0.1661 - acc: 0.301 - 15s 385ms/step - loss: 0.1737 - acc: 0.3000 - val_loss: 1.6306 - val_acc: 0.0871\n",
      "Epoch 231/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.1816 - acc: 0.283 - ETA: 1s - loss: 0.1946 - acc: 0.313 - 15s 378ms/step - loss: 0.1870 - acc: 0.3024 - val_loss: 1.6215 - val_acc: 0.0952\n",
      "Epoch 232/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.1685 - acc: 0.295 - ETA: 1s - loss: 0.1765 - acc: 0.298 - 15s 387ms/step - loss: 0.1818 - acc: 0.3008 - val_loss: 1.7276 - val_acc: 0.0790\n",
      "Epoch 233/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.1903 - acc: 0.313 - ETA: 0s - loss: 0.1849 - acc: 0.310 - 15s 378ms/step - loss: 0.1824 - acc: 0.3024 - val_loss: 1.6347 - val_acc: 0.0984\n",
      "Epoch 234/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.1790 - acc: 0.294 - ETA: 1s - loss: 0.1812 - acc: 0.302 - 15s 385ms/step - loss: 0.1829 - acc: 0.3020 - val_loss: 1.6885 - val_acc: 0.0903\n",
      "Epoch 235/1000\n",
      "40/40 [==============================] - ETA: 2s - loss: 0.1586 - acc: 0.240 - ETA: 1s - loss: 0.1662 - acc: 0.319 - 16s 391ms/step - loss: 0.1754 - acc: 0.3048 - val_loss: 1.6793 - val_acc: 0.0968\n",
      "Epoch 236/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.1592 - acc: 0.267 - ETA: 1s - loss: 0.1694 - acc: 0.296 - 16s 391ms/step - loss: 0.1711 - acc: 0.3048 - val_loss: 1.6418 - val_acc: 0.0903\n",
      "Epoch 237/1000\n",
      "40/40 [==============================] - ETA: 2s - loss: 0.1811 - acc: 0.324 - ETA: 0s - loss: 0.1907 - acc: 0.319 - 16s 396ms/step - loss: 0.1877 - acc: 0.3048 - val_loss: 1.7547 - val_acc: 0.0871\n",
      "Epoch 238/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.2047 - acc: 0.389 - ETA: 1s - loss: 0.1936 - acc: 0.305 - 15s 384ms/step - loss: 0.1848 - acc: 0.3020 - val_loss: 1.6502 - val_acc: 0.0903\n",
      "Epoch 239/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.1777 - acc: 0.353 - ETA: 1s - loss: 0.1815 - acc: 0.331 - 16s 393ms/step - loss: 0.1705 - acc: 0.3028 - val_loss: 1.6864 - val_acc: 0.0984\n",
      "Epoch 240/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.1859 - acc: 0.317 - ETA: 1s - loss: 0.1686 - acc: 0.318 - 16s 402ms/step - loss: 0.1747 - acc: 0.3048 - val_loss: 1.7166 - val_acc: 0.0935\n",
      "Epoch 241/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.1833 - acc: 0.274 - ETA: 1s - loss: 0.1825 - acc: 0.297 - 16s 391ms/step - loss: 0.1843 - acc: 0.3016 - val_loss: 1.6746 - val_acc: 0.0968\n",
      "Epoch 242/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.1815 - acc: 0.290 - ETA: 1s - loss: 0.1780 - acc: 0.339 - 16s 389ms/step - loss: 0.1755 - acc: 0.3060 - val_loss: 1.6599 - val_acc: 0.0952\n",
      "Epoch 243/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.1782 - acc: 0.311 - ETA: 1s - loss: 0.1713 - acc: 0.291 - 16s 389ms/step - loss: 0.1708 - acc: 0.3101 - val_loss: 1.7084 - val_acc: 0.0919\n",
      "Epoch 244/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.1776 - acc: 0.326 - ETA: 1s - loss: 0.1731 - acc: 0.313 - 16s 388ms/step - loss: 0.1732 - acc: 0.3020 - val_loss: 1.7123 - val_acc: 0.0935\n",
      "Epoch 245/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.2208 - acc: 0.274 - ETA: 0s - loss: 0.1964 - acc: 0.316 - 16s 388ms/step - loss: 0.1875 - acc: 0.3020 - val_loss: 1.6822 - val_acc: 0.0855\n",
      "Epoch 246/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.1890 - acc: 0.293 - ETA: 1s - loss: 0.1862 - acc: 0.304 - 16s 391ms/step - loss: 0.1786 - acc: 0.3040 - val_loss: 1.7582 - val_acc: 0.1016\n",
      "Epoch 247/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.1795 - acc: 0.268 - ETA: 1s - loss: 0.1789 - acc: 0.283 - 16s 390ms/step - loss: 0.1803 - acc: 0.3020 - val_loss: 1.6589 - val_acc: 0.0935\n",
      "Epoch 248/1000\n",
      "40/40 [==============================] - ETA: 2s - loss: 0.1647 - acc: 0.302 - ETA: 0s - loss: 0.1700 - acc: 0.309 - 16s 389ms/step - loss: 0.1705 - acc: 0.3073 - val_loss: 1.7075 - val_acc: 0.0935\n",
      "Epoch 249/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.1790 - acc: 0.288 - ETA: 1s - loss: 0.1785 - acc: 0.289 - 18s 438ms/step - loss: 0.1806 - acc: 0.3008 - val_loss: 1.6946 - val_acc: 0.0935\n",
      "Epoch 250/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.1519 - acc: 0.230 - ETA: 1s - loss: 0.1851 - acc: 0.293 - 16s 403ms/step - loss: 0.1823 - acc: 0.3040 - val_loss: 1.7093 - val_acc: 0.0935\n",
      "Epoch 251/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.1552 - acc: 0.291 - ETA: 1s - loss: 0.1704 - acc: 0.279 - 15s 382ms/step - loss: 0.1750 - acc: 0.3060 - val_loss: 1.6810 - val_acc: 0.1032\n",
      "Epoch 252/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.1927 - acc: 0.338 - ETA: 1s - loss: 0.1835 - acc: 0.299 - 16s 393ms/step - loss: 0.1814 - acc: 0.3016 - val_loss: 1.7780 - val_acc: 0.1000\n",
      "Epoch 253/1000\n",
      "40/40 [==============================] - ETA: 2s - loss: 0.1790 - acc: 0.281 - ETA: 1s - loss: 0.1742 - acc: 0.328 - 15s 382ms/step - loss: 0.1685 - acc: 0.3065 - val_loss: 1.7214 - val_acc: 0.0968\n",
      "Epoch 254/1000\n",
      "40/40 [==============================] - ETA: 2s - loss: 0.1605 - acc: 0.335 - ETA: 0s - loss: 0.1625 - acc: 0.326 - 15s 383ms/step - loss: 0.1638 - acc: 0.3101 - val_loss: 1.7099 - val_acc: 0.0984\n",
      "Epoch 255/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.1801 - acc: 0.301 - ETA: 1s - loss: 0.1675 - acc: 0.305 - 15s 387ms/step - loss: 0.1621 - acc: 0.3097 - val_loss: 1.7022 - val_acc: 0.0935\n",
      "Epoch 256/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.1554 - acc: 0.323 - ETA: 1s - loss: 0.1655 - acc: 0.313 - 15s 386ms/step - loss: 0.1629 - acc: 0.3081 - val_loss: 1.6848 - val_acc: 0.1000\n",
      "Epoch 257/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.1525 - acc: 0.290 - ETA: 1s - loss: 0.1717 - acc: 0.323 - 15s 382ms/step - loss: 0.1673 - acc: 0.3109 - val_loss: 1.6862 - val_acc: 0.0952\n",
      "Epoch 258/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.1611 - acc: 0.366 - ETA: 1s - loss: 0.1614 - acc: 0.320 - 16s 394ms/step - loss: 0.1666 - acc: 0.3069 - val_loss: 1.7675 - val_acc: 0.0903\n",
      "Epoch 259/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.1730 - acc: 0.259 - ETA: 0s - loss: 0.1603 - acc: 0.269 - 16s 390ms/step - loss: 0.1693 - acc: 0.3024 - val_loss: 1.6821 - val_acc: 0.1097\n",
      "Epoch 260/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.1858 - acc: 0.320 - ETA: 1s - loss: 0.1745 - acc: 0.327 - 16s 390ms/step - loss: 0.1636 - acc: 0.3085 - val_loss: 1.6388 - val_acc: 0.1000\n",
      "Epoch 261/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.1489 - acc: 0.285 - ETA: 1s - loss: 0.1536 - acc: 0.297 - 15s 384ms/step - loss: 0.1561 - acc: 0.3101 - val_loss: 1.7543 - val_acc: 0.0984\n",
      "Epoch 262/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.1553 - acc: 0.326 - ETA: 1s - loss: 0.1520 - acc: 0.312 - 17s 421ms/step - loss: 0.1546 - acc: 0.3113 - val_loss: 1.7666 - val_acc: 0.1016\n",
      "Epoch 263/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.1583 - acc: 0.297 - ETA: 1s - loss: 0.1673 - acc: 0.327 - 18s 455ms/step - loss: 0.1628 - acc: 0.3113 - val_loss: 1.8060 - val_acc: 0.0968\n",
      "Epoch 264/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.1385 - acc: 0.307 - ETA: 1s - loss: 0.1592 - acc: 0.325 - 18s 457ms/step - loss: 0.1600 - acc: 0.3085 - val_loss: 1.7027 - val_acc: 0.0919\n",
      "Epoch 265/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.1502 - acc: 0.245 - ETA: 1s - loss: 0.1630 - acc: 0.309 - 16s 396ms/step - loss: 0.1603 - acc: 0.3056 - val_loss: 1.7531 - val_acc: 0.0968\n",
      "Epoch 266/1000\n",
      "40/40 [==============================] - ETA: 2s - loss: 0.1316 - acc: 0.296 - ETA: 1s - loss: 0.1471 - acc: 0.311 - 16s 402ms/step - loss: 0.1456 - acc: 0.3129 - val_loss: 1.7216 - val_acc: 0.0968\n",
      "Epoch 267/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.1430 - acc: 0.280 - ETA: 1s - loss: 0.1495 - acc: 0.304 - 16s 391ms/step - loss: 0.1517 - acc: 0.3085 - val_loss: 1.7016 - val_acc: 0.1000\n",
      "Epoch 268/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40/40 [==============================] - ETA: 3s - loss: 0.1879 - acc: 0.318 - ETA: 0s - loss: 0.1616 - acc: 0.328 - 15s 385ms/step - loss: 0.1577 - acc: 0.3069 - val_loss: 1.8101 - val_acc: 0.0952\n",
      "Epoch 269/1000\n",
      "40/40 [==============================] - ETA: 2s - loss: 0.1503 - acc: 0.324 - ETA: 0s - loss: 0.1598 - acc: 0.322 - 16s 389ms/step - loss: 0.1600 - acc: 0.3097 - val_loss: 1.7352 - val_acc: 0.1000\n",
      "Epoch 270/1000\n",
      "40/40 [==============================] - ETA: 2s - loss: 0.1724 - acc: 0.324 - ETA: 0s - loss: 0.1604 - acc: 0.343 - 15s 384ms/step - loss: 0.1561 - acc: 0.3101 - val_loss: 1.7610 - val_acc: 0.0919\n",
      "Epoch 271/1000\n",
      "40/40 [==============================] - ETA: 2s - loss: 0.1549 - acc: 0.254 - ETA: 1s - loss: 0.1602 - acc: 0.290 - 16s 389ms/step - loss: 0.1655 - acc: 0.3065 - val_loss: 1.7303 - val_acc: 0.1000\n",
      "Epoch 272/1000\n",
      "40/40 [==============================] - ETA: 2s - loss: 0.1812 - acc: 0.360 - ETA: 0s - loss: 0.1698 - acc: 0.334 - 15s 384ms/step - loss: 0.1619 - acc: 0.3125 - val_loss: 1.7309 - val_acc: 0.0968\n",
      "Epoch 273/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.1611 - acc: 0.298 - ETA: 1s - loss: 0.1611 - acc: 0.315 - 16s 394ms/step - loss: 0.1577 - acc: 0.3081 - val_loss: 1.7934 - val_acc: 0.0952\n",
      "Epoch 274/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.1632 - acc: 0.266 - ETA: 1s - loss: 0.1584 - acc: 0.296 - 16s 409ms/step - loss: 0.1545 - acc: 0.3085 - val_loss: 1.7556 - val_acc: 0.0968\n",
      "Epoch 275/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.1474 - acc: 0.252 - ETA: 1s - loss: 0.1425 - acc: 0.303 - 16s 390ms/step - loss: 0.1493 - acc: 0.3194 - val_loss: 1.7227 - val_acc: 0.0968\n",
      "Epoch 276/1000\n",
      "40/40 [==============================] - ETA: 2s - loss: 0.1555 - acc: 0.351 - ETA: 1s - loss: 0.1572 - acc: 0.313 - 15s 386ms/step - loss: 0.1654 - acc: 0.3085 - val_loss: 1.7339 - val_acc: 0.1065\n",
      "Epoch 277/1000\n",
      "40/40 [==============================] - ETA: 2s - loss: 0.1547 - acc: 0.272 - ETA: 1s - loss: 0.1496 - acc: 0.320 - 16s 389ms/step - loss: 0.1526 - acc: 0.3117 - val_loss: 1.8577 - val_acc: 0.0806\n",
      "Epoch 278/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.1440 - acc: 0.261 - ETA: 1s - loss: 0.1793 - acc: 0.300 - 15s 382ms/step - loss: 0.1732 - acc: 0.3044 - val_loss: 1.6768 - val_acc: 0.0903\n",
      "Epoch 279/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.1603 - acc: 0.286 - ETA: 1s - loss: 0.1545 - acc: 0.320 - 16s 391ms/step - loss: 0.1537 - acc: 0.3109 - val_loss: 1.6443 - val_acc: 0.1000\n",
      "Epoch 280/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.1400 - acc: 0.300 - ETA: 1s - loss: 0.1595 - acc: 0.304 - 16s 399ms/step - loss: 0.1603 - acc: 0.3069 - val_loss: 1.6955 - val_acc: 0.0839\n",
      "Epoch 281/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.1463 - acc: 0.366 - ETA: 1s - loss: 0.1521 - acc: 0.322 - 16s 388ms/step - loss: 0.1528 - acc: 0.3109 - val_loss: 1.7078 - val_acc: 0.0984\n",
      "Epoch 282/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.1369 - acc: 0.303 - ETA: 1s - loss: 0.1485 - acc: 0.325 - 16s 402ms/step - loss: 0.1407 - acc: 0.3169 - val_loss: 1.7620 - val_acc: 0.0903\n",
      "Epoch 283/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.1524 - acc: 0.312 - ETA: 1s - loss: 0.1458 - acc: 0.310 - 16s 402ms/step - loss: 0.1504 - acc: 0.3149 - val_loss: 1.7555 - val_acc: 0.0871\n",
      "Epoch 284/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.1678 - acc: 0.290 - ETA: 1s - loss: 0.1586 - acc: 0.306 - 15s 385ms/step - loss: 0.1582 - acc: 0.3089 - val_loss: 1.7485 - val_acc: 0.0903\n",
      "Epoch 285/1000\n",
      "40/40 [==============================] - ETA: 2s - loss: 0.1399 - acc: 0.288 - ETA: 1s - loss: 0.1518 - acc: 0.309 - 16s 390ms/step - loss: 0.1524 - acc: 0.3073 - val_loss: 1.7463 - val_acc: 0.0855\n",
      "Epoch 286/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.1541 - acc: 0.293 - ETA: 1s - loss: 0.1502 - acc: 0.309 - 15s 380ms/step - loss: 0.1495 - acc: 0.3109 - val_loss: 1.7125 - val_acc: 0.0903\n",
      "Epoch 287/1000\n",
      "40/40 [==============================] - ETA: 2s - loss: 0.1537 - acc: 0.266 - ETA: 0s - loss: 0.1533 - acc: 0.315 - 15s 385ms/step - loss: 0.1586 - acc: 0.3113 - val_loss: 1.7327 - val_acc: 0.0887\n",
      "Epoch 288/1000\n",
      "40/40 [==============================] - ETA: 2s - loss: 0.1567 - acc: 0.249 - ETA: 1s - loss: 0.1539 - acc: 0.296 - 16s 389ms/step - loss: 0.1517 - acc: 0.3141 - val_loss: 1.7061 - val_acc: 0.0968\n",
      "Epoch 289/1000\n",
      "40/40 [==============================] - ETA: 2s - loss: 0.1510 - acc: 0.304 - ETA: 0s - loss: 0.1382 - acc: 0.287 - 16s 390ms/step - loss: 0.1407 - acc: 0.3133 - val_loss: 1.8221 - val_acc: 0.0919\n",
      "Epoch 290/1000\n",
      "40/40 [==============================] - ETA: 2s - loss: 0.1410 - acc: 0.312 - ETA: 0s - loss: 0.1476 - acc: 0.302 - 15s 382ms/step - loss: 0.1478 - acc: 0.3121 - val_loss: 1.9119 - val_acc: 0.0790\n",
      "Epoch 291/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.1376 - acc: 0.269 - ETA: 0s - loss: 0.1400 - acc: 0.314 - 15s 382ms/step - loss: 0.1441 - acc: 0.3133 - val_loss: 1.8575 - val_acc: 0.0871\n",
      "Epoch 292/1000\n",
      "40/40 [==============================] - ETA: 2s - loss: 0.1352 - acc: 0.348 - ETA: 0s - loss: 0.1375 - acc: 0.313 - 15s 379ms/step - loss: 0.1418 - acc: 0.3113 - val_loss: 1.8041 - val_acc: 0.0935\n",
      "Epoch 293/1000\n",
      "40/40 [==============================] - ETA: 2s - loss: 0.1773 - acc: 0.306 - ETA: 1s - loss: 0.1574 - acc: 0.306 - 16s 399ms/step - loss: 0.1581 - acc: 0.3101 - val_loss: 1.8390 - val_acc: 0.0887\n",
      "Epoch 294/1000\n",
      "40/40 [==============================] - ETA: 2s - loss: 0.1530 - acc: 0.329 - ETA: 0s - loss: 0.1459 - acc: 0.323 - 16s 397ms/step - loss: 0.1420 - acc: 0.3149 - val_loss: 1.8356 - val_acc: 0.1000\n",
      "Epoch 295/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.1194 - acc: 0.263 - ETA: 1s - loss: 0.1312 - acc: 0.306 - 15s 383ms/step - loss: 0.1354 - acc: 0.3202 - val_loss: 1.8504 - val_acc: 0.0952\n",
      "Epoch 296/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.1291 - acc: 0.294 - ETA: 1s - loss: 0.1364 - acc: 0.289 - 16s 392ms/step - loss: 0.1448 - acc: 0.3125 - val_loss: 1.8510 - val_acc: 0.0919\n",
      "Epoch 297/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.1454 - acc: 0.382 - ETA: 1s - loss: 0.1442 - acc: 0.320 - 15s 382ms/step - loss: 0.1454 - acc: 0.3137 - val_loss: 1.8653 - val_acc: 0.0903\n",
      "Epoch 298/1000\n",
      "40/40 [==============================] - ETA: 2s - loss: 0.1473 - acc: 0.308 - ETA: 0s - loss: 0.1534 - acc: 0.320 - 16s 392ms/step - loss: 0.1499 - acc: 0.3137 - val_loss: 1.8610 - val_acc: 0.0952\n",
      "Epoch 299/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.1238 - acc: 0.347 - ETA: 1s - loss: 0.1394 - acc: 0.311 - 16s 398ms/step - loss: 0.1435 - acc: 0.3109 - val_loss: 1.9243 - val_acc: 0.0968\n",
      "Epoch 300/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.1409 - acc: 0.351 - ETA: 1s - loss: 0.1489 - acc: 0.350 - 16s 389ms/step - loss: 0.1420 - acc: 0.3173 - val_loss: 1.9220 - val_acc: 0.0984\n",
      "Epoch 301/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.1314 - acc: 0.279 - ETA: 1s - loss: 0.1439 - acc: 0.305 - 15s 382ms/step - loss: 0.1488 - acc: 0.3149 - val_loss: 1.9288 - val_acc: 0.0903\n",
      "Epoch 302/1000\n",
      "40/40 [==============================] - ETA: 2s - loss: 0.1501 - acc: 0.273 - ETA: 1s - loss: 0.1465 - acc: 0.297 - 15s 382ms/step - loss: 0.1467 - acc: 0.3161 - val_loss: 1.9636 - val_acc: 0.0871\n",
      "Epoch 303/1000\n",
      "40/40 [==============================] - ETA: 2s - loss: 0.1480 - acc: 0.342 - ETA: 1s - loss: 0.1453 - acc: 0.280 - 16s 406ms/step - loss: 0.1458 - acc: 0.3109 - val_loss: 1.8349 - val_acc: 0.0919\n",
      "Epoch 304/1000\n",
      "40/40 [==============================] - ETA: 2s - loss: 0.1555 - acc: 0.355 - ETA: 1s - loss: 0.1502 - acc: 0.311 - 15s 381ms/step - loss: 0.1495 - acc: 0.3141 - val_loss: 1.9146 - val_acc: 0.0903\n",
      "Epoch 305/1000\n",
      "40/40 [==============================] - ETA: 2s - loss: 0.1360 - acc: 0.315 - ETA: 1s - loss: 0.1291 - acc: 0.309 - 15s 380ms/step - loss: 0.1322 - acc: 0.3194 - val_loss: 1.8552 - val_acc: 0.0968\n",
      "Epoch 306/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40/40 [==============================] - ETA: 2s - loss: 0.1350 - acc: 0.276 - ETA: 1s - loss: 0.1411 - acc: 0.292 - 16s 391ms/step - loss: 0.1448 - acc: 0.3121 - val_loss: 1.7859 - val_acc: 0.0935\n",
      "Epoch 307/1000\n",
      "40/40 [==============================] - ETA: 2s - loss: 0.1511 - acc: 0.346 - ETA: 0s - loss: 0.1440 - acc: 0.343 - 16s 393ms/step - loss: 0.1411 - acc: 0.3157 - val_loss: 1.8183 - val_acc: 0.0919\n",
      "Epoch 308/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.1433 - acc: 0.294 - ETA: 1s - loss: 0.1512 - acc: 0.347 - 15s 384ms/step - loss: 0.1442 - acc: 0.3121 - val_loss: 1.8648 - val_acc: 0.0935\n",
      "Epoch 309/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.1405 - acc: 0.321 - ETA: 1s - loss: 0.1376 - acc: 0.308 - 15s 386ms/step - loss: 0.1371 - acc: 0.3202 - val_loss: 1.8199 - val_acc: 0.0919\n",
      "Epoch 310/1000\n",
      "40/40 [==============================] - ETA: 2s - loss: 0.1319 - acc: 0.278 - ETA: 0s - loss: 0.1348 - acc: 0.297 - 16s 388ms/step - loss: 0.1370 - acc: 0.3157 - val_loss: 1.8612 - val_acc: 0.0887\n",
      "Epoch 311/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.1295 - acc: 0.305 - ETA: 1s - loss: 0.1281 - acc: 0.282 - 15s 381ms/step - loss: 0.1288 - acc: 0.3218 - val_loss: 1.9151 - val_acc: 0.0839\n",
      "Epoch 312/1000\n",
      "40/40 [==============================] - ETA: 2s - loss: 0.1449 - acc: 0.294 - ETA: 1s - loss: 0.1428 - acc: 0.309 - 16s 394ms/step - loss: 0.1447 - acc: 0.3153 - val_loss: 1.8680 - val_acc: 0.0968\n",
      "Epoch 313/1000\n",
      "40/40 [==============================] - ETA: 2s - loss: 0.1412 - acc: 0.345 - ETA: 1s - loss: 0.1266 - acc: 0.358 - 16s 390ms/step - loss: 0.1276 - acc: 0.3214 - val_loss: 1.8272 - val_acc: 0.0855\n",
      "Epoch 314/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.1408 - acc: 0.337 - ETA: 1s - loss: 0.1391 - acc: 0.307 - 15s 384ms/step - loss: 0.1471 - acc: 0.3153 - val_loss: 1.8555 - val_acc: 0.0968\n",
      "Epoch 315/1000\n",
      "40/40 [==============================] - ETA: 2s - loss: 0.1358 - acc: 0.294 - ETA: 0s - loss: 0.1374 - acc: 0.311 - 15s 381ms/step - loss: 0.1345 - acc: 0.3202 - val_loss: 1.8879 - val_acc: 0.0839\n",
      "Epoch 316/1000\n",
      "40/40 [==============================] - ETA: 2s - loss: 0.1283 - acc: 0.289 - ETA: 0s - loss: 0.1407 - acc: 0.307 - 15s 387ms/step - loss: 0.1460 - acc: 0.3145 - val_loss: 1.8425 - val_acc: 0.0968\n",
      "Epoch 317/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.1274 - acc: 0.255 - ETA: 1s - loss: 0.1347 - acc: 0.322 - 16s 393ms/step - loss: 0.1370 - acc: 0.3177 - val_loss: 1.9784 - val_acc: 0.0903\n",
      "Epoch 318/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.1371 - acc: 0.324 - ETA: 1s - loss: 0.1391 - acc: 0.331 - 15s 387ms/step - loss: 0.1337 - acc: 0.3165 - val_loss: 1.8398 - val_acc: 0.0903\n",
      "Epoch 319/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.1376 - acc: 0.292 - ETA: 1s - loss: 0.1402 - acc: 0.297 - 15s 387ms/step - loss: 0.1433 - acc: 0.3177 - val_loss: 1.8668 - val_acc: 0.0935\n",
      "Epoch 320/1000\n",
      "40/40 [==============================] - ETA: 2s - loss: 0.1614 - acc: 0.325 - ETA: 1s - loss: 0.1473 - acc: 0.316 - 16s 394ms/step - loss: 0.1413 - acc: 0.3153 - val_loss: 1.9005 - val_acc: 0.0935\n",
      "Epoch 321/1000\n",
      "40/40 [==============================] - ETA: 2s - loss: 0.1346 - acc: 0.344 - ETA: 0s - loss: 0.1390 - acc: 0.309 - 16s 391ms/step - loss: 0.1423 - acc: 0.3133 - val_loss: 1.8551 - val_acc: 0.0887\n",
      "Epoch 322/1000\n",
      "40/40 [==============================] - ETA: 2s - loss: 0.1375 - acc: 0.266 - ETA: 1s - loss: 0.1436 - acc: 0.303 - 15s 377ms/step - loss: 0.1404 - acc: 0.3165 - val_loss: 1.8951 - val_acc: 0.0935\n",
      "Epoch 323/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.1294 - acc: 0.365 - ETA: 1s - loss: 0.1264 - acc: 0.332 - 15s 384ms/step - loss: 0.1293 - acc: 0.3177 - val_loss: 1.8618 - val_acc: 0.0919\n",
      "Epoch 324/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.1500 - acc: 0.324 - ETA: 1s - loss: 0.1519 - acc: 0.316 - 16s 390ms/step - loss: 0.1518 - acc: 0.3093 - val_loss: 1.8644 - val_acc: 0.0887\n",
      "Epoch 325/1000\n",
      "40/40 [==============================] - ETA: 2s - loss: 0.1486 - acc: 0.371 - ETA: 0s - loss: 0.1320 - acc: 0.330 - 15s 383ms/step - loss: 0.1381 - acc: 0.3149 - val_loss: 1.8871 - val_acc: 0.0968\n",
      "Epoch 326/1000\n",
      "40/40 [==============================] - ETA: 2s - loss: 0.1128 - acc: 0.319 - ETA: 0s - loss: 0.1287 - acc: 0.302 - 15s 385ms/step - loss: 0.1297 - acc: 0.3157 - val_loss: 1.8984 - val_acc: 0.0984\n",
      "Epoch 327/1000\n",
      "40/40 [==============================] - ETA: 2s - loss: 0.1368 - acc: 0.363 - ETA: 0s - loss: 0.1334 - acc: 0.343 - 16s 389ms/step - loss: 0.1326 - acc: 0.3190 - val_loss: 1.9163 - val_acc: 0.0839\n",
      "Epoch 328/1000\n",
      "40/40 [==============================] - ETA: 2s - loss: 0.1269 - acc: 0.336 - ETA: 1s - loss: 0.1248 - acc: 0.321 - 16s 391ms/step - loss: 0.1270 - acc: 0.3214 - val_loss: 1.8433 - val_acc: 0.1000\n",
      "Epoch 329/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.1330 - acc: 0.319 - ETA: 1s - loss: 0.1314 - acc: 0.305 - 16s 399ms/step - loss: 0.1289 - acc: 0.3185 - val_loss: 1.8793 - val_acc: 0.0919\n",
      "Epoch 330/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.1275 - acc: 0.289 - ETA: 1s - loss: 0.1253 - acc: 0.316 - 16s 393ms/step - loss: 0.1267 - acc: 0.3181 - val_loss: 1.8561 - val_acc: 0.1016\n",
      "Epoch 331/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.1240 - acc: 0.306 - ETA: 1s - loss: 0.1249 - acc: 0.315 - 18s 442ms/step - loss: 0.1272 - acc: 0.3210 - val_loss: 1.8162 - val_acc: 0.1016\n",
      "Epoch 332/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.1281 - acc: 0.344 - ETA: 1s - loss: 0.1254 - acc: 0.315 - 18s 443ms/step - loss: 0.1272 - acc: 0.3198 - val_loss: 1.8649 - val_acc: 0.0903\n",
      "Epoch 333/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.1240 - acc: 0.370 - ETA: 1s - loss: 0.1306 - acc: 0.329 - 17s 432ms/step - loss: 0.1277 - acc: 0.3218 - val_loss: 1.8305 - val_acc: 0.0919\n",
      "Epoch 334/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.1186 - acc: 0.300 - ETA: 1s - loss: 0.1256 - acc: 0.329 - 16s 391ms/step - loss: 0.1292 - acc: 0.3214 - val_loss: 1.8427 - val_acc: 0.0952\n",
      "Epoch 335/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.1415 - acc: 0.337 - ETA: 1s - loss: 0.1353 - acc: 0.327 - 15s 383ms/step - loss: 0.1346 - acc: 0.3169 - val_loss: 1.7854 - val_acc: 0.1048\n",
      "Epoch 336/1000\n",
      "40/40 [==============================] - ETA: 2s - loss: 0.1398 - acc: 0.320 - ETA: 0s - loss: 0.1339 - acc: 0.325 - 16s 393ms/step - loss: 0.1362 - acc: 0.3145 - val_loss: 1.8073 - val_acc: 0.0952\n",
      "Epoch 337/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.1253 - acc: 0.299 - ETA: 1s - loss: 0.1305 - acc: 0.331 - 15s 384ms/step - loss: 0.1274 - acc: 0.3218 - val_loss: 1.8652 - val_acc: 0.0952\n",
      "Epoch 338/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.1143 - acc: 0.247 - ETA: 1s - loss: 0.1217 - acc: 0.302 - 16s 388ms/step - loss: 0.1256 - acc: 0.3185 - val_loss: 1.7648 - val_acc: 0.0952\n",
      "Epoch 339/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.1555 - acc: 0.354 - ETA: 1s - loss: 0.1459 - acc: 0.332 - 16s 395ms/step - loss: 0.1474 - acc: 0.3145 - val_loss: 1.7384 - val_acc: 0.0984\n",
      "Epoch 340/1000\n",
      "40/40 [==============================] - ETA: 2s - loss: 0.1452 - acc: 0.322 - ETA: 0s - loss: 0.1413 - acc: 0.332 - 15s 380ms/step - loss: 0.1372 - acc: 0.3149 - val_loss: 1.7958 - val_acc: 0.0887\n",
      "Epoch 341/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.1256 - acc: 0.282 - ETA: 1s - loss: 0.1237 - acc: 0.311 - 16s 396ms/step - loss: 0.1235 - acc: 0.3194 - val_loss: 1.8368 - val_acc: 0.0903\n",
      "Epoch 342/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.1239 - acc: 0.394 - ETA: 1s - loss: 0.1325 - acc: 0.342 - 15s 386ms/step - loss: 0.1339 - acc: 0.3190 - val_loss: 1.7703 - val_acc: 0.1000\n",
      "Epoch 343/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.1392 - acc: 0.317 - ETA: 1s - loss: 0.1342 - acc: 0.315 - 15s 387ms/step - loss: 0.1326 - acc: 0.3165 - val_loss: 1.8011 - val_acc: 0.0968\n",
      "Epoch 344/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40/40 [==============================] - ETA: 2s - loss: 0.1262 - acc: 0.261 - ETA: 0s - loss: 0.1299 - acc: 0.316 - 15s 384ms/step - loss: 0.1268 - acc: 0.3194 - val_loss: 1.7962 - val_acc: 0.1048\n",
      "Epoch 345/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.1266 - acc: 0.372 - ETA: 1s - loss: 0.1212 - acc: 0.336 - 16s 396ms/step - loss: 0.1202 - acc: 0.3238 - val_loss: 1.8860 - val_acc: 0.1016\n",
      "Epoch 346/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.1299 - acc: 0.261 - ETA: 1s - loss: 0.1242 - acc: 0.302 - 16s 398ms/step - loss: 0.1282 - acc: 0.3202 - val_loss: 1.8198 - val_acc: 0.0984\n",
      "Epoch 347/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.1326 - acc: 0.296 - ETA: 1s - loss: 0.1338 - acc: 0.317 - 16s 392ms/step - loss: 0.1308 - acc: 0.3190 - val_loss: 1.8557 - val_acc: 0.0919\n",
      "Epoch 348/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.1369 - acc: 0.355 - ETA: 1s - loss: 0.1307 - acc: 0.344 - 16s 393ms/step - loss: 0.1306 - acc: 0.3173 - val_loss: 1.8876 - val_acc: 0.0952\n",
      "Epoch 349/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.1258 - acc: 0.319 - ETA: 0s - loss: 0.1206 - acc: 0.325 - 15s 383ms/step - loss: 0.1224 - acc: 0.3190 - val_loss: 1.8304 - val_acc: 0.0903\n",
      "Epoch 350/1000\n",
      "40/40 [==============================] - ETA: 2s - loss: 0.1255 - acc: 0.292 - ETA: 1s - loss: 0.1257 - acc: 0.351 - 16s 391ms/step - loss: 0.1239 - acc: 0.3210 - val_loss: 1.8466 - val_acc: 0.0871\n",
      "Epoch 351/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.1056 - acc: 0.278 - ETA: 1s - loss: 0.1191 - acc: 0.292 - 16s 392ms/step - loss: 0.1257 - acc: 0.3181 - val_loss: 1.8775 - val_acc: 0.0903\n",
      "Epoch 352/1000\n",
      "40/40 [==============================] - ETA: 2s - loss: 0.1501 - acc: 0.327 - ETA: 0s - loss: 0.1366 - acc: 0.321 - 15s 386ms/step - loss: 0.1333 - acc: 0.3161 - val_loss: 1.8721 - val_acc: 0.0887\n",
      "Epoch 353/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.1307 - acc: 0.320 - ETA: 1s - loss: 0.1334 - acc: 0.323 - 15s 385ms/step - loss: 0.1292 - acc: 0.3185 - val_loss: 1.9039 - val_acc: 0.0887\n",
      "Epoch 354/1000\n",
      "40/40 [==============================] - ETA: 2s - loss: 0.1154 - acc: 0.362 - ETA: 1s - loss: 0.1322 - acc: 0.309 - 16s 391ms/step - loss: 0.1351 - acc: 0.3165 - val_loss: 1.8512 - val_acc: 0.0935\n",
      "Epoch 355/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.1125 - acc: 0.320 - ETA: 1s - loss: 0.1336 - acc: 0.319 - 15s 379ms/step - loss: 0.1275 - acc: 0.3206 - val_loss: 1.8499 - val_acc: 0.0919\n",
      "Epoch 356/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.1110 - acc: 0.354 - ETA: 1s - loss: 0.1123 - acc: 0.318 - 15s 376ms/step - loss: 0.1101 - acc: 0.3278 - val_loss: 1.9175 - val_acc: 0.0935\n",
      "Epoch 357/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.1326 - acc: 0.336 - ETA: 0s - loss: 0.1208 - acc: 0.304 - 15s 379ms/step - loss: 0.1229 - acc: 0.3185 - val_loss: 1.8664 - val_acc: 0.0919\n",
      "Epoch 358/1000\n",
      "40/40 [==============================] - ETA: 2s - loss: 0.1203 - acc: 0.236 - ETA: 0s - loss: 0.1173 - acc: 0.284 - 16s 388ms/step - loss: 0.1244 - acc: 0.3206 - val_loss: 1.7879 - val_acc: 0.0968\n",
      "Epoch 359/1000\n",
      "40/40 [==============================] - ETA: 2s - loss: 0.1184 - acc: 0.261 - ETA: 0s - loss: 0.1192 - acc: 0.276 - 15s 386ms/step - loss: 0.1242 - acc: 0.3206 - val_loss: 1.9395 - val_acc: 0.0790\n",
      "Epoch 360/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.1562 - acc: 0.359 - ETA: 1s - loss: 0.1351 - acc: 0.318 - 16s 406ms/step - loss: 0.1301 - acc: 0.3202 - val_loss: 1.8942 - val_acc: 0.0935\n",
      "Epoch 361/1000\n",
      "40/40 [==============================] - ETA: 2s - loss: 0.1141 - acc: 0.397 - ETA: 0s - loss: 0.1091 - acc: 0.321 - 16s 389ms/step - loss: 0.1076 - acc: 0.3274 - val_loss: 1.9364 - val_acc: 0.0887\n",
      "Epoch 362/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.1517 - acc: 0.335 - ETA: 1s - loss: 0.1392 - acc: 0.307 - 18s 450ms/step - loss: 0.1369 - acc: 0.3125 - val_loss: 1.9391 - val_acc: 0.0903\n",
      "Epoch 363/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.1558 - acc: 0.225 - ETA: 1s - loss: 0.1423 - acc: 0.322 - 18s 439ms/step - loss: 0.1318 - acc: 0.3194 - val_loss: 1.8785 - val_acc: 0.0919\n",
      "Epoch 364/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.1210 - acc: 0.399 - ETA: 1s - loss: 0.1269 - acc: 0.342 - 17s 429ms/step - loss: 0.1249 - acc: 0.3202 - val_loss: 1.9212 - val_acc: 0.0984\n",
      "Epoch 365/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.1165 - acc: 0.366 - ETA: 1s - loss: 0.1225 - acc: 0.335 - 16s 410ms/step - loss: 0.1211 - acc: 0.3214 - val_loss: 1.9644 - val_acc: 0.0968\n",
      "Epoch 366/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.1219 - acc: 0.322 - ETA: 1s - loss: 0.1185 - acc: 0.323 - 17s 421ms/step - loss: 0.1196 - acc: 0.3198 - val_loss: 1.9603 - val_acc: 0.0839\n",
      "Epoch 367/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.1075 - acc: 0.286 - ETA: 1s - loss: 0.1043 - acc: 0.316 - 16s 398ms/step - loss: 0.1089 - acc: 0.3246 - val_loss: 1.9000 - val_acc: 0.0887\n",
      "Epoch 368/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.1221 - acc: 0.330 - ETA: 1s - loss: 0.1201 - acc: 0.322 - 16s 389ms/step - loss: 0.1193 - acc: 0.3181 - val_loss: 1.9513 - val_acc: 0.0806\n",
      "Epoch 369/1000\n",
      "40/40 [==============================] - ETA: 2s - loss: 0.1175 - acc: 0.309 - ETA: 0s - loss: 0.1207 - acc: 0.287 - 16s 390ms/step - loss: 0.1279 - acc: 0.3177 - val_loss: 1.9094 - val_acc: 0.0903\n",
      "Epoch 370/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.1085 - acc: 0.285 - ETA: 1s - loss: 0.1105 - acc: 0.307 - 15s 384ms/step - loss: 0.1136 - acc: 0.3238 - val_loss: 1.9555 - val_acc: 0.0903\n",
      "Epoch 371/1000\n",
      "40/40 [==============================] - ETA: 2s - loss: 0.1165 - acc: 0.307 - ETA: 0s - loss: 0.1188 - acc: 0.316 - 15s 381ms/step - loss: 0.1209 - acc: 0.3222 - val_loss: 2.0209 - val_acc: 0.0887\n",
      "Epoch 372/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.1243 - acc: 0.288 - ETA: 1s - loss: 0.1139 - acc: 0.314 - 15s 386ms/step - loss: 0.1191 - acc: 0.3238 - val_loss: 1.9464 - val_acc: 0.0871\n",
      "Epoch 373/1000\n",
      "40/40 [==============================] - ETA: 2s - loss: 0.1286 - acc: 0.350 - ETA: 0s - loss: 0.1203 - acc: 0.325 - 15s 387ms/step - loss: 0.1196 - acc: 0.3234 - val_loss: 2.0513 - val_acc: 0.0839\n",
      "Epoch 374/1000\n",
      "40/40 [==============================] - ETA: 2s - loss: 0.1029 - acc: 0.319 - ETA: 0s - loss: 0.1135 - acc: 0.356 - 16s 391ms/step - loss: 0.1121 - acc: 0.3266 - val_loss: 2.1018 - val_acc: 0.0823\n",
      "Epoch 375/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.0959 - acc: 0.291 - ETA: 1s - loss: 0.1008 - acc: 0.333 - 16s 389ms/step - loss: 0.1028 - acc: 0.3262 - val_loss: 2.0206 - val_acc: 0.0758\n",
      "Epoch 376/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.1285 - acc: 0.351 - ETA: 1s - loss: 0.1208 - acc: 0.339 - 15s 382ms/step - loss: 0.1169 - acc: 0.3270 - val_loss: 2.0376 - val_acc: 0.0823\n",
      "Epoch 377/1000\n",
      "40/40 [==============================] - ETA: 2s - loss: 0.1198 - acc: 0.289 - ETA: 0s - loss: 0.1160 - acc: 0.283 - 16s 388ms/step - loss: 0.1188 - acc: 0.3206 - val_loss: 2.0266 - val_acc: 0.0935\n",
      "Epoch 378/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.1146 - acc: 0.267 - ETA: 1s - loss: 0.1283 - acc: 0.304 - 16s 391ms/step - loss: 0.1291 - acc: 0.3194 - val_loss: 1.9179 - val_acc: 0.0952\n",
      "Epoch 379/1000\n",
      "40/40 [==============================] - ETA: 2s - loss: 0.1195 - acc: 0.346 - ETA: 1s - loss: 0.1233 - acc: 0.315 - 15s 385ms/step - loss: 0.1241 - acc: 0.3206 - val_loss: 1.8537 - val_acc: 0.0935\n",
      "Epoch 380/1000\n",
      "40/40 [==============================] - ETA: 2s - loss: 0.1203 - acc: 0.332 - ETA: 1s - loss: 0.1145 - acc: 0.311 - 15s 387ms/step - loss: 0.1171 - acc: 0.3238 - val_loss: 1.9329 - val_acc: 0.0887\n",
      "Epoch 381/1000\n",
      "40/40 [==============================] - ETA: 2s - loss: 0.1038 - acc: 0.324 - ETA: 0s - loss: 0.1158 - acc: 0.337 - 15s 385ms/step - loss: 0.1459 - acc: 0.3165 - val_loss: 2.0211 - val_acc: 0.0903\n",
      "Epoch 382/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40/40 [==============================] - ETA: 3s - loss: 0.1635 - acc: 0.318 - ETA: 1s - loss: 0.1368 - acc: 0.320 - 15s 386ms/step - loss: 0.1310 - acc: 0.3185 - val_loss: 1.8889 - val_acc: 0.0839\n",
      "Epoch 383/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.1211 - acc: 0.302 - ETA: 1s - loss: 0.1171 - acc: 0.316 - 15s 387ms/step - loss: 0.1183 - acc: 0.3230 - val_loss: 1.8997 - val_acc: 0.0806\n",
      "Epoch 384/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.1010 - acc: 0.294 - ETA: 1s - loss: 0.1077 - acc: 0.329 - 15s 386ms/step - loss: 0.1112 - acc: 0.3282 - val_loss: 1.9445 - val_acc: 0.0887\n",
      "Epoch 385/1000\n",
      "40/40 [==============================] - ETA: 2s - loss: 0.1070 - acc: 0.292 - ETA: 0s - loss: 0.1180 - acc: 0.312 - 16s 397ms/step - loss: 0.1189 - acc: 0.3214 - val_loss: 1.9601 - val_acc: 0.1000\n",
      "Epoch 386/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.1554 - acc: 0.323 - ETA: 1s - loss: 0.1325 - acc: 0.317 - 17s 425ms/step - loss: 0.1259 - acc: 0.3222 - val_loss: 1.9349 - val_acc: 0.0919\n",
      "Epoch 387/1000\n",
      "40/40 [==============================] - ETA: 2s - loss: 0.1336 - acc: 0.323 - ETA: 0s - loss: 0.1170 - acc: 0.319 - 15s 386ms/step - loss: 0.1146 - acc: 0.3238 - val_loss: 1.9243 - val_acc: 0.0855\n",
      "Epoch 388/1000\n",
      "40/40 [==============================] - ETA: 2s - loss: 0.1112 - acc: 0.285 - ETA: 1s - loss: 0.1064 - acc: 0.315 - 15s 387ms/step - loss: 0.1105 - acc: 0.3270 - val_loss: 1.9614 - val_acc: 0.0839\n",
      "Epoch 389/1000\n",
      "40/40 [==============================] - ETA: 2s - loss: 0.1097 - acc: 0.300 - ETA: 0s - loss: 0.1155 - acc: 0.302 - 16s 391ms/step - loss: 0.1155 - acc: 0.3226 - val_loss: 1.9176 - val_acc: 0.0839\n",
      "Epoch 390/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.1034 - acc: 0.314 - ETA: 1s - loss: 0.1049 - acc: 0.303 - 16s 397ms/step - loss: 0.1099 - acc: 0.3254 - val_loss: 1.8617 - val_acc: 0.0935\n",
      "Epoch 391/1000\n",
      "40/40 [==============================] - ETA: 2s - loss: 0.1307 - acc: 0.333 - ETA: 0s - loss: 0.1221 - acc: 0.308 - 15s 382ms/step - loss: 0.1188 - acc: 0.3198 - val_loss: 1.8560 - val_acc: 0.0887\n",
      "Epoch 392/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.1243 - acc: 0.324 - ETA: 1s - loss: 0.1169 - acc: 0.344 - 16s 392ms/step - loss: 0.1129 - acc: 0.3250 - val_loss: 1.8930 - val_acc: 0.0952\n",
      "Epoch 393/1000\n",
      "40/40 [==============================] - ETA: 2s - loss: 0.1171 - acc: 0.303 - ETA: 0s - loss: 0.1174 - acc: 0.317 - 15s 384ms/step - loss: 0.1167 - acc: 0.3206 - val_loss: 1.9316 - val_acc: 0.0839\n",
      "Epoch 394/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.1051 - acc: 0.292 - ETA: 1s - loss: 0.1000 - acc: 0.311 - 16s 395ms/step - loss: 0.1080 - acc: 0.3246 - val_loss: 1.9096 - val_acc: 0.0935\n",
      "Epoch 395/1000\n",
      "40/40 [==============================] - ETA: 2s - loss: 0.1182 - acc: 0.329 - ETA: 0s - loss: 0.1127 - acc: 0.331 - 15s 386ms/step - loss: 0.1171 - acc: 0.3238 - val_loss: 1.9052 - val_acc: 0.0871\n",
      "Epoch 396/1000\n",
      "40/40 [==============================] - ETA: 2s - loss: 0.1101 - acc: 0.308 - ETA: 0s - loss: 0.1179 - acc: 0.316 - 15s 378ms/step - loss: 0.1223 - acc: 0.3210 - val_loss: 1.8625 - val_acc: 0.0887\n",
      "Epoch 397/1000\n",
      "40/40 [==============================] - ETA: 2s - loss: 0.1063 - acc: 0.325 - ETA: 1s - loss: 0.1131 - acc: 0.316 - 15s 387ms/step - loss: 0.1125 - acc: 0.3226 - val_loss: 1.9505 - val_acc: 0.0952\n",
      "Epoch 398/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.1236 - acc: 0.341 - ETA: 0s - loss: 0.1169 - acc: 0.336 - 16s 389ms/step - loss: 0.1166 - acc: 0.3230 - val_loss: 1.9463 - val_acc: 0.0871\n",
      "Epoch 399/1000\n",
      "40/40 [==============================] - ETA: 2s - loss: 0.1118 - acc: 0.302 - ETA: 0s - loss: 0.1072 - acc: 0.307 - 16s 394ms/step - loss: 0.1087 - acc: 0.3254 - val_loss: 1.8916 - val_acc: 0.0919\n",
      "Epoch 400/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.1132 - acc: 0.321 - ETA: 1s - loss: 0.1088 - acc: 0.310 - 16s 390ms/step - loss: 0.1098 - acc: 0.3238 - val_loss: 1.9310 - val_acc: 0.0871\n",
      "Epoch 401/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.0988 - acc: 0.324 - ETA: 1s - loss: 0.1025 - acc: 0.332 - 15s 381ms/step - loss: 0.1072 - acc: 0.3262 - val_loss: 1.9605 - val_acc: 0.0952\n",
      "Epoch 402/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.1331 - acc: 0.369 - ETA: 1s - loss: 0.1270 - acc: 0.326 - 15s 382ms/step - loss: 0.1222 - acc: 0.3222 - val_loss: 1.9332 - val_acc: 0.0968\n",
      "Epoch 403/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.1055 - acc: 0.316 - ETA: 0s - loss: 0.1347 - acc: 0.300 - 16s 390ms/step - loss: 0.1379 - acc: 0.3202 - val_loss: 2.0296 - val_acc: 0.0774\n",
      "Epoch 404/1000\n",
      "40/40 [==============================] - ETA: 2s - loss: 0.1333 - acc: 0.356 - ETA: 1s - loss: 0.1290 - acc: 0.341 - 16s 388ms/step - loss: 0.1232 - acc: 0.3230 - val_loss: 1.9133 - val_acc: 0.0806\n",
      "Epoch 405/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.1296 - acc: 0.344 - ETA: 0s - loss: 0.1249 - acc: 0.345 - 16s 390ms/step - loss: 0.1175 - acc: 0.3234 - val_loss: 1.9373 - val_acc: 0.0871\n",
      "Epoch 406/1000\n",
      "40/40 [==============================] - ETA: 2s - loss: 0.1088 - acc: 0.321 - ETA: 0s - loss: 0.1246 - acc: 0.315 - 15s 372ms/step - loss: 0.1219 - acc: 0.3173 - val_loss: 1.9253 - val_acc: 0.0887\n",
      "Epoch 407/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.1158 - acc: 0.361 - ETA: 1s - loss: 0.1157 - acc: 0.314 - 16s 396ms/step - loss: 0.1154 - acc: 0.3254 - val_loss: 2.0495 - val_acc: 0.0952\n",
      "Epoch 408/1000\n",
      "40/40 [==============================] - ETA: 2s - loss: 0.1165 - acc: 0.341 - ETA: 0s - loss: 0.1112 - acc: 0.330 - 15s 387ms/step - loss: 0.1117 - acc: 0.3218 - val_loss: 1.9572 - val_acc: 0.0984\n",
      "Epoch 409/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.1162 - acc: 0.341 - ETA: 1s - loss: 0.1112 - acc: 0.318 - 15s 386ms/step - loss: 0.1105 - acc: 0.3226 - val_loss: 1.8690 - val_acc: 0.0984\n",
      "Epoch 410/1000\n",
      "40/40 [==============================] - ETA: 2s - loss: 0.1271 - acc: 0.298 - ETA: 0s - loss: 0.1207 - acc: 0.290 - 16s 391ms/step - loss: 0.1246 - acc: 0.3153 - val_loss: 2.0160 - val_acc: 0.0871\n",
      "Epoch 411/1000\n",
      "40/40 [==============================] - ETA: 2s - loss: 0.1160 - acc: 0.324 - ETA: 0s - loss: 0.1139 - acc: 0.304 - 15s 377ms/step - loss: 0.1126 - acc: 0.3230 - val_loss: 1.9442 - val_acc: 0.0903\n",
      "Epoch 412/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.1059 - acc: 0.275 - ETA: 1s - loss: 0.1095 - acc: 0.345 - 16s 399ms/step - loss: 0.1080 - acc: 0.3270 - val_loss: 1.9181 - val_acc: 0.0855\n",
      "Epoch 413/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.1175 - acc: 0.353 - ETA: 0s - loss: 0.1121 - acc: 0.330 - 16s 389ms/step - loss: 0.1099 - acc: 0.3226 - val_loss: 1.9607 - val_acc: 0.0887\n",
      "Epoch 414/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.1205 - acc: 0.331 - ETA: 1s - loss: 0.1113 - acc: 0.327 - 16s 392ms/step - loss: 0.1145 - acc: 0.3242 - val_loss: 1.9724 - val_acc: 0.0839\n",
      "Epoch 415/1000\n",
      "40/40 [==============================] - ETA: 2s - loss: 0.1096 - acc: 0.358 - ETA: 0s - loss: 0.1111 - acc: 0.312 - 15s 378ms/step - loss: 0.1122 - acc: 0.3238 - val_loss: 2.0542 - val_acc: 0.0823\n",
      "Epoch 416/1000\n",
      "40/40 [==============================] - ETA: 2s - loss: 0.1048 - acc: 0.334 - ETA: 0s - loss: 0.1023 - acc: 0.318 - 16s 392ms/step - loss: 0.1053 - acc: 0.3258 - val_loss: 2.0048 - val_acc: 0.0887\n",
      "Epoch 417/1000\n",
      "40/40 [==============================] - ETA: 2s - loss: 0.1099 - acc: 0.292 - ETA: 0s - loss: 0.1126 - acc: 0.305 - 15s 377ms/step - loss: 0.1139 - acc: 0.3238 - val_loss: 2.0357 - val_acc: 0.0855\n",
      "Epoch 418/1000\n",
      "40/40 [==============================] - ETA: 2s - loss: 0.1057 - acc: 0.245 - ETA: 0s - loss: 0.1107 - acc: 0.310 - 15s 384ms/step - loss: 0.1086 - acc: 0.3230 - val_loss: 2.0044 - val_acc: 0.0887\n",
      "Epoch 419/1000\n",
      "40/40 [==============================] - ETA: 2s - loss: 0.1138 - acc: 0.302 - ETA: 1s - loss: 0.1143 - acc: 0.341 - 16s 400ms/step - loss: 0.1103 - acc: 0.3274 - val_loss: 1.9547 - val_acc: 0.0855\n",
      "Epoch 420/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40/40 [==============================] - ETA: 3s - loss: 0.0990 - acc: 0.288 - ETA: 1s - loss: 0.1031 - acc: 0.315 - 17s 416ms/step - loss: 0.1024 - acc: 0.3278 - val_loss: 1.9379 - val_acc: 0.0903\n",
      "Epoch 421/1000\n",
      "40/40 [==============================] - ETA: 2s - loss: 0.1291 - acc: 0.315 - ETA: 1s - loss: 0.1146 - acc: 0.297 - 16s 391ms/step - loss: 0.1138 - acc: 0.3234 - val_loss: 1.9363 - val_acc: 0.0968\n",
      "Epoch 422/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.1151 - acc: 0.280 - ETA: 1s - loss: 0.1033 - acc: 0.290 - 15s 381ms/step - loss: 0.1052 - acc: 0.3270 - val_loss: 2.0419 - val_acc: 0.0903\n",
      "Epoch 423/1000\n",
      "40/40 [==============================] - ETA: 2s - loss: 0.1134 - acc: 0.302 - ETA: 0s - loss: 0.1118 - acc: 0.323 - 16s 396ms/step - loss: 0.1130 - acc: 0.3254 - val_loss: 2.0203 - val_acc: 0.0919\n",
      "Epoch 424/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.1050 - acc: 0.338 - ETA: 1s - loss: 0.1025 - acc: 0.328 - 16s 403ms/step - loss: 0.1010 - acc: 0.3298 - val_loss: 2.0301 - val_acc: 0.0871\n",
      "Epoch 425/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.1106 - acc: 0.331 - ETA: 1s - loss: 0.1071 - acc: 0.317 - 16s 390ms/step - loss: 0.1122 - acc: 0.3246 - val_loss: 2.0541 - val_acc: 0.0871\n",
      "Epoch 426/1000\n",
      "40/40 [==============================] - ETA: 2s - loss: 0.1237 - acc: 0.386 - ETA: 0s - loss: 0.1172 - acc: 0.330 - 16s 393ms/step - loss: 0.1228 - acc: 0.3210 - val_loss: 2.0134 - val_acc: 0.0919\n",
      "Epoch 427/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.1120 - acc: 0.233 - ETA: 1s - loss: 0.1082 - acc: 0.276 - 16s 388ms/step - loss: 0.1118 - acc: 0.3214 - val_loss: 2.0216 - val_acc: 0.0903\n",
      "Epoch 428/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.1125 - acc: 0.356 - ETA: 1s - loss: 0.1086 - acc: 0.314 - 16s 394ms/step - loss: 0.1066 - acc: 0.3246 - val_loss: 2.0668 - val_acc: 0.0855\n",
      "Epoch 429/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.1045 - acc: 0.378 - ETA: 1s - loss: 0.1042 - acc: 0.354 - 16s 399ms/step - loss: 0.1059 - acc: 0.3254 - val_loss: 2.1095 - val_acc: 0.0839\n",
      "Epoch 430/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.1043 - acc: 0.420 - ETA: 1s - loss: 0.1059 - acc: 0.358 - 16s 393ms/step - loss: 0.1052 - acc: 0.3270 - val_loss: 2.0358 - val_acc: 0.0903\n",
      "Epoch 431/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.1142 - acc: 0.391 - ETA: 1s - loss: 0.1040 - acc: 0.335 - 16s 400ms/step - loss: 0.1039 - acc: 0.3270 - val_loss: 2.1184 - val_acc: 0.0790\n",
      "Epoch 432/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.1101 - acc: 0.400 - ETA: 1s - loss: 0.1066 - acc: 0.344 - 16s 393ms/step - loss: 0.1038 - acc: 0.3262 - val_loss: 2.1201 - val_acc: 0.0839\n",
      "Epoch 433/1000\n",
      "40/40 [==============================] - ETA: 2s - loss: 0.1123 - acc: 0.318 - ETA: 0s - loss: 0.1043 - acc: 0.299 - 15s 382ms/step - loss: 0.1060 - acc: 0.3238 - val_loss: 2.1203 - val_acc: 0.0806\n",
      "Epoch 434/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.1125 - acc: 0.348 - ETA: 1s - loss: 0.1052 - acc: 0.316 - 15s 383ms/step - loss: 0.1061 - acc: 0.3234 - val_loss: 2.1001 - val_acc: 0.0903\n",
      "Epoch 435/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.1114 - acc: 0.345 - ETA: 1s - loss: 0.1064 - acc: 0.315 - 16s 394ms/step - loss: 0.1067 - acc: 0.3258 - val_loss: 2.0601 - val_acc: 0.0806\n",
      "Epoch 436/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.1303 - acc: 0.382 - ETA: 1s - loss: 0.1162 - acc: 0.340 - 16s 389ms/step - loss: 0.1107 - acc: 0.3246 - val_loss: 2.0743 - val_acc: 0.0935\n",
      "Epoch 437/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.0799 - acc: 0.312 - ETA: 1s - loss: 0.0968 - acc: 0.349 - 18s 441ms/step - loss: 0.0983 - acc: 0.3306 - val_loss: 2.1440 - val_acc: 0.0871\n",
      "Epoch 438/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.1087 - acc: 0.332 - ETA: 1s - loss: 0.1052 - acc: 0.327 - 18s 457ms/step - loss: 0.1062 - acc: 0.3266 - val_loss: 2.0566 - val_acc: 0.0903\n",
      "Epoch 439/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.1157 - acc: 0.319 - ETA: 1s - loss: 0.1078 - acc: 0.335 - 17s 413ms/step - loss: 0.1048 - acc: 0.3282 - val_loss: 2.0731 - val_acc: 0.0839\n",
      "Epoch 440/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.1124 - acc: 0.383 - ETA: 1s - loss: 0.1070 - acc: 0.339 - 18s 459ms/step - loss: 0.1069 - acc: 0.3274 - val_loss: 2.1052 - val_acc: 0.0806\n",
      "Epoch 441/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.1024 - acc: 0.381 - ETA: 1s - loss: 0.1048 - acc: 0.334 - 17s 425ms/step - loss: 0.1034 - acc: 0.3278 - val_loss: 2.0795 - val_acc: 0.0806\n",
      "Epoch 442/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.1078 - acc: 0.322 - ETA: 1s - loss: 0.1101 - acc: 0.308 - 16s 403ms/step - loss: 0.1076 - acc: 0.3266 - val_loss: 2.1418 - val_acc: 0.0790\n",
      "Epoch 443/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.1024 - acc: 0.284 - ETA: 1s - loss: 0.1075 - acc: 0.314 - 16s 395ms/step - loss: 0.1069 - acc: 0.3270 - val_loss: 2.0559 - val_acc: 0.0952\n",
      "Epoch 444/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.0904 - acc: 0.270 - ETA: 1s - loss: 0.0987 - acc: 0.318 - 16s 389ms/step - loss: 0.1029 - acc: 0.3294 - val_loss: 2.0570 - val_acc: 0.0871\n",
      "Epoch 445/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.1320 - acc: 0.293 - ETA: 1s - loss: 0.1183 - acc: 0.343 - 15s 384ms/step - loss: 0.1138 - acc: 0.3258 - val_loss: 2.1650 - val_acc: 0.0839\n",
      "Epoch 446/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.1089 - acc: 0.332 - ETA: 1s - loss: 0.1014 - acc: 0.333 - 16s 395ms/step - loss: 0.1022 - acc: 0.3282 - val_loss: 2.1541 - val_acc: 0.0903\n",
      "Epoch 447/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.1184 - acc: 0.341 - ETA: 1s - loss: 0.1100 - acc: 0.340 - 16s 392ms/step - loss: 0.1038 - acc: 0.3262 - val_loss: 2.1030 - val_acc: 0.0935\n",
      "Epoch 448/1000\n",
      "40/40 [==============================] - ETA: 2s - loss: 0.0984 - acc: 0.312 - ETA: 1s - loss: 0.1034 - acc: 0.335 - 15s 378ms/step - loss: 0.1022 - acc: 0.3274 - val_loss: 2.1298 - val_acc: 0.0887\n",
      "Epoch 449/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.0968 - acc: 0.364 - ETA: 0s - loss: 0.1054 - acc: 0.327 - 15s 373ms/step - loss: 0.1064 - acc: 0.3274 - val_loss: 2.0916 - val_acc: 0.0935\n",
      "Epoch 450/1000\n",
      "40/40 [==============================] - ETA: 2s - loss: 0.1101 - acc: 0.284 - ETA: 0s - loss: 0.1101 - acc: 0.318 - 15s 382ms/step - loss: 0.1087 - acc: 0.3230 - val_loss: 2.0931 - val_acc: 0.0790\n",
      "Epoch 451/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.1340 - acc: 0.338 - ETA: 1s - loss: 0.1168 - acc: 0.344 - 15s 383ms/step - loss: 0.1142 - acc: 0.3242 - val_loss: 2.0672 - val_acc: 0.0871\n",
      "Epoch 452/1000\n",
      "40/40 [==============================] - ETA: 2s - loss: 0.1125 - acc: 0.306 - ETA: 0s - loss: 0.1086 - acc: 0.322 - 16s 393ms/step - loss: 0.1110 - acc: 0.3266 - val_loss: 2.1319 - val_acc: 0.0919\n",
      "Epoch 453/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.1031 - acc: 0.314 - ETA: 0s - loss: 0.1068 - acc: 0.279 - 16s 388ms/step - loss: 0.1076 - acc: 0.3270 - val_loss: 2.1056 - val_acc: 0.0903\n",
      "Epoch 454/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.1032 - acc: 0.330 - ETA: 1s - loss: 0.1041 - acc: 0.340 - 15s 386ms/step - loss: 0.1033 - acc: 0.3258 - val_loss: 2.1246 - val_acc: 0.0871\n",
      "Epoch 455/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.1033 - acc: 0.308 - ETA: 0s - loss: 0.1008 - acc: 0.314 - 15s 378ms/step - loss: 0.0981 - acc: 0.3274 - val_loss: 2.1222 - val_acc: 0.0887\n",
      "Epoch 456/1000\n",
      "40/40 [==============================] - ETA: 2s - loss: 0.1031 - acc: 0.387 - ETA: 0s - loss: 0.1086 - acc: 0.346 - 15s 382ms/step - loss: 0.1078 - acc: 0.3234 - val_loss: 2.1269 - val_acc: 0.0952\n",
      "Epoch 457/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.1148 - acc: 0.394 - ETA: 1s - loss: 0.1026 - acc: 0.325 - 15s 376ms/step - loss: 0.1025 - acc: 0.3294 - val_loss: 2.0809 - val_acc: 0.0919\n",
      "Epoch 458/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40/40 [==============================] - ETA: 2s - loss: 0.0994 - acc: 0.370 - ETA: 1s - loss: 0.1032 - acc: 0.338 - 15s 383ms/step - loss: 0.1028 - acc: 0.3274 - val_loss: 2.0906 - val_acc: 0.0871\n",
      "Epoch 459/1000\n",
      "40/40 [==============================] - ETA: 2s - loss: 0.1125 - acc: 0.356 - ETA: 1s - loss: 0.1111 - acc: 0.327 - 15s 382ms/step - loss: 0.1107 - acc: 0.3254 - val_loss: 2.0783 - val_acc: 0.0919\n",
      "Epoch 460/1000\n",
      "40/40 [==============================] - ETA: 2s - loss: 0.0877 - acc: 0.277 - ETA: 0s - loss: 0.0946 - acc: 0.312 - 15s 386ms/step - loss: 0.1005 - acc: 0.3266 - val_loss: 2.1626 - val_acc: 0.0839\n",
      "Epoch 461/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.1088 - acc: 0.311 - ETA: 1s - loss: 0.1042 - acc: 0.337 - 15s 385ms/step - loss: 0.1023 - acc: 0.3258 - val_loss: 2.0768 - val_acc: 0.0919\n",
      "Epoch 462/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.1108 - acc: 0.347 - ETA: 1s - loss: 0.1055 - acc: 0.343 - 16s 390ms/step - loss: 0.1017 - acc: 0.3294 - val_loss: 2.0532 - val_acc: 0.0919\n",
      "Epoch 463/1000\n",
      "40/40 [==============================] - ETA: 2s - loss: 0.1033 - acc: 0.284 - ETA: 0s - loss: 0.1061 - acc: 0.325 - 15s 381ms/step - loss: 0.1042 - acc: 0.3262 - val_loss: 2.0886 - val_acc: 0.0903\n",
      "Epoch 464/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.1081 - acc: 0.387 - ETA: 0s - loss: 0.1064 - acc: 0.329 - 15s 381ms/step - loss: 0.1079 - acc: 0.3254 - val_loss: 2.0526 - val_acc: 0.0935\n",
      "Epoch 465/1000\n",
      "40/40 [==============================] - ETA: 2s - loss: 0.0937 - acc: 0.316 - ETA: 1s - loss: 0.0923 - acc: 0.323 - 15s 383ms/step - loss: 0.0918 - acc: 0.3302 - val_loss: 2.0576 - val_acc: 0.0919\n",
      "Epoch 466/1000\n",
      "40/40 [==============================] - ETA: 2s - loss: 0.1046 - acc: 0.383 - ETA: 1s - loss: 0.1103 - acc: 0.335 - 15s 378ms/step - loss: 0.1059 - acc: 0.3262 - val_loss: 2.0584 - val_acc: 0.0903\n",
      "Epoch 467/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.1156 - acc: 0.327 - ETA: 1s - loss: 0.1123 - acc: 0.339 - 16s 391ms/step - loss: 0.1110 - acc: 0.3226 - val_loss: 1.9645 - val_acc: 0.0919\n",
      "Epoch 468/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.1142 - acc: 0.364 - ETA: 1s - loss: 0.1102 - acc: 0.334 - 15s 382ms/step - loss: 0.1061 - acc: 0.3218 - val_loss: 2.0093 - val_acc: 0.0839\n",
      "Epoch 469/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.1075 - acc: 0.335 - ETA: 1s - loss: 0.1097 - acc: 0.318 - 16s 393ms/step - loss: 0.1086 - acc: 0.3238 - val_loss: 2.0447 - val_acc: 0.0903\n",
      "Epoch 470/1000\n",
      "40/40 [==============================] - ETA: 2s - loss: 0.1021 - acc: 0.338 - ETA: 0s - loss: 0.0999 - acc: 0.317 - 15s 375ms/step - loss: 0.0996 - acc: 0.3282 - val_loss: 2.0035 - val_acc: 0.0952\n",
      "Epoch 471/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.1012 - acc: 0.273 - ETA: 0s - loss: 0.1060 - acc: 0.291 - 15s 384ms/step - loss: 0.1077 - acc: 0.3218 - val_loss: 1.9946 - val_acc: 0.0968\n",
      "Epoch 472/1000\n",
      "40/40 [==============================] - ETA: 2s - loss: 0.1129 - acc: 0.360 - ETA: 0s - loss: 0.1065 - acc: 0.341 - 15s 386ms/step - loss: 0.1019 - acc: 0.3250 - val_loss: 2.0317 - val_acc: 0.0968\n",
      "Epoch 473/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.0971 - acc: 0.332 - ETA: 1s - loss: 0.1020 - acc: 0.341 - 15s 386ms/step - loss: 0.0997 - acc: 0.3290 - val_loss: 2.0868 - val_acc: 0.0952\n",
      "Epoch 474/1000\n",
      "40/40 [==============================] - ETA: 2s - loss: 0.0948 - acc: 0.313 - ETA: 0s - loss: 0.1023 - acc: 0.318 - 15s 375ms/step - loss: 0.1003 - acc: 0.3266 - val_loss: 2.0213 - val_acc: 0.0855\n",
      "Epoch 475/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.0980 - acc: 0.438 - ETA: 0s - loss: 0.0922 - acc: 0.343 - 15s 384ms/step - loss: 0.0932 - acc: 0.3306 - val_loss: 2.0707 - val_acc: 0.0919\n",
      "Epoch 476/1000\n",
      "40/40 [==============================] - ETA: 2s - loss: 0.0894 - acc: 0.348 - ETA: 0s - loss: 0.1011 - acc: 0.337 - 16s 395ms/step - loss: 0.1018 - acc: 0.3282 - val_loss: 2.0860 - val_acc: 0.0839\n",
      "Epoch 477/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.1138 - acc: 0.342 - ETA: 1s - loss: 0.1043 - acc: 0.325 - 16s 394ms/step - loss: 0.1048 - acc: 0.3274 - val_loss: 2.0971 - val_acc: 0.0823\n",
      "Epoch 478/1000\n",
      "40/40 [==============================] - ETA: 2s - loss: 0.1106 - acc: 0.338 - ETA: 1s - loss: 0.1134 - acc: 0.325 - 15s 386ms/step - loss: 0.1085 - acc: 0.3246 - val_loss: 2.1719 - val_acc: 0.0871\n",
      "Epoch 479/1000\n",
      "40/40 [==============================] - ETA: 2s - loss: 0.0958 - acc: 0.319 - ETA: 1s - loss: 0.0979 - acc: 0.322 - 16s 389ms/step - loss: 0.1013 - acc: 0.3262 - val_loss: 2.2308 - val_acc: 0.0871\n",
      "Epoch 480/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.1167 - acc: 0.310 - ETA: 1s - loss: 0.1105 - acc: 0.324 - 15s 385ms/step - loss: 0.1078 - acc: 0.3238 - val_loss: 2.0778 - val_acc: 0.1032\n",
      "Epoch 481/1000\n",
      "40/40 [==============================] - ETA: 2s - loss: 0.0867 - acc: 0.299 - ETA: 0s - loss: 0.0880 - acc: 0.321 - 15s 381ms/step - loss: 0.0882 - acc: 0.3319 - val_loss: 2.1305 - val_acc: 0.0871\n",
      "Epoch 482/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.1017 - acc: 0.380 - ETA: 0s - loss: 0.0952 - acc: 0.360 - 16s 394ms/step - loss: 0.0957 - acc: 0.3294 - val_loss: 2.1561 - val_acc: 0.0855\n",
      "Epoch 483/1000\n",
      "40/40 [==============================] - ETA: 2s - loss: 0.1000 - acc: 0.314 - ETA: 0s - loss: 0.0927 - acc: 0.327 - 16s 391ms/step - loss: 0.0920 - acc: 0.3290 - val_loss: 2.1314 - val_acc: 0.0758\n",
      "Epoch 484/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.0933 - acc: 0.345 - ETA: 0s - loss: 0.1053 - acc: 0.348 - 16s 392ms/step - loss: 0.1023 - acc: 0.3286 - val_loss: 2.0463 - val_acc: 0.0935\n",
      "Epoch 485/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.0997 - acc: 0.328 - ETA: 1s - loss: 0.0929 - acc: 0.309 - 17s 414ms/step - loss: 0.0970 - acc: 0.3298 - val_loss: 2.0515 - val_acc: 0.0887\n",
      "Epoch 486/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.1002 - acc: 0.339 - ETA: 1s - loss: 0.0973 - acc: 0.342 - 16s 396ms/step - loss: 0.0961 - acc: 0.3315 - val_loss: 2.0532 - val_acc: 0.0871\n",
      "Epoch 487/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.1081 - acc: 0.312 - ETA: 1s - loss: 0.1110 - acc: 0.308 - 17s 420ms/step - loss: 0.1176 - acc: 0.3242 - val_loss: 2.1369 - val_acc: 0.0903\n",
      "Epoch 488/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.1124 - acc: 0.353 - ETA: 1s - loss: 0.1091 - acc: 0.338 - 17s 421ms/step - loss: 0.1039 - acc: 0.3270 - val_loss: 2.1168 - val_acc: 0.0871\n",
      "Epoch 489/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.0963 - acc: 0.253 - ETA: 1s - loss: 0.1062 - acc: 0.324 - 16s 409ms/step - loss: 0.1052 - acc: 0.3246 - val_loss: 2.0493 - val_acc: 0.0935\n",
      "Epoch 490/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.0989 - acc: 0.365 - ETA: 1s - loss: 0.1000 - acc: 0.336 - 15s 384ms/step - loss: 0.1002 - acc: 0.3298 - val_loss: 2.1055 - val_acc: 0.0790\n",
      "Epoch 491/1000\n",
      "40/40 [==============================] - ETA: 2s - loss: 0.1040 - acc: 0.364 - ETA: 0s - loss: 0.0976 - acc: 0.329 - 15s 379ms/step - loss: 0.0973 - acc: 0.3294 - val_loss: 2.0898 - val_acc: 0.0806\n",
      "Epoch 492/1000\n",
      "40/40 [==============================] - ETA: 2s - loss: 0.1042 - acc: 0.330 - ETA: 1s - loss: 0.1001 - acc: 0.313 - 15s 386ms/step - loss: 0.1036 - acc: 0.3302 - val_loss: 2.1564 - val_acc: 0.0839\n",
      "Epoch 493/1000\n",
      "40/40 [==============================] - ETA: 2s - loss: 0.1086 - acc: 0.251 - ETA: 1s - loss: 0.1056 - acc: 0.317 - 16s 395ms/step - loss: 0.1030 - acc: 0.3274 - val_loss: 2.1504 - val_acc: 0.0871\n",
      "Epoch 494/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.1050 - acc: 0.296 - ETA: 1s - loss: 0.1041 - acc: 0.332 - 16s 390ms/step - loss: 0.1019 - acc: 0.3294 - val_loss: 2.1930 - val_acc: 0.0774\n",
      "Epoch 495/1000\n",
      "40/40 [==============================] - ETA: 2s - loss: 0.1081 - acc: 0.349 - ETA: 0s - loss: 0.0995 - acc: 0.339 - 15s 386ms/step - loss: 0.0987 - acc: 0.3310 - val_loss: 2.2814 - val_acc: 0.0806\n",
      "Epoch 496/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40/40 [==============================] - ETA: 3s - loss: 0.0956 - acc: 0.342 - ETA: 1s - loss: 0.1071 - acc: 0.339 - 16s 392ms/step - loss: 0.1040 - acc: 0.3282 - val_loss: 2.1116 - val_acc: 0.0823\n",
      "Epoch 497/1000\n",
      "40/40 [==============================] - ETA: 2s - loss: 0.0978 - acc: 0.381 - ETA: 0s - loss: 0.1026 - acc: 0.316 - 15s 370ms/step - loss: 0.1014 - acc: 0.3262 - val_loss: 2.1157 - val_acc: 0.0839\n",
      "Epoch 498/1000\n",
      "40/40 [==============================] - ETA: 2s - loss: 0.1008 - acc: 0.326 - ETA: 0s - loss: 0.1033 - acc: 0.332 - 15s 379ms/step - loss: 0.0995 - acc: 0.3278 - val_loss: 2.1374 - val_acc: 0.0968\n",
      "Epoch 499/1000\n",
      "40/40 [==============================] - ETA: 2s - loss: 0.0995 - acc: 0.304 - ETA: 0s - loss: 0.0969 - acc: 0.319 - 15s 380ms/step - loss: 0.0997 - acc: 0.3278 - val_loss: 2.0819 - val_acc: 0.0855\n",
      "Epoch 500/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.1091 - acc: 0.376 - ETA: 1s - loss: 0.1056 - acc: 0.331 - 18s 446ms/step - loss: 0.1026 - acc: 0.3274 - val_loss: 2.0930 - val_acc: 0.0919\n",
      "Epoch 501/1000\n",
      "40/40 [==============================] - ETA: 4s - loss: 0.0869 - acc: 0.341 - ETA: 1s - loss: 0.0994 - acc: 0.347 - 17s 419ms/step - loss: 0.0978 - acc: 0.3274 - val_loss: 2.0914 - val_acc: 0.0871\n",
      "Epoch 502/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.0889 - acc: 0.342 - ETA: 1s - loss: 0.0975 - acc: 0.334 - 15s 381ms/step - loss: 0.0971 - acc: 0.3290 - val_loss: 2.1102 - val_acc: 0.0806\n",
      "Epoch 503/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.0868 - acc: 0.304 - ETA: 1s - loss: 0.1159 - acc: 0.318 - 16s 389ms/step - loss: 0.1119 - acc: 0.3286 - val_loss: 2.1408 - val_acc: 0.0823\n",
      "Epoch 504/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.1141 - acc: 0.337 - ETA: 1s - loss: 0.1069 - acc: 0.316 - 15s 384ms/step - loss: 0.1146 - acc: 0.3238 - val_loss: 2.0352 - val_acc: 0.0839\n",
      "Epoch 505/1000\n",
      "40/40 [==============================] - ETA: 2s - loss: 0.1129 - acc: 0.351 - ETA: 0s - loss: 0.1060 - acc: 0.322 - 16s 391ms/step - loss: 0.1042 - acc: 0.3250 - val_loss: 2.1187 - val_acc: 0.0823\n",
      "Epoch 506/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.1016 - acc: 0.394 - ETA: 0s - loss: 0.1053 - acc: 0.338 - 15s 377ms/step - loss: 0.1015 - acc: 0.3254 - val_loss: 2.0565 - val_acc: 0.0726\n",
      "Epoch 507/1000\n",
      "40/40 [==============================] - ETA: 2s - loss: 0.0919 - acc: 0.340 - ETA: 0s - loss: 0.0948 - acc: 0.328 - 16s 397ms/step - loss: 0.0947 - acc: 0.3266 - val_loss: 2.0765 - val_acc: 0.0823\n",
      "Epoch 508/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.0982 - acc: 0.319 - ETA: 1s - loss: 0.1014 - acc: 0.342 - 15s 385ms/step - loss: 0.0993 - acc: 0.3274 - val_loss: 2.0868 - val_acc: 0.0887\n",
      "Epoch 509/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.0971 - acc: 0.317 - ETA: 1s - loss: 0.1006 - acc: 0.326 - 18s 456ms/step - loss: 0.1024 - acc: 0.3266 - val_loss: 2.1240 - val_acc: 0.0758\n",
      "Epoch 510/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.0964 - acc: 0.308 - ETA: 1s - loss: 0.0972 - acc: 0.329 - 18s 462ms/step - loss: 0.0961 - acc: 0.3286 - val_loss: 2.1290 - val_acc: 0.0806\n",
      "Epoch 511/1000\n",
      "40/40 [==============================] - ETA: 5s - loss: 0.0938 - acc: 0.349 - ETA: 1s - loss: 0.0981 - acc: 0.341 - 21s 536ms/step - loss: 0.0973 - acc: 0.3262 - val_loss: 2.1532 - val_acc: 0.0903\n",
      "Epoch 512/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.0906 - acc: 0.311 - ETA: 1s - loss: 0.0947 - acc: 0.335 - 19s 465ms/step - loss: 0.0959 - acc: 0.3290 - val_loss: 2.1007 - val_acc: 0.0855\n",
      "Epoch 513/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.0972 - acc: 0.393 - ETA: 1s - loss: 0.1008 - acc: 0.349 - 17s 435ms/step - loss: 0.1000 - acc: 0.3282 - val_loss: 2.1387 - val_acc: 0.0726\n",
      "Epoch 514/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.0929 - acc: 0.350 - ETA: 1s - loss: 0.0905 - acc: 0.334 - 17s 434ms/step - loss: 0.0932 - acc: 0.3323 - val_loss: 2.0855 - val_acc: 0.0855\n",
      "Epoch 515/1000\n",
      "40/40 [==============================] - ETA: 4s - loss: 0.0924 - acc: 0.266 - ETA: 1s - loss: 0.0947 - acc: 0.309 - 19s 481ms/step - loss: 0.0915 - acc: 0.3331 - val_loss: 2.1133 - val_acc: 0.0887\n",
      "Epoch 516/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.0958 - acc: 0.344 - ETA: 1s - loss: 0.0945 - acc: 0.324 - 18s 462ms/step - loss: 0.0935 - acc: 0.3327 - val_loss: 2.0121 - val_acc: 0.0823\n",
      "Epoch 517/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.0898 - acc: 0.333 - ETA: 1s - loss: 0.0909 - acc: 0.295 - 18s 461ms/step - loss: 0.0935 - acc: 0.3294 - val_loss: 2.0167 - val_acc: 0.0919\n",
      "Epoch 518/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.0955 - acc: 0.343 - ETA: 1s - loss: 0.0932 - acc: 0.323 - 18s 451ms/step - loss: 0.1004 - acc: 0.3242 - val_loss: 2.1267 - val_acc: 0.0839\n",
      "Epoch 519/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.1101 - acc: 0.378 - ETA: 1s - loss: 0.1033 - acc: 0.335 - 16s 411ms/step - loss: 0.1021 - acc: 0.3294 - val_loss: 2.1075 - val_acc: 0.0919\n",
      "Epoch 520/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.1124 - acc: 0.341 - ETA: 1s - loss: 0.1174 - acc: 0.330 - 17s 413ms/step - loss: 0.1116 - acc: 0.3226 - val_loss: 2.0572 - val_acc: 0.0903\n",
      "Epoch 521/1000\n",
      "40/40 [==============================] - ETA: 3s - loss: 0.0926 - acc: 0.303 - ETA: 1s - loss: 0.0952 - acc: 0.335 - 16s 391ms/step - loss: 0.0929 - acc: 0.3302 - val_loss: 2.0484 - val_acc: 0.0855\n",
      "Epoch 522/1000\n",
      "16/40 [===========>..................] - ETA: 3s - loss: 0.0791 - acc: 0.3226"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-228-c5f9cbb7e73d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m                     \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m16\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m                     \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m                     validation_split=0.2)\n\u001b[0m\u001b[0;32m      9\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m             \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python36\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1040\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1042\u001b[1;33m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1043\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1044\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32mc:\\python36\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python36\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2659\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2660\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2661\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2662\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2663\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python36\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2629\u001b[0m                                 \u001b[0msymbol_vals\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2630\u001b[0m                                 session)\n\u001b[1;32m-> 2631\u001b[1;33m         \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2632\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2633\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[0;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1439\u001b[1;33m               run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1440\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    try:\n",
    "        run_again = int(input(\"Do you want to run the Model again for 1 epoch more? (1 - Yes | 0 - No) : \"))\n",
    "        if run_again == 1:\n",
    "            history = model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
    "                    batch_size=16,\n",
    "                    epochs=1000,\n",
    "                    validation_split=0.2)\n",
    "        else:\n",
    "            break\n",
    "    except Exception as e:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Plot training & validation accuracy values\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.savefig(\"Models/Model_Acc_Character_Level_1\")\n",
    "plt.show()\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.savefig(\"Models/Model_Loss_Character_Level_1\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    # Encode the input as state vectors.\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    target_seq[0, 0, target_token_index['\\t']] = 1.\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict(\n",
    "            [target_seq] + states_value)\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
    "        decoded_sentence += sampled_char\n",
    "\n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if (sampled_char == '\\n' or\n",
    "           len(decoded_sentence) > max_decoder_seq_length):\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "Input sentence: i have bought several of the vitality canned dog food products and have found them all to be of good quality. the product looks more like a stew than a processed meat and it smells better. my labrador is finicky and she appreciates this product better than most.\n",
      "Decoded sentence: ggggggggggyyyyyyyyyuyyyyyyuyyyyyuyyyyyuyyyyyuyyyyyuyyyyyuyyyyyuyyyyy\n",
      "-\n",
      "Input sentence: product arrived labeled as jumbo salted peanuts...the peanuts were actually small sized unsalted. not sure if this was an error or if the vendor intended to represent the product as  jumbo .\n",
      "Decoded sentence: ggggggggggyyyyyyyyyuyyyyyyuyyyyyuyyyyyuyyyyyuyyyyyuyyyyyuyyyyyuyyyyy\n",
      "-\n",
      "Input sentence: this is a confection that has been around a few centuries. it is a light, pillowy citrus gelatin with nuts   in this case filberts. and it is cut into tiny squares and then liberally coated with powdered sugar. and it is a tiny mouthful of heaven. not too chewy, and very flavorful. i highly recommend this yummy treat. if you are familiar with the story of c.s. lewis   the lion, the witch, and the wardrobe    this is the treat that seduces edmund into selling out his brother and sisters to the witch.\n",
      "Decoded sentence: ggggggggggyyyyyyyyyuyyyyyyuyyyyyuyyyyyuyyyyyuyyyyyuyyyyyuyyyyyuyyyyy\n",
      "-\n",
      "Input sentence: if you are looking for the secret ingredient in robitussin i believe i have found it. i got this in addition to the root beer extract i ordered  which was good  and made some cherry soda. the flavor is very medicinal.\n",
      "Decoded sentence: ggggggggggyyyyyyyyyuyyyyyyuyyyyyuyyyyyuyyyyyuyyyyyuyyyyyuyyyyyuyyyyy\n",
      "-\n",
      "Input sentence: great taffy at a great price. there was a wide assortment of yummy taffy. delivery was very quick. if your a taffy lover, this is a deal.\n",
      "Decoded sentence: ggggggggggyyyyyyyyyuyyyyyyuyyyyyuyyyyyuyyyyyuyyyyyuyyyyyuyyyyyuyyyyy\n"
     ]
    }
   ],
   "source": [
    "for seq_index in range(5):\n",
    "\n",
    "    # Take one sequence (part of the training set) for trying out decoding.\n",
    "    input_seq = encoder_input_data[seq_index: seq_index + 1]\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "\n",
    "    print('-')\n",
    "    print('Input sentence:', input_texts[seq_index])\n",
    "    print('Decoded sentence:', decoded_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "_uuid": "46585c7317f64250f9c1846f4d04e2c4907422ee"
   },
   "outputs": [],
   "source": [
    "def generic_Model(n_input, n_output, n_units):\n",
    "    # define training encoder\n",
    "    encoder_inputs = Input(shape=(None, n_input))\n",
    "    encoder = LSTM(n_units, return_state=True)\n",
    "    encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "    encoder_states = [state_h, state_c]\n",
    "    # define training decoder\n",
    "    decoder_inputs = Input(shape=(None, n_output))\n",
    "    decoder_lstm = LSTM(n_units, return_sequences=True, return_state=True)\n",
    "    decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
    "    decoder_dense = Dense(n_output, activation='softmax')\n",
    "    \n",
    "    decoder_outputs = decoder_dense(decoder_outputs)\n",
    "    model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "    # define inference encoder\n",
    "    encoder_model = Model(encoder_inputs, encoder_states)\n",
    "    # define inference decoder\n",
    "    decoder_state_input_h = Input(shape=(n_units,))\n",
    "    decoder_state_input_c = Input(shape=(n_units,))\n",
    "    decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "    decoder_outputs, state_h, state_c = decoder_lstm(decoder_inputs,  initial_state=decoder_states_inputs)\n",
    "    decoder_states = [state_h, state_c]\n",
    "    decoder_outputs = decoder_dense(decoder_outputs)\n",
    "    decoder_model = Model([decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states)\n",
    "    # return all models\n",
    "    return model, encoder_model, decoder_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "_uuid": "ea1e80648de3d72954da93f934d9f2210f7f4738"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_7 (InputLayer)            (None, None, 2297)   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_8 (InputLayer)            (None, None, 67)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_4 (LSTM)                   [(None, 128), (None, 1242112     input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_5 (LSTM)                   [(None, None, 128),  100352      input_8[0][0]                    \n",
      "                                                                 lstm_4[0][1]                     \n",
      "                                                                 lstm_4[0][2]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_13 (Dense)                (None, None, 67)     8643        lstm_5[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 1,351,107\n",
      "Trainable params: 1,351,107\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "training_model, encoder_model, decoder_model = generic_Model(max_encoder_seq_length, max_decoder_seq_length, 128)\n",
    "training_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "_uuid": "234e7a92e06db16dee5c08bb7699ec8dc03894d7"
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected input_7 to have 3 dimensions, but got array with shape (42, 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-99-e2c7b0ebd1bb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m validation_split=0.2)\n\u001b[0m",
      "\u001b[1;32mc:\\python36\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m    953\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    954\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 955\u001b[1;33m             batch_size=batch_size)\n\u001b[0m\u001b[0;32m    956\u001b[0m         \u001b[1;31m# Prepare validation data.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    957\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python36\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[0;32m    752\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    753\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# Don't enforce the batch size.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 754\u001b[1;33m             exception_prefix='input')\n\u001b[0m\u001b[0;32m    755\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    756\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python36\\lib\\site-packages\\keras\\engine\\training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[1;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[0;32m    124\u001b[0m                         \u001b[1;34m': expected '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' to have '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    125\u001b[0m                         \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' dimensions, but got array '\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 126\u001b[1;33m                         'with shape ' + str(data_shape))\n\u001b[0m\u001b[0;32m    127\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    128\u001b[0m                     \u001b[0mdata_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_shape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Error when checking input: expected input_7 to have 3 dimensions, but got array with shape (42, 1)"
     ]
    }
   ],
   "source": [
    "training_model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
    "training_model.fit([input_characters, target_characters], target_characters,\n",
    "batch_size=batch_size,\n",
    "epochs=epochs,\n",
    "validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "aa670a6d30f64756f81c558931d58084044bc307"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "_uuid": "0f82d087944a99d6e2f2eb7832a3e98ce143cbb1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "400000it [00:06, 64830.90it/s]\n"
     ]
    }
   ],
   "source": [
    "word_to_index, index_to_word, word_to_vec_map = read_glove_vecs('dataset/Glove Embeddings/glove.6B.50d.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "_uuid": "523e40ced0dfa29c4ae66de97fb2a07a83296f16",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400000\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"didn't\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-101-0001a2c272b4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword_to_vec_map\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword_to_index\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"didn't\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword_to_vec_map\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"strife-torn\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: \"didn't\""
     ]
    }
   ],
   "source": [
    "print(len(word_to_vec_map))\n",
    "print(word_to_index[\"didn't\"])\n",
    "print(word_to_vec_map[\"strife-torn\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "_uuid": "b85eee801058d40f9b1967632b6e75b4ad15ef6d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n",
      "the index of cucumber in the vocabulary is 113317\n",
      "the 289846th word in the vocabulary is potatos\n",
      "400000\n"
     ]
    }
   ],
   "source": [
    "word = \"cucumber\"\n",
    "index = 289846\n",
    "print(len(word_to_vec_map[word]))\n",
    "print(\"the index of\", word, \"in the vocabulary is\", word_to_index[word])\n",
    "print(\"the\", str(index) + \"th word in the vocabulary is\", index_to_word[index])\n",
    "print(len(word_to_vec_map))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "36fc694f6d61a67426121d05d5d7ecbe5f6576e8"
   },
   "source": [
    "**Exercise**: Implement `sentence_to_avg()`. This function performs two steps described as follows:\n",
    "1. Convert every sentence to lower-case, then split the sentence into a list of words.\n",
    "2. For each word in the sentence, access its GloVe representation. Then, average all these values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "_uuid": "370e34b8fd3664c187fdf1f24bb718216ef3eb58"
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: sentence_to_avg\n",
    "\n",
    "def sentence_to_avg(sentence, word_to_vec_map):\n",
    "    \"\"\"\n",
    "    Converts a sentence (string) into a list of words (strings). Extracts the GloVe representation of each word\n",
    "    and averages its value into a single vector encoding the meaning of the sentence.\n",
    "    \n",
    "    Arguments:\n",
    "    sentence -- string, one training example from X\n",
    "    word_to_vec_map -- dictionary mapping every word in a vocabulary into its 50-dimensional vector representation\n",
    "    \n",
    "    Returns:\n",
    "    avg -- average vector encoding information about the sentence, numpy-array of shape (50,)\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    # Step 1: Split sentence into list of lower case words ( 1 line)\n",
    "    words = [i.lower() for i in sentence.split()]\n",
    "    \n",
    "    # Initialize the average word vector, should have the same shape as your word vectors.\n",
    "    avg = np.zeros((len(word_to_vec_map[\"a\"]),))\n",
    "    \n",
    "    # Step 2: average the word vectors. You can loop over the words in the list \"words\".\n",
    "    for w in words:\n",
    "        try:\n",
    "            avg += word_to_vec_map[w]\n",
    "        except KeyError:\n",
    "            print(w)\n",
    "            continue\n",
    "        \n",
    "    avg = avg / len(words)\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "_uuid": "28d98594c056c9c0e584f53f0eeaece1d1fdeb3d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "strife-torn\n",
      "avg =  [ 6.00406333e-01 -3.32836241e-02  1.87963111e-01  6.77701481e-02\n",
      "  3.54529296e-01  5.80289259e-02 -5.28430741e-01  2.34516926e-02\n",
      " -4.44399430e-02 -4.49052852e-01 -4.94001852e-02 -5.91713370e-01\n",
      " -3.30848889e-01 -3.86380000e-02  2.55290974e-01 -1.00995963e-01\n",
      " -2.50230633e-01  1.57850185e-01 -8.12166667e-01  1.53866296e-02\n",
      "  1.62483900e-01  3.39234815e-01  1.11209556e-01  1.11625963e-02\n",
      " -1.28282022e-01 -1.71446667e+00 -3.79737037e-02  7.29834074e-02\n",
      "  7.51024444e-02 -5.31830370e-02  3.21913704e+00 -1.57129481e-02\n",
      " -3.00656678e-01 -2.03952637e-01  3.13164624e-01  7.13015852e-02\n",
      "  1.96886630e-01 -2.07194259e-01 -1.68992593e-03  3.62324111e-01\n",
      " -3.64241751e-01  2.49083281e-01  2.42211593e-01 -2.78113704e-01\n",
      "  2.00010611e-01  2.41158148e-02 -4.04617674e-01 -2.34434815e-02\n",
      "  1.46426704e-01 -3.46355148e-01]\n"
     ]
    }
   ],
   "source": [
    "avg = sentence_to_avg(\"at least two people were killed in a suspected bomb attack on a passenger bus in the strife-torn southern philippines on monday , the military said .\\n\", word_to_vec_map)\n",
    "print(\"avg = \", avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "_uuid": "fd9716098a859c18687a3761bcc4401916ad81be"
   },
   "outputs": [],
   "source": [
    "def preprocess_data(X):\n",
    "    from tqdm import tqdm\n",
    "    \n",
    "    m = len(X)\n",
    "    str_check = [\",\", \".\", '\"', \"'\", \"(\", \")\", \"$\", \"mph\", \"-\", \"_\"]\n",
    "    str_remove = [\"-lrb-\", \"-rrb-\"]\n",
    "    \n",
    "    for i in tqdm(range(m)):\n",
    "        sentence_words = [w for w in X[i].split()]\n",
    "        j = 0\n",
    "        new_string = \"\"\n",
    "        if(i == 0):\n",
    "            print(X[i])\n",
    "        \n",
    "        for w in sentence_words:\n",
    "            try:\n",
    "               _ = word_to_index[w]\n",
    "            except KeyError:\n",
    "                for stri_r in str_remove:\n",
    "                    if(stri_r in w):\n",
    "                        #print(stri_r)\n",
    "                        w = w.replace(stri_r,\" \")\n",
    "                for stri in str_check:\n",
    "                    if(stri in w):\n",
    "                        idx = w.index(stri)\n",
    "                        if(w[idx:idx+2] == \"'s\"):\n",
    "                            stri = \"'s\"\n",
    "                            idx = w.index(stri)\n",
    "                            a = w[:idx]\n",
    "                            b = w[idx + 2:]\n",
    "                            w = a + \" \" + stri + \" \" + b\n",
    "                            continue\n",
    "                        elif(w[idx:idx+3] == \"mph\"):\n",
    "                            stri = \"mph\"\n",
    "                            idx = w.index(stri)\n",
    "                            if(w[idx - 1] == \" \"):\n",
    "                                a = \"\"\n",
    "                            else:\n",
    "                                a = w[:idx]\n",
    "                            b = w[idx + 3:]\n",
    "                            w = a + \" \" + stri + \" \" + b\n",
    "                            continue\n",
    "                        if(w[idx - 1] == \" \"):\n",
    "                            a = \"\"\n",
    "                        else:\n",
    "                            a = w[:idx]\n",
    "\n",
    "                        b = w[idx + 1:]\n",
    "                        w = a + \" \" + stri + \" \" + b\n",
    "            \n",
    "            new_string += w + \" \"\n",
    "        X[i] = new_string\n",
    "        if(i == 0):\n",
    "            print(X[i])\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "_uuid": "b0d1502e9d10aa5eefb9fb45c5fef9578c4f0364"
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: sentences_to_indices\n",
    "\n",
    "def sentences_to_indices(X, word_to_index, max_len):\n",
    "    from tqdm import tqdm\n",
    "    \"\"\"\n",
    "    Converts an array of sentences (strings) into an array of indices corresponding to words in the sentences.\n",
    "    The output shape should be such that it can be given to `Embedding()` (described in Figure 4). \n",
    "    \n",
    "    Arguments:\n",
    "    X -- array of sentences (strings), of shape (m, 1)\n",
    "    word_to_index -- a dictionary containing the each word mapped to its index\n",
    "    max_len -- maximum number of words in a sentence. You can assume every sentence in X is no longer than this. \n",
    "    \n",
    "    Returns:\n",
    "    X_indices -- array of indices corresponding to words in the sentences from X, of shape (m, max_len)\n",
    "    \"\"\"\n",
    "    \n",
    "    m = len(X)                                # number of training examples\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    # Initialize X_indices as a numpy matrix of zeros and the correct shape ( 1 line)\n",
    "    X_indices = np.zeros((m, max_len))\n",
    "    \n",
    "    for i in tqdm(range(m)):                               # loop over training examples\n",
    "        \n",
    "        # Convert the ith training sentence in lower case and split is into words. You should get a list of words.\n",
    "        sentence_words = [w.lower() for w in X[i].split()]\n",
    "        \n",
    "        # Initialize j to 0\n",
    "        j = 0\n",
    "        \n",
    "        # Loop over the words of sentence_words\n",
    "        for w in sentence_words:\n",
    "            # Set the (i,j)th entry of X_indices to the index of the correct word.\n",
    "            try:\n",
    "                #if(w == \"<unk>\"):\n",
    "                    #X_indices[i, j] = -1\n",
    "                #else:\n",
    "                    #X_indices[i, j] = word_to_index[w]\n",
    "                X_indices[i, j] = word_to_index[w]\n",
    "            except Exception as e:\n",
    "                #print(e)\n",
    "                pass\n",
    "            # Increment j to j + 1\n",
    "            j += 1\n",
    "            \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return X_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "_uuid": "6c5e925bca627dc0f6008eeb9cb1cf19672f596e",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "australia 's current account deficit shrunk by a record #.## billion dollars -lrb- #.## billion us -rrb- in the june quarter due to soaring commodity prices , figures released monday showed .\n",
      "\n",
      "australia 's current account deficit shrunk by a record # . ## billion dollars   # . ## billion us   in the june quarter due to soaring commodity prices , figures released monday showed . \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 100/100 [00:00<00:00, 113.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 100/100 [00:00<00:00, 6267.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 41)\n",
      "X[0] = the united nations ' humanitarian chief john holmes arrived in ethiopia monday to tour regions affected by drought , which has left some eight million people in need of urgent food aid . \n",
      "X1_indices[0] = [3.57266e+05 3.72191e+05 2.57214e+05 4.80000e+01 1.83878e+05 9.88340e+04\n",
      " 1.98418e+05 1.80703e+05 6.00960e+04 1.88481e+05 1.40306e+05 2.48403e+05\n",
      " 3.60915e+05 3.62970e+05 3.04268e+05 4.74860e+04 8.81260e+04 1.29635e+05\n",
      " 4.52000e+02 3.86474e+05 1.74032e+05 2.19577e+05 3.37259e+05 1.34390e+05\n",
      " 2.44641e+05 2.80944e+05 1.88481e+05 2.58451e+05 2.68046e+05 3.73814e+05\n",
      " 1.51204e+05 4.88310e+04 8.67000e+02 0.00000e+00 0.00000e+00 0.00000e+00\n",
      " 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00]\n",
      "X[0] = president george w. bush declared an emergency in texas and state authorities began ordering evacuations as deadly hurricane ike , which strengthened wednesday to a powerful category two storm , churned in the gulf of mexico toward the us state . \n",
      "X1_indices[0] = [291804. 159864. 381773.  87571. 118832.  54273. 136330. 188481. 356797.\n",
      "  54718. 341956.  63526.  71917. 271178. 141683.  60665. 118226. 184300.\n",
      " 187122.    452. 386474. 344582. 384674. 360915.  43010. 290201.  93724.\n",
      " 368321. 343914.    452. 100759. 188481. 357266. 168964. 268046. 242556.\n",
      " 363080. 357266. 374021. 341956.    867.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "australian current account deficit narrows sharply\n",
      "\n",
      "australian current account deficit narrows sharply \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 100/100 [00:00<00:00, 1474.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 100/100 [00:00<00:00, 100198.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 13)\n",
      "Y[0] = australian current account deficit narrows sharply \n",
      "Y1_indices[0] = [ 63426. 113946.  45230. 119346. 256776. 327623.      0.      0.      0.\n",
      "      0.      0.      0.      0.]\n",
      "Y[0] = algeria adopts #### finance bill with oil put at ## dollars a barrel \n",
      "Y1_indices[0] = [5.12450e+04 4.67900e+04 0.00000e+00 1.48365e+05 7.60840e+04 3.88711e+05\n",
      " 2.68641e+05 2.95763e+05 6.20650e+04 1.00000e+01 1.27119e+05 4.30100e+04\n",
      " 6.93030e+04]\n"
     ]
    }
   ],
   "source": [
    "X = open(\"dataset/sumdata/train/train.article.10000.txt\").readlines()[:100]\n",
    "X = preprocess_data(X)\n",
    "print(len(X))\n",
    "length = [len(x.split()) for x in X]\n",
    "maxLen = max(length)\n",
    "print(maxLen)\n",
    "X1 = sentences_to_indices(X, word_to_index, maxLen)\n",
    "print(X1.shape)\n",
    "\n",
    "print(\"X[0] =\", X[9])\n",
    "print(\"X1_indices[0] =\", X1[9])\n",
    "\n",
    "print(\"X[0] =\", X[length.index(max(length))])\n",
    "print(\"X1_indices[0] =\", X1[length.index(max(length))])\n",
    "\n",
    "\n",
    "Y = open(\"dataset/sumdata/train/train.title.10000.txt\").readlines()[:100]\n",
    "Y = preprocess_data(Y)\n",
    "print(len(Y))\n",
    "length_Y = [len(y.split()) for y in Y]\n",
    "maxLen_Y = max(length_Y)\n",
    "print(maxLen_Y)\n",
    "Y1 = sentences_to_indices(Y, word_to_index, maxLen_Y)\n",
    "print(Y1.shape)\n",
    "print(\"Y[0] =\", Y[0])\n",
    "print(\"Y1_indices[0] =\", Y1[0])\n",
    "print(\"Y[0] =\", Y[length_Y.index(max(length_Y))])\n",
    "print(\"Y1_indices[0] =\", Y1[length_Y.index(max(length_Y))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "_uuid": "ae2f7b38b5174f5297b18575ceb771b4207bf9e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\n"
     ]
    }
   ],
   "source": [
    "print(index_to_word[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "_uuid": "5a82a5bfbe6cbc2fea60ff56a81c85a75f87f7e8"
   },
   "outputs": [],
   "source": [
    "def convert_to_one_hot(X, word_to_index):\n",
    "    \n",
    "    m = X.shape[0]\n",
    "    dim = X.shape[1]\n",
    "    print(type(X))\n",
    "    length = len(word_to_index) + 1\n",
    "    print(length)\n",
    "    \n",
    "    Z = np.zeros((m, dim, length + 1))\n",
    "    \n",
    "    \n",
    "    for i in tqdm(range(m)):\n",
    "        for j in range(dim):\n",
    "            if(j > 0):\n",
    "                if(X[i,j] == -1):\n",
    "                    Z[i, j, length] = 1\n",
    "                else:\n",
    "                    idx = X[i,j]\n",
    "                    Z[i, j - 1, int(idx)] = 1\n",
    "                \n",
    "    return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "_uuid": "b62099dd7797b287e190732711c251eb77e3e972",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "400001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 100/100 [00:00<00:00, 24917.15it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_target_data = convert_to_one_hot(Y1, word_to_index)\n",
    "decoder_target_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "0689fd7ea82293bae5c9286d7b72ad275265b4d4"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "eadb80dec6592ce803d45afb40e5d12e78f83255",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X2 = open(\"dataset/DUC/duc2002/data/test/summaries/Text.txt\").readlines()\n",
    "preprocess_data(X2)\n",
    "X3 = sentences_to_indices(X2, word_to_index, max_len = 1500)\n",
    "print(\"X2[0] =\", X2[1])\n",
    "print(\"X3_indices[0] =\", X3[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "fe401b4d9105962786b170ef10d274c9b6ddd667"
   },
   "outputs": [],
   "source": [
    "print(index_to_word[8])\n",
    "print(word_to_index[\"#\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "_uuid": "27d23f789b586bf6c87e7a38a2eac968a48eb4ed"
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: pretrained_embedding_layer\n",
    "\n",
    "def pretrained_embedding_layer(word_to_vec_map, word_to_index):\n",
    "    \"\"\"\n",
    "    Creates a Keras Embedding() layer and loads in pre-trained GloVe 50-dimensional vectors.\n",
    "    \n",
    "    Arguments:\n",
    "    word_to_vec_map -- dictionary mapping words to their GloVe vector representation.\n",
    "    word_to_index -- dictionary mapping from words to their indices in the vocabulary (400,001 words)\n",
    "\n",
    "    Returns:\n",
    "    embedding_layer -- pretrained layer Keras instance\n",
    "    \"\"\"\n",
    "    \n",
    "    vocab_len = len(word_to_index) + 1                  # adding 1 to fit Keras embedding (requirement)\n",
    "    emb_dim = word_to_vec_map[\"cucumber\"].shape[0]      # define dimensionality of your GloVe word vectors (= 50)\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    # Initialize the embedding matrix as a numpy array of zeros of shape (vocab_len, dimensions of word vectors = emb_dim)\n",
    "    emb_matrix = np.zeros((vocab_len, emb_dim))\n",
    "    \n",
    "    # Set each row \"index\" of the embedding matrix to be the word vector representation of the \"index\"th word of the vocabulary\n",
    "    for word, index in word_to_index.items():\n",
    "        emb_matrix[index, :] = word_to_vec_map[word]\n",
    "\n",
    "    # Define Keras embedding layer with the correct output/input sizes, make it trainable. Use Embedding(...). Make sure to set trainable=False. \n",
    "    embedding_layer = Embedding(vocab_len, emb_dim, trainable = True)\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    # Build the embedding layer, it is required before setting the weights of the embedding layer. Do not modify the \"None\".\n",
    "    embedding_layer.build((None,))\n",
    "    \n",
    "    # Set the weights of the embedding layer to the embedding matrix. Your layer is now pretrained.\n",
    "    embedding_layer.set_weights([emb_matrix])\n",
    "    \n",
    "    return embedding_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "_uuid": "80e0aa982cf969d29f8a21193e208eb02408da93"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights[0][1][3] = -0.3403\n"
     ]
    }
   ],
   "source": [
    "embedding_layer = pretrained_embedding_layer(word_to_vec_map, word_to_index)\n",
    "print(\"weights[0][1][3] =\", embedding_layer.get_weights()[0][1][3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "_uuid": "f9059859daa58dddf01024342d95f0a77f3e26e1"
   },
   "outputs": [],
   "source": [
    "def Model_Text(encoder_input_shape, word_to_vec_map, word_to_index, decoder_input_shape):\n",
    "    \n",
    "    vocab_len = len(word_to_index) + 1                  # adding 1 to fit Keras embedding (requirement)\n",
    "    emb_dim = word_to_vec_map[\"cucumber\"].shape[0]      # define dimensionality of GloVe word vectors (= 50)\n",
    "    \n",
    "    sentence_indices = Input(encoder_input_shape, dtype='int32')\n",
    "    \n",
    "    embedding_layer = pretrained_embedding_layer(word_to_vec_map, word_to_index)\n",
    "    \n",
    "    embeddings = embedding_layer(sentence_indices) \n",
    "    \n",
    "    X, state_h, state_c = LSTM(emb_dim, return_state=True)(embeddings)\n",
    "    \n",
    "    encoder_states = [state_h, state_c]\n",
    "    \n",
    "    \n",
    "    #Set up the decoder, using `encoder_states` as initial state.\n",
    "\n",
    "    decoder_inputs = Input(decoder_input_shape, dtype = 'int32')\n",
    "    \n",
    "    \n",
    "    embedding_layer = pretrained_embedding_layer(word_to_vec_map, word_to_index)\n",
    "    embeddings = embedding_layer(decoder_inputs) \n",
    "    \n",
    "    x = LSTM(emb_dim, return_sequences = True)(embeddings, initial_state=encoder_states)\n",
    "    \n",
    "    decoder_outputs = Dense(vocab_len + 1, activation='softmax')(x)\n",
    "\n",
    "    \n",
    "    model = Model(inputs=[sentence_indices, decoder_inputs], outputs= decoder_outputs)\n",
    "    \n",
    "    \n",
    "    # Encoder Model for Inference\n",
    "    encoder_model = Model(sentence_indices, encoder_states)\n",
    "\n",
    "    \n",
    "    # Decoder Model for Inference\n",
    "    decoder_state_input_h = Input(shape=(emb_dim,), name = \"Inference_decoder_input_hidden_state\")\n",
    "    decoder_state_input_c = Input(shape=(emb_dim,), name = \"Inference_decoder_input_cell_state\")\n",
    "    decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "    \n",
    "    decoder_outputs_1, state_h, state_c = LSTM(emb_dim,\n",
    "                                               return_sequences=True, \n",
    "                                               return_state=True,\n",
    "                                               name = \"Inference_decoder_LSTM\")(embeddings, initial_state=decoder_states_inputs)\n",
    "    \n",
    "    decoder_states = [state_h, state_c]\n",
    "    \n",
    "    decoder_outputs_1 = Dense(vocab_len + 1, \n",
    "                              activation='softmax',\n",
    "                              kernel_initializer= glorot_uniform(seed = 0),\n",
    "                              bias_initializer='zeros',\n",
    "                              name = \"decoder_Dense_Output\")(decoder_outputs_1)\n",
    "    decoder_model = Model([decoder_inputs] + decoder_states_inputs,\n",
    "                          [decoder_outputs_1] + decoder_states)\n",
    "\n",
    "    return model, encoder_model, decoder_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "_uuid": "028c7323e0aec568b1a613e7750a9f8c6604a676"
   },
   "outputs": [],
   "source": [
    "concatenator = Concatenate(axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "_uuid": "ec781ef1d05b557fe5a0d6cdb2700c24edd5a552"
   },
   "outputs": [],
   "source": [
    "def Model_Text3(encoder_input_shape, word_to_vec_map, word_to_index, decoder_input_shape):\n",
    "    \n",
    "    vocab_len = len(word_to_index) + 1                  # adding 1 to fit Keras embedding (requirement)\n",
    "    emb_dim = word_to_vec_map[\"cucumber\"].shape[0]      # define dimensionality of GloVe word vectors (= 50)\n",
    "    \n",
    "    sentence_indices = Input(encoder_input_shape, dtype='int32')\n",
    "    \n",
    "    embedding_layer = pretrained_embedding_layer(word_to_vec_map, word_to_index)\n",
    "    \n",
    "    embeddings = embedding_layer(sentence_indices) \n",
    "    \n",
    "    X, _, _ = LSTM(emb_dim, return_state=True)(embeddings)\n",
    "    \n",
    "    \n",
    "    \n",
    "    #Set up the decoder, using `encoder_states` as initial state.\n",
    "    print(decoder_input_shape)\n",
    "    print(encoder_input_shape)\n",
    "    decoder_inputs = Input(decoder_input_shape, dtype='int32')\n",
    "    \n",
    "    \n",
    "    embedding_layer = pretrained_embedding_layer(word_to_vec_map, word_to_index)\n",
    "    \n",
    "    embeddings = embedding_layer(decoder_inputs)\n",
    "    \n",
    "    x, _, _ = LSTM(emb_dim, return_state=True)(embeddings)\n",
    "    \n",
    "    \n",
    "    decoder1 = concatenator([X, x])\n",
    "    \n",
    "    decoder_outputs = Dense(vocab_len + 1, activation='softmax')(decoder1)\n",
    "\n",
    "    \n",
    "    model = Model(inputs=[sentence_indices, decoder_inputs], outputs= decoder_outputs)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "_uuid": "55b6b67f02384d0de0d1bf1c7152975aed9073ce"
   },
   "outputs": [],
   "source": [
    "def Model_Text2(encoder_input_shape, word_to_vec_map, word_to_index, decoder_input_shape):\n",
    "    \n",
    "    vocab_len = len(word_to_index) + 1                  # adding 1 to fit Keras embedding (requirement)\n",
    "    emb_dim = word_to_vec_map[\"cucumber\"].shape[0]      # define dimensionality of GloVe word vectors (= 50)\n",
    "    \n",
    "    sentence_indices = Input(encoder_input_shape, dtype='int32')\n",
    "    \n",
    "    embedding_layer = pretrained_embedding_layer(word_to_vec_map, word_to_index)\n",
    "    \n",
    "    embeddings = embedding_layer(sentence_indices) \n",
    "    \n",
    "    X, _, _ = LSTM(emb_dim, return_state=True)(embeddings)\n",
    "    \n",
    "    \n",
    "    #x = LSTM(emb_dim)(X)\n",
    "    \n",
    "    decoder_outputs = Dense(vocab_len + 1, activation='softmax')(X)\n",
    "\n",
    "    \n",
    "    model = Model(inputs=sentence_indices, outputs= decoder_outputs)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "_uuid": "1ef6d5abc51408698e05f2d65fb77e619c4d0aa2",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_21 (InputLayer)           (None, 41)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_22 (InputLayer)           (None, 13)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_12 (Embedding)        (None, 41, 50)       20000050    input_21[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "embedding_13 (Embedding)        (None, 13, 50)       20000050    input_22[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lstm_16 (LSTM)                  [(None, 50), (None,  20200       embedding_12[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "lstm_17 (LSTM)                  (None, 13, 50)       20200       embedding_13[0][0]               \n",
      "                                                                 lstm_16[0][1]                    \n",
      "                                                                 lstm_16[0][2]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_19 (Dense)                (None, 13, 400002)   20400102    lstm_17[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 60,440,602\n",
      "Trainable params: 60,440,602\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"337pt\" viewBox=\"0.00 0.00 344.00 337.00\" width=\"344pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g class=\"graph\" id=\"graph0\" transform=\"scale(1 1) rotate(0) translate(4 333)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"white\" points=\"-4,4 -4,-333 340,-333 340,4 -4,4\" stroke=\"none\"/>\n",
       "<!-- 1712455982048 -->\n",
       "<g class=\"node\" id=\"node1\"><title>1712455982048</title>\n",
       "<polygon fill=\"none\" points=\"18.5,-292.5 18.5,-328.5 151.5,-328.5 151.5,-292.5 18.5,-292.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"85\" y=\"-306.8\">input_21: InputLayer</text>\n",
       "</g>\n",
       "<!-- 1712455982888 -->\n",
       "<g class=\"node\" id=\"node3\"><title>1712455982888</title>\n",
       "<polygon fill=\"none\" points=\"0,-219.5 0,-255.5 170,-255.5 170,-219.5 0,-219.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"85\" y=\"-233.8\">embedding_12: Embedding</text>\n",
       "</g>\n",
       "<!-- 1712455982048&#45;&gt;1712455982888 -->\n",
       "<g class=\"edge\" id=\"edge1\"><title>1712455982048-&gt;1712455982888</title>\n",
       "<path d=\"M85,-292.313C85,-284.289 85,-274.547 85,-265.569\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"88.5001,-265.529 85,-255.529 81.5001,-265.529 88.5001,-265.529\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 1712455963368 -->\n",
       "<g class=\"node\" id=\"node2\"><title>1712455963368</title>\n",
       "<polygon fill=\"none\" points=\"188.5,-219.5 188.5,-255.5 321.5,-255.5 321.5,-219.5 188.5,-219.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"255\" y=\"-233.8\">input_22: InputLayer</text>\n",
       "</g>\n",
       "<!-- 1712455953264 -->\n",
       "<g class=\"node\" id=\"node4\"><title>1712455953264</title>\n",
       "<polygon fill=\"none\" points=\"166,-146.5 166,-182.5 336,-182.5 336,-146.5 166,-146.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"251\" y=\"-160.8\">embedding_13: Embedding</text>\n",
       "</g>\n",
       "<!-- 1712455963368&#45;&gt;1712455953264 -->\n",
       "<g class=\"edge\" id=\"edge2\"><title>1712455963368-&gt;1712455953264</title>\n",
       "<path d=\"M254.032,-219.313C253.58,-211.289 253.031,-201.547 252.525,-192.569\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"256.016,-192.316 251.959,-182.529 249.027,-192.71 256.016,-192.316\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 1712453461328 -->\n",
       "<g class=\"node\" id=\"node5\"><title>1712453461328</title>\n",
       "<polygon fill=\"none\" points=\"35.5,-146.5 35.5,-182.5 140.5,-182.5 140.5,-146.5 35.5,-146.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"88\" y=\"-160.8\">lstm_16: LSTM</text>\n",
       "</g>\n",
       "<!-- 1712455982888&#45;&gt;1712453461328 -->\n",
       "<g class=\"edge\" id=\"edge3\"><title>1712455982888-&gt;1712453461328</title>\n",
       "<path d=\"M85.7262,-219.313C86.0652,-211.289 86.4769,-201.547 86.8562,-192.569\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"90.3551,-192.668 87.2805,-182.529 83.3613,-192.372 90.3551,-192.668\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 1712455951584 -->\n",
       "<g class=\"node\" id=\"node6\"><title>1712455951584</title>\n",
       "<polygon fill=\"none\" points=\"116.5,-73.5 116.5,-109.5 221.5,-109.5 221.5,-73.5 116.5,-73.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"169\" y=\"-87.8\">lstm_17: LSTM</text>\n",
       "</g>\n",
       "<!-- 1712455953264&#45;&gt;1712455951584 -->\n",
       "<g class=\"edge\" id=\"edge4\"><title>1712455953264-&gt;1712455951584</title>\n",
       "<path d=\"M231.15,-146.313C220.675,-137.243 207.663,-125.977 196.257,-116.1\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"198.518,-113.429 188.667,-109.529 193.936,-118.721 198.518,-113.429\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 1712453461328&#45;&gt;1712455951584 -->\n",
       "<g class=\"edge\" id=\"edge5\"><title>1712453461328-&gt;1712455951584</title>\n",
       "<path d=\"M107.608,-146.313C117.856,-137.33 130.561,-126.193 141.75,-116.386\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"144.36,-118.753 149.573,-109.529 139.746,-113.488 144.36,-118.753\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 1712667311576 -->\n",
       "<g class=\"node\" id=\"node7\"><title>1712667311576</title>\n",
       "<polygon fill=\"none\" points=\"113.5,-0.5 113.5,-36.5 224.5,-36.5 224.5,-0.5 113.5,-0.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"169\" y=\"-14.8\">dense_19: Dense</text>\n",
       "</g>\n",
       "<!-- 1712455951584&#45;&gt;1712667311576 -->\n",
       "<g class=\"edge\" id=\"edge7\"><title>1712455951584-&gt;1712667311576</title>\n",
       "<path d=\"M169,-73.3129C169,-65.2895 169,-55.5475 169,-46.5691\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"172.5,-46.5288 169,-36.5288 165.5,-46.5289 172.5,-46.5288\" stroke=\"black\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model, encoder_model, decoder_model = Model_Text((maxLen,), word_to_vec_map, word_to_index, (maxLen_Y,))\n",
    "model.summary()\n",
    "# Print Model Summary\n",
    "plot_model(model, to_file='model_word.png')\n",
    "SVG(model_to_dot(model).create(prog='dot', format='svg'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_21 (InputLayer)        (None, 41)                0         \n",
      "_________________________________________________________________\n",
      "embedding_12 (Embedding)     (None, 41, 50)            20000050  \n",
      "_________________________________________________________________\n",
      "lstm_16 (LSTM)               [(None, 50), (None, 50),  20200     \n",
      "=================================================================\n",
      "Total params: 20,020,250\n",
      "Trainable params: 20,020,250\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"191pt\" viewBox=\"0.00 0.00 178.00 191.00\" width=\"178pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g class=\"graph\" id=\"graph0\" transform=\"scale(1 1) rotate(0) translate(4 187)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"white\" points=\"-4,4 -4,-187 174,-187 174,4 -4,4\" stroke=\"none\"/>\n",
       "<!-- 1712455982048 -->\n",
       "<g class=\"node\" id=\"node1\"><title>1712455982048</title>\n",
       "<polygon fill=\"none\" points=\"18.5,-146.5 18.5,-182.5 151.5,-182.5 151.5,-146.5 18.5,-146.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"85\" y=\"-160.8\">input_21: InputLayer</text>\n",
       "</g>\n",
       "<!-- 1712455982888 -->\n",
       "<g class=\"node\" id=\"node2\"><title>1712455982888</title>\n",
       "<polygon fill=\"none\" points=\"0,-73.5 0,-109.5 170,-109.5 170,-73.5 0,-73.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"85\" y=\"-87.8\">embedding_12: Embedding</text>\n",
       "</g>\n",
       "<!-- 1712455982048&#45;&gt;1712455982888 -->\n",
       "<g class=\"edge\" id=\"edge1\"><title>1712455982048-&gt;1712455982888</title>\n",
       "<path d=\"M85,-146.313C85,-138.289 85,-128.547 85,-119.569\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"88.5001,-119.529 85,-109.529 81.5001,-119.529 88.5001,-119.529\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 1712453461328 -->\n",
       "<g class=\"node\" id=\"node3\"><title>1712453461328</title>\n",
       "<polygon fill=\"none\" points=\"32.5,-0.5 32.5,-36.5 137.5,-36.5 137.5,-0.5 32.5,-0.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"85\" y=\"-14.8\">lstm_16: LSTM</text>\n",
       "</g>\n",
       "<!-- 1712455982888&#45;&gt;1712453461328 -->\n",
       "<g class=\"edge\" id=\"edge2\"><title>1712455982888-&gt;1712453461328</title>\n",
       "<path d=\"M85,-73.3129C85,-65.2895 85,-55.5475 85,-46.5691\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"88.5001,-46.5288 85,-36.5288 81.5001,-46.5289 88.5001,-46.5288\" stroke=\"black\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(encoder_model.summary())\n",
    "SVG(model_to_dot(encoder_model).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_21 (InputLayer)        (None, 41)                0         \n",
      "_________________________________________________________________\n",
      "embedding_12 (Embedding)     (None, 41, 50)            20000050  \n",
      "_________________________________________________________________\n",
      "lstm_16 (LSTM)               [(None, 50), (None, 50),  20200     \n",
      "=================================================================\n",
      "Total params: 20,020,250\n",
      "Trainable params: 20,020,250\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"264pt\" viewBox=\"0.00 0.00 800.00 264.00\" width=\"800pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g class=\"graph\" id=\"graph0\" transform=\"scale(1 1) rotate(0) translate(4 260)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"white\" points=\"-4,4 -4,-260 796,-260 796,4 -4,4\" stroke=\"none\"/>\n",
       "<!-- 1712455963368 -->\n",
       "<g class=\"node\" id=\"node1\"><title>1712455963368</title>\n",
       "<polygon fill=\"none\" points=\"18.5,-219.5 18.5,-255.5 151.5,-255.5 151.5,-219.5 18.5,-219.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"85\" y=\"-233.8\">input_22: InputLayer</text>\n",
       "</g>\n",
       "<!-- 1712455953264 -->\n",
       "<g class=\"node\" id=\"node2\"><title>1712455953264</title>\n",
       "<polygon fill=\"none\" points=\"0,-146.5 0,-182.5 170,-182.5 170,-146.5 0,-146.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"85\" y=\"-160.8\">embedding_13: Embedding</text>\n",
       "</g>\n",
       "<!-- 1712455963368&#45;&gt;1712455953264 -->\n",
       "<g class=\"edge\" id=\"edge1\"><title>1712455963368-&gt;1712455953264</title>\n",
       "<path d=\"M85,-219.313C85,-211.289 85,-201.547 85,-192.569\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"88.5001,-192.529 85,-182.529 81.5001,-192.529 88.5001,-192.529\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 1712751524832 -->\n",
       "<g class=\"node\" id=\"node5\"><title>1712751524832</title>\n",
       "<polygon fill=\"none\" points=\"233.5,-73.5 233.5,-109.5 444.5,-109.5 444.5,-73.5 233.5,-73.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"339\" y=\"-87.8\">Inference_decoder_LSTM: LSTM</text>\n",
       "</g>\n",
       "<!-- 1712455953264&#45;&gt;1712751524832 -->\n",
       "<g class=\"edge\" id=\"edge2\"><title>1712455953264-&gt;1712751524832</title>\n",
       "<path d=\"M145.839,-146.494C182.485,-136.25 229.326,-123.157 267.704,-112.429\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"269.088,-115.677 277.777,-109.614 267.204,-108.935 269.088,-115.677\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 1711832090160 -->\n",
       "<g class=\"node\" id=\"node3\"><title>1711832090160</title>\n",
       "<polygon fill=\"none\" points=\"188,-146.5 188,-182.5 490,-182.5 490,-146.5 188,-146.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"339\" y=\"-160.8\">Inference_decoder_input_hidden_state: InputLayer</text>\n",
       "</g>\n",
       "<!-- 1711832090160&#45;&gt;1712751524832 -->\n",
       "<g class=\"edge\" id=\"edge3\"><title>1711832090160-&gt;1712751524832</title>\n",
       "<path d=\"M339,-146.313C339,-138.289 339,-128.547 339,-119.569\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"342.5,-119.529 339,-109.529 335.5,-119.529 342.5,-119.529\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 1712751521864 -->\n",
       "<g class=\"node\" id=\"node4\"><title>1712751521864</title>\n",
       "<polygon fill=\"none\" points=\"508,-146.5 508,-182.5 792,-182.5 792,-146.5 508,-146.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"650\" y=\"-160.8\">Inference_decoder_input_cell_state: InputLayer</text>\n",
       "</g>\n",
       "<!-- 1712751521864&#45;&gt;1712751524832 -->\n",
       "<g class=\"edge\" id=\"edge4\"><title>1712751521864-&gt;1712751524832</title>\n",
       "<path d=\"M575.508,-146.494C529.791,-136.057 471.114,-122.661 423.643,-111.824\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"424.221,-108.366 413.692,-109.552 422.663,-115.19 424.221,-108.366\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 1712751637336 -->\n",
       "<g class=\"node\" id=\"node6\"><title>1712751637336</title>\n",
       "<polygon fill=\"none\" points=\"243,-0.5 243,-36.5 435,-36.5 435,-0.5 243,-0.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"339\" y=\"-14.8\">decoder_Dense_Output: Dense</text>\n",
       "</g>\n",
       "<!-- 1712751524832&#45;&gt;1712751637336 -->\n",
       "<g class=\"edge\" id=\"edge5\"><title>1712751524832-&gt;1712751637336</title>\n",
       "<path d=\"M339,-73.3129C339,-65.2895 339,-55.5475 339,-46.5691\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"342.5,-46.5288 339,-36.5288 335.5,-46.5289 342.5,-46.5288\" stroke=\"black\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(encoder_model.summary())\n",
    "SVG(model_to_dot(decoder_model).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "_uuid": "5e868e819218f82da6c032d9e22985d87dbf8f1c"
   },
   "outputs": [],
   "source": [
    "rmsprop = optimizers.RMSprop(lr=0.002)\n",
    "# Compile & run training\n",
    "model.compile(optimizer=rmsprop, loss='categorical_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "_uuid": "aabf7a6de1e2a003de21b7c076479c16e1c9c43c",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "100/100 [==============================] - ETA: 6:12 - loss: 11.9089 - acc: 0.0000e+ - ETA: 3:58 - loss: 11.8949 - acc: 0.1090   - ETA: 3:07 - loss: 11.8752 - acc: 0.22 - ETA: 2:38 - loss: 11.8462 - acc: 0.29 - ETA: 2:16 - loss: 11.7939 - acc: 0.32 - ETA: 1:59 - loss: 11.7314 - acc: 0.33 - ETA: 1:45 - loss: 11.6427 - acc: 0.35 - ETA: 1:33 - loss: 11.5554 - acc: 0.36 - ETA: 1:21 - loss: 11.4256 - acc: 0.38 - ETA: 1:09 - loss: 11.3164 - acc: 0.39 - ETA: 57s - loss: 11.1980 - acc: 0.4021 - ETA: 47s - loss: 11.0949 - acc: 0.407 - ETA: 36s - loss: 10.9896 - acc: 0.411 - ETA: 26s - loss: 10.8741 - acc: 0.417 - ETA: 16s - loss: 10.7551 - acc: 0.420 - ETA: 6s - loss: 10.6421 - acc: 0.426 - 162s 2s/step - loss: 10.5704 - acc: 0.4246\n",
      "Epoch 2/5\n",
      "100/100 [==============================] - ETA: 2:16 - loss: 8.3645 - acc: 0.500 - ETA: 2:04 - loss: 8.1103 - acc: 0.506 - ETA: 1:55 - loss: 8.0678 - acc: 0.495 - ETA: 1:46 - loss: 8.0002 - acc: 0.487 - ETA: 1:37 - loss: 7.8910 - acc: 0.494 - ETA: 1:29 - loss: 7.8338 - acc: 0.489 - ETA: 1:20 - loss: 7.7214 - acc: 0.500 - ETA: 1:12 - loss: 7.6430 - acc: 0.503 - ETA: 1:03 - loss: 7.5957 - acc: 0.485 - ETA: 55s - loss: 7.5278 - acc: 0.485 - ETA: 47s - loss: 7.4813 - acc: 0.48 - ETA: 39s - loss: 7.3910 - acc: 0.47 - ETA: 30s - loss: 7.3274 - acc: 0.47 - ETA: 22s - loss: 7.2559 - acc: 0.47 - ETA: 14s - loss: 7.1874 - acc: 0.46 - ETA: 5s - loss: 7.1140 - acc: 0.4671 - 142s 1s/step - loss: 7.0805 - acc: 0.4646\n",
      "Epoch 3/5\n",
      "100/100 [==============================] - ETA: 2:12 - loss: 5.5262 - acc: 0.525 - ETA: 2:03 - loss: 5.3695 - acc: 0.532 - ETA: 1:57 - loss: 5.4030 - acc: 0.512 - ETA: 1:53 - loss: 5.3209 - acc: 0.519 - ETA: 1:45 - loss: 5.4139 - acc: 0.494 - ETA: 1:34 - loss: 5.3284 - acc: 0.487 - ETA: 1:25 - loss: 5.3016 - acc: 0.483 - ETA: 1:16 - loss: 5.2596 - acc: 0.477 - ETA: 1:07 - loss: 5.2499 - acc: 0.472 - ETA: 58s - loss: 5.1949 - acc: 0.473 - ETA: 49s - loss: 5.1836 - acc: 0.46 - ETA: 40s - loss: 5.1402 - acc: 0.46 - ETA: 31s - loss: 5.1200 - acc: 0.46 - ETA: 22s - loss: 5.0911 - acc: 0.46 - ETA: 14s - loss: 5.0568 - acc: 0.46 - ETA: 5s - loss: 5.0194 - acc: 0.4655 - 143s 1s/step - loss: 4.9984 - acc: 0.4646\n",
      "Epoch 4/5\n",
      "100/100 [==============================] - ETA: 2:04 - loss: 4.4382 - acc: 0.500 - ETA: 1:58 - loss: 4.1833 - acc: 0.500 - ETA: 1:50 - loss: 4.1104 - acc: 0.504 - ETA: 1:42 - loss: 4.2247 - acc: 0.464 - ETA: 1:34 - loss: 4.2117 - acc: 0.469 - ETA: 1:26 - loss: 4.1868 - acc: 0.470 - ETA: 1:18 - loss: 4.1845 - acc: 0.470 - ETA: 1:10 - loss: 4.1929 - acc: 0.466 - ETA: 1:02 - loss: 4.2008 - acc: 0.463 - ETA: 54s - loss: 4.1701 - acc: 0.466 - ETA: 46s - loss: 4.1502 - acc: 0.46 - ETA: 38s - loss: 4.1760 - acc: 0.46 - ETA: 29s - loss: 4.1068 - acc: 0.46 - ETA: 21s - loss: 4.1205 - acc: 0.46 - ETA: 13s - loss: 4.1422 - acc: 0.46 - ETA: 5s - loss: 4.1665 - acc: 0.4623 - 137s 1s/step - loss: 4.1503 - acc: 0.4646\n",
      "Epoch 5/5\n",
      "100/100 [==============================] - ETA: 2:08 - loss: 4.0928 - acc: 0.435 - ETA: 2:06 - loss: 3.9098 - acc: 0.480 - ETA: 1:54 - loss: 3.9882 - acc: 0.470 - ETA: 1:45 - loss: 3.8862 - acc: 0.480 - ETA: 1:37 - loss: 3.8290 - acc: 0.487 - ETA: 1:29 - loss: 3.8952 - acc: 0.476 - ETA: 1:21 - loss: 3.8320 - acc: 0.489 - ETA: 1:12 - loss: 3.8089 - acc: 0.490 - ETA: 1:04 - loss: 3.8491 - acc: 0.482 - ETA: 55s - loss: 3.8845 - acc: 0.476 - ETA: 47s - loss: 3.8889 - acc: 0.47 - ETA: 38s - loss: 3.9182 - acc: 0.46 - ETA: 30s - loss: 3.9040 - acc: 0.46 - ETA: 22s - loss: 3.9448 - acc: 0.45 - ETA: 13s - loss: 3.9439 - acc: 0.45 - ETA: 5s - loss: 3.9015 - acc: 0.4655 - 139s 1s/step - loss: 3.9141 - acc: 0.4646\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x18dc5581630>"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([X1, Y1], decoder_target_data, batch_size = 6, epochs = 5, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "100/100 [==============================] - ETA: 2:10 - loss: 3.4168 - acc: 0.487 - ETA: 2:02 - loss: 3.5411 - acc: 0.467 - ETA: 1:53 - loss: 3.5712 - acc: 0.474 - ETA: 1:44 - loss: 3.6777 - acc: 0.458 - ETA: 1:36 - loss: 3.7065 - acc: 0.456 - ETA: 1:28 - loss: 3.7533 - acc: 0.450 - ETA: 1:20 - loss: 3.8413 - acc: 0.441 - ETA: 1:12 - loss: 3.8035 - acc: 0.447 - ETA: 1:04 - loss: 3.7928 - acc: 0.448 - ETA: 55s - loss: 3.7556 - acc: 0.451 - ETA: 47s - loss: 3.6839 - acc: 0.46 - ETA: 39s - loss: 3.6377 - acc: 0.47 - ETA: 30s - loss: 3.6064 - acc: 0.48 - ETA: 22s - loss: 3.6689 - acc: 0.47 - ETA: 13s - loss: 3.7224 - acc: 0.46 - ETA: 5s - loss: 3.7434 - acc: 0.4639 - 140s 1s/step - loss: 3.7408 - acc: 0.4646\n",
      "Epoch 2/5\n",
      "100/100 [==============================] - ETA: 2:10 - loss: 3.4598 - acc: 0.474 - ETA: 2:01 - loss: 3.5369 - acc: 0.461 - ETA: 1:53 - loss: 3.5034 - acc: 0.474 - ETA: 1:45 - loss: 3.5641 - acc: 0.464 - ETA: 1:36 - loss: 3.5255 - acc: 0.476 - ETA: 1:28 - loss: 3.4711 - acc: 0.489 - ETA: 1:19 - loss: 3.4153 - acc: 0.496 - ETA: 1:11 - loss: 3.5171 - acc: 0.482 - ETA: 1:03 - loss: 3.5301 - acc: 0.482 - ETA: 55s - loss: 3.5620 - acc: 0.479 - ETA: 46s - loss: 3.5669 - acc: 0.47 - ETA: 38s - loss: 3.5797 - acc: 0.47 - ETA: 30s - loss: 3.5984 - acc: 0.47 - ETA: 22s - loss: 3.5909 - acc: 0.46 - ETA: 13s - loss: 3.5883 - acc: 0.47 - ETA: 5s - loss: 3.6216 - acc: 0.4647 - 138s 1s/step - loss: 3.6165 - acc: 0.4646\n",
      "Epoch 3/5\n",
      "100/100 [==============================] - ETA: 2:04 - loss: 3.2554 - acc: 0.474 - ETA: 1:58 - loss: 3.2072 - acc: 0.487 - ETA: 1:51 - loss: 3.2536 - acc: 0.478 - ETA: 1:43 - loss: 3.1721 - acc: 0.487 - ETA: 1:35 - loss: 3.2303 - acc: 0.492 - ETA: 1:28 - loss: 3.2423 - acc: 0.489 - ETA: 1:19 - loss: 3.3339 - acc: 0.481 - ETA: 1:11 - loss: 3.3865 - acc: 0.477 - ETA: 1:03 - loss: 3.4035 - acc: 0.475 - ETA: 54s - loss: 3.4071 - acc: 0.478 - ETA: 46s - loss: 3.4019 - acc: 0.48 - ETA: 38s - loss: 3.4448 - acc: 0.47 - ETA: 30s - loss: 3.4644 - acc: 0.47 - ETA: 21s - loss: 3.4767 - acc: 0.47 - ETA: 13s - loss: 3.5070 - acc: 0.46 - ETA: 5s - loss: 3.5112 - acc: 0.4647 - 137s 1s/step - loss: 3.5169 - acc: 0.4646\n",
      "Epoch 4/5\n",
      "100/100 [==============================] - ETA: 2:11 - loss: 3.2928 - acc: 0.448 - ETA: 2:01 - loss: 3.5359 - acc: 0.423 - ETA: 1:53 - loss: 3.6215 - acc: 0.414 - ETA: 1:44 - loss: 3.5704 - acc: 0.426 - ETA: 1:36 - loss: 3.4577 - acc: 0.446 - ETA: 1:27 - loss: 3.3688 - acc: 0.461 - ETA: 1:18 - loss: 3.3533 - acc: 0.467 - ETA: 1:10 - loss: 3.4137 - acc: 0.459 - ETA: 1:03 - loss: 3.3887 - acc: 0.463 - ETA: 54s - loss: 3.4085 - acc: 0.465 - ETA: 46s - loss: 3.4317 - acc: 0.46 - ETA: 38s - loss: 3.4758 - acc: 0.45 - ETA: 30s - loss: 3.4643 - acc: 0.45 - ETA: 22s - loss: 3.4393 - acc: 0.46 - ETA: 13s - loss: 3.4585 - acc: 0.45 - ETA: 5s - loss: 3.4310 - acc: 0.4655 - 139s 1s/step - loss: 3.4322 - acc: 0.4646\n",
      "Epoch 5/5\n",
      "100/100 [==============================] - ETA: 2:09 - loss: 3.3168 - acc: 0.410 - ETA: 1:59 - loss: 3.4612 - acc: 0.429 - ETA: 1:51 - loss: 3.3822 - acc: 0.448 - ETA: 1:43 - loss: 3.3686 - acc: 0.448 - ETA: 1:35 - loss: 3.2933 - acc: 0.461 - ETA: 1:29 - loss: 3.2340 - acc: 0.476 - ETA: 1:20 - loss: 3.3435 - acc: 0.457 - ETA: 1:12 - loss: 3.3496 - acc: 0.461 - ETA: 1:03 - loss: 3.3868 - acc: 0.457 - ETA: 55s - loss: 3.3801 - acc: 0.460 - ETA: 47s - loss: 3.3256 - acc: 0.47 - ETA: 38s - loss: 3.3291 - acc: 0.46 - ETA: 30s - loss: 3.3470 - acc: 0.46 - ETA: 22s - loss: 3.3386 - acc: 0.46 - ETA: 13s - loss: 3.3716 - acc: 0.46 - ETA: 5s - loss: 3.3503 - acc: 0.4671 - 139s 1s/step - loss: 3.3693 - acc: 0.4646\n"
     ]
    }
   ],
   "source": [
    "history = model.fit([X1, Y1], decoder_target_data, batch_size = 6, epochs = 5, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot training & validation accuracy values\n",
    "plt.plot(history.history['acc'])\n",
    "#plt.plot(history.history['val_acc'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.plot(history.history['loss'])\n",
    "#plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "]-\\plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_word_sequence(input_seq):\n",
    "    # Encode the input as state vectors.\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros(( 1, 13))\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    #target_seq[0, 0] = word_to_index['\\t']\n",
    "    \n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    i = 0\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict(\n",
    "            [target_seq] + states_value)\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, 0, :])\n",
    "        sampled_word = index_to_word[sampled_token_index]\n",
    "        decoded_sentence += sampled_word + \" \"\n",
    "\n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if (sampled_word == '\\n' or\n",
    "           len(decoded_sentence) > 41):\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1, 13))\n",
    "        target_seq[0, i] = sampled_token_index\n",
    "        i += 1\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 41)\n",
      "-\n",
      "Input sentence: australia 's current account deficit shrunk by a record # . ## billion dollars   # . ## billion us   in the june quarter due to soaring commodity prices , figures released monday showed . \n",
      "Decoded sentence: cause-and-effect bacall someshwara someshwara \n",
      "(1, 41)\n",
      "-\n",
      "Input sentence: at least two people were killed in a suspected bomb attack on a passenger bus in the strife - torn southern philippines on monday , the military said . \n",
      "Decoded sentence: cause-and-effect bacall someshwara someshwara \n",
      "(1, 41)\n",
      "-\n",
      "Input sentence: australian shares closed down # . # percent monday following a weak lead from the united states and lower commodity prices , dealers said . \n",
      "Decoded sentence: cause-and-effect bacall someshwara someshwara \n",
      "(1, 41)\n",
      "-\n",
      "Input sentence: south korea 's nuclear envoy kim sook urged north korea monday to restart work to disable its nuclear plants and stop its `` typical '' brinkmanship in negotiations . \n",
      "Decoded sentence: cause-and-effect bacall someshwara someshwara \n",
      "(1, 41)\n",
      "-\n",
      "Input sentence: south korea on monday announced sweeping tax reforms , including income and corporate tax cuts to boost growth by stimulating sluggish private consumption and business investment . \n",
      "Decoded sentence: cause-and-effect ratangarh someshwara someshwara \n"
     ]
    }
   ],
   "source": [
    "for seq_index in range(5):\n",
    "\n",
    "    # Take one sequence (part of the training set) for trying out decoding.\n",
    "    input_seq = X1[seq_index: seq_index + 1]\n",
    "    print(input_seq.shape)\n",
    "    decoded_sentence = decode_word_sequence(input_seq)\n",
    "\n",
    "    print('-')\n",
    "    print('Input sentence:', X[seq_index])\n",
    "    print('Decoded sentence:', decoded_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "_uuid": "c718df170fa508d6e5649cc4de3b7c728e5bad1e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_target_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "_uuid": "2895c7fbf13b49003db453dcea6d88df70ed5c90"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - ETA: 2: - ETA: 50s - ETA: 4 - 117s 1s/step\n",
      "Accuracy0.46461538195610047\n"
     ]
    }
   ],
   "source": [
    "loss, acc = model.evaluate([X1 , Y1], decoder_target_data[:,:,:])\n",
    "print(\"Accuracy\" + str(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "_uuid": "a73d4970b7c0b2a43da9887a763603e1d287aa0e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.46461538195610047"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "_uuid": "84e550cfe4bbb00ebe457f06e9475b4153b333da",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 13, 400002)\n",
      "[[[3.9995812e-02 1.9187546e-06 1.9409463e-06 ... 1.9284203e-06\n",
      "   1.9194865e-06 1.9712222e-06]\n",
      "  [3.9996184e-02 1.9187485e-06 1.9409422e-06 ... 1.9284159e-06\n",
      "   1.9194804e-06 1.9712161e-06]\n",
      "  [3.9996240e-02 1.9187494e-06 1.9409392e-06 ... 1.9284153e-06\n",
      "   1.9194797e-06 1.9712152e-06]\n",
      "  ...\n",
      "  [3.9996248e-02 1.9187478e-06 1.9409397e-06 ... 1.9284155e-06\n",
      "   1.9194799e-06 1.9712154e-06]\n",
      "  [3.9996248e-02 1.9187496e-06 1.9409397e-06 ... 1.9284155e-06\n",
      "   1.9194799e-06 1.9712154e-06]\n",
      "  [3.9996248e-02 1.9187496e-06 1.9409397e-06 ... 1.9284155e-06\n",
      "   1.9194817e-06 1.9712154e-06]]\n",
      "\n",
      " [[3.9995898e-02 1.9187530e-06 1.9409429e-06 ... 1.9284189e-06\n",
      "   1.9194813e-06 1.9712188e-06]\n",
      "  [3.9996196e-02 1.9187489e-06 1.9409408e-06 ... 1.9284166e-06\n",
      "   1.9194811e-06 1.9712165e-06]\n",
      "  [3.9996248e-02 1.9187478e-06 1.9409397e-06 ... 1.9284155e-06\n",
      "   1.9194799e-06 1.9712154e-06]\n",
      "  ...\n",
      "  [3.9996266e-02 1.9187489e-06 1.9409406e-06 ... 1.9284166e-06\n",
      "   1.9194811e-06 1.9712165e-06]\n",
      "  [3.9996266e-02 1.9187505e-06 1.9409406e-06 ... 1.9284164e-06\n",
      "   1.9194808e-06 1.9712163e-06]\n",
      "  [3.9996266e-02 1.9187505e-06 1.9409406e-06 ... 1.9284164e-06\n",
      "   1.9194808e-06 1.9712163e-06]]\n",
      "\n",
      " [[3.9994713e-02 1.9187621e-06 1.9409540e-06 ... 1.9284334e-06\n",
      "   1.9194940e-06 1.9712320e-06]\n",
      "  [3.9995976e-02 1.9187496e-06 1.9409413e-06 ... 1.9284189e-06\n",
      "   1.9194815e-06 1.9712172e-06]\n",
      "  [3.9996218e-02 1.9187501e-06 1.9409420e-06 ... 1.9284178e-06\n",
      "   1.9194820e-06 1.9712177e-06]\n",
      "  ...\n",
      "  [3.9996266e-02 1.9187489e-06 1.9409406e-06 ... 1.9284166e-06\n",
      "   1.9194811e-06 1.9712165e-06]\n",
      "  [3.9996266e-02 1.9187487e-06 1.9409406e-06 ... 1.9284164e-06\n",
      "   1.9194808e-06 1.9712163e-06]\n",
      "  [3.9996259e-02 1.9187503e-06 1.9409401e-06 ... 1.9284162e-06\n",
      "   1.9194806e-06 1.9712161e-06]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[3.9995257e-02 1.9187551e-06 1.9409490e-06 ... 1.9284248e-06\n",
      "   1.9194872e-06 1.9712229e-06]\n",
      "  [3.9996110e-02 1.9187485e-06 1.9409422e-06 ... 1.9284162e-06\n",
      "   1.9194806e-06 1.9712161e-06]\n",
      "  [3.9978333e-02 1.9188526e-06 1.9410436e-06 ... 1.9285023e-06\n",
      "   1.9195718e-06 1.9713043e-06]\n",
      "  ...\n",
      "  [3.9996147e-02 1.9187503e-06 1.9409385e-06 ... 1.9284180e-06\n",
      "   1.9194806e-06 1.9712163e-06]\n",
      "  [3.9996114e-02 1.9187487e-06 1.9409406e-06 ... 1.9284164e-06\n",
      "   1.9194811e-06 1.9712165e-06]\n",
      "  [3.9996099e-02 1.9187480e-06 1.9409397e-06 ... 1.9284155e-06\n",
      "   1.9194802e-06 1.9712156e-06]]\n",
      "\n",
      " [[3.9643925e-02 1.9195245e-06 1.9422253e-06 ... 1.9297847e-06\n",
      "   1.9206486e-06 1.9725983e-06]\n",
      "  [3.9996084e-02 1.9187491e-06 1.9409429e-06 ... 1.9284187e-06\n",
      "   1.9194813e-06 1.9712188e-06]\n",
      "  [3.9501268e-02 1.9200388e-06 1.9421213e-06 ... 1.9299173e-06\n",
      "   1.9210515e-06 1.9727411e-06]\n",
      "  ...\n",
      "  [3.9996248e-02 1.9187496e-06 1.9409397e-06 ... 1.9284155e-06\n",
      "   1.9194799e-06 1.9712154e-06]\n",
      "  [3.9996248e-02 1.9187496e-06 1.9409415e-06 ... 1.9284155e-06\n",
      "   1.9194817e-06 1.9712174e-06]\n",
      "  [3.9996210e-02 1.9187496e-06 1.9409415e-06 ... 1.9284173e-06\n",
      "   1.9194817e-06 1.9712174e-06]]\n",
      "\n",
      " [[3.9995976e-02 1.9187512e-06 1.9409431e-06 ... 1.9284189e-06\n",
      "   1.9194833e-06 1.9712172e-06]\n",
      "  [3.9996218e-02 1.9187501e-06 1.9409420e-06 ... 1.9284178e-06\n",
      "   1.9194820e-06 1.9712177e-06]\n",
      "  [3.9996266e-02 1.9187489e-06 1.9409406e-06 ... 1.9284166e-06\n",
      "   1.9194811e-06 1.9712165e-06]\n",
      "  ...\n",
      "  [3.9996248e-02 1.9187496e-06 1.9409397e-06 ... 1.9284173e-06\n",
      "   1.9194799e-06 1.9712154e-06]\n",
      "  [3.9996248e-02 1.9187514e-06 1.9409397e-06 ... 1.9284173e-06\n",
      "   1.9194799e-06 1.9712174e-06]\n",
      "  [3.9996222e-02 1.9187503e-06 1.9409404e-06 ... 1.9284180e-06\n",
      "   1.9194806e-06 1.9712181e-06]]]\n",
      "{'0': ['!', '!', '!', '!', '!', '!', '!', '!', '!', '!', '!', '!', '!'], '1': ['!', '!', '!', '!', '!', '!', '!', '!', '!', '!', '!', '!', '!'], '2': ['!', '!', '!', '!', '!', '!', '!', '!', '!', '!', '!', '!', '!'], '3': ['!', '!', '!', '!', '!', '!', '!', '!', '!', '!', '!', '!', '!'], '4': ['!', '!', '!', '!', '!', '!', '!', '!', '!', '!', '!', '!', '!'], '5': ['!', '!', '!', '!', '!', '!', '!', '!', '!', '!', '!', '!', '!'], '6': ['!', '!', '!', '!', '!', '!', '!', '!', '!', '!', '!', '!', '!'], '7': ['!', '!', '!', '!', '!', '!', '!', '!', '!', '!', '!', '!', '!'], '8': ['!', '!', '!', '!', '!', '!', '!', '!', '!', '!', '!', '!', '!'], '9': ['!', '!', '!', '!', '!', '!', '!', '!', '!', '!', '!', '!', '!']}\n"
     ]
    }
   ],
   "source": [
    "idx = model.predict([X1[:10,:], Y1[:10,:]], verbose = 0)\n",
    "print(idx.shape)\n",
    "print(idx)\n",
    "id1 = {}\n",
    "for i in range(idx.shape[0]):\n",
    "    id1[str(i)] = []\n",
    "    for j in range(idx.shape[1]):\n",
    "        try:\n",
    "            id1[str(i)].append(index_to_word[np.argmax(idx[i,j,:])])\n",
    "        except Exception as e:\n",
    "            id1[str(i)].append(index_to_word[np.argmax(idx[i,j,:]) + 1])\n",
    "            pass\n",
    "print(id1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for seq_index in range(5):\n",
    "\n",
    "    # Take one sequence (part of the training set) for trying out decoding.\n",
    "    input_seq = encoder_input_data[seq_index: seq_index + 1]\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "\n",
    "    print('-')\n",
    "    print('Input sentence:', input_texts[seq_index])\n",
    "    print('Decoded sentence:', decoded_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "4c1aa4eaf7e7fc182a7a9cf72cf76712a81026fe"
   },
   "outputs": [],
   "source": [
    "idx = model.predict([X1, Y1], verbose = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "60360c9d31c4fc6d386d0890e09d58d1abcceb47"
   },
   "outputs": [],
   "source": [
    "print(idx.shape)\n",
    "id = []\n",
    "for i in range(idx.shape[0]):\n",
    "    id.append(index_to_word[np.argmax(idx[i,:])])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "937a3d33385299e14f9a5408ed8e5a1d4b6c5d8b"
   },
   "outputs": [],
   "source": [
    "id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8c91b0b9469281cc380b296a7d8e9543182127af"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
