{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "kernel1.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "metadata": {
        "_uuid": "6ab092a614aeaf674ef51af6c23badcfcadbac73",
        "id": "3o0yiA5Huu1W",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This Notebook is a Sequence-to-Sequence Model for Text Summarization task using Attention Model"
      ]
    },
    {
      "metadata": {
        "_uuid": "850b9874d086f145a3d3dfeaeab4100e24bceb84",
        "id": "RZcjVq4Puu1X",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from keras.models import Model\n",
        "from keras.layers import Dense, Input, Dropout, LSTM, Activation, Bidirectional, Concatenate, Permute, Dot, Multiply\n",
        "from keras.layers import RepeatVector, Lambda, TimeDistributed, merge, Reshape\n",
        "from keras.layers import multiply as mul\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras import optimizers\n",
        "from keras.utils import to_categorical\n",
        "from keras.initializers import glorot_uniform\n",
        "import matplotlib.pyplot as plt\n",
        "from utils import *\n",
        "import pandas as pd\n",
        "#from nltk.corpus import stopwords\n",
        "from pickle import dump, load\n",
        "import re\n",
        "from IPython.display import SVG\n",
        "from keras.utils.vis_utils import model_to_dot\n",
        "from keras.utils import plot_model\n",
        "#from attention import AttentionDecoder"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "248397c756765fcdf11ee47cb9844adfc5edfb08",
        "id": "QE6L4Tikuu1b",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "reviews = pd.read_csv(\"dataset/Reviews.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "b1346c4221e5c02b619265d9788e9168086a4a30",
        "id": "VUPcEHGAuu1d",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(reviews.shape)\n",
        "print(reviews.head())\n",
        "print(reviews.isnull().sum())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "78067bc72a29493f7fd7e7a8fa56fe8d4e552f22",
        "id": "4ZYednifuu1g",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "reviews = reviews.dropna()\n",
        "reviews = reviews.drop(['Id','ProductId','UserId','ProfileName','HelpfulnessNumerator','HelpfulnessDenominator', 'Score','Time'], 1)\n",
        "reviews = reviews.reset_index(drop=True) \n",
        "print(reviews.head())\n",
        "for i in range(5):\n",
        "    print(\"Review #\",i+1)\n",
        "    print(reviews.Summary[i])\n",
        "    print(reviews.Text[i])\n",
        "    print()\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "db0331b9fa896a8bd10d3f2f317779336d63f3f6",
        "id": "2sr0QH8zuu1i",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "contractions = {\"ain't\": \"am not\",\n",
        "                \"aren't\": \"are not\",\n",
        "                \"can't\": \"cannot\",\n",
        "                \"can't've\": \"cannot have\",\n",
        "                \"'cause\": \"because\",\n",
        "                \"could've\": \"could have\",\n",
        "                \"couldn't\": \"could not\",\n",
        "                \"couldn't've\": \"could not have\",\n",
        "                \"didn't\": \"did not\",\n",
        "                \"doesn't\": \"does not\",\n",
        "                \"don't\": \"do not\",\n",
        "                \"hadn't\": \"had not\",\n",
        "                \"hadn't've\": \"had not have\",\n",
        "                \"hasn't\": \"has not\",\n",
        "                \"haven't\": \"have not\",\n",
        "                \"he'd\": \"he would\",\n",
        "                \"he'd've\": \"he would have\"}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "26f764fa33ad7f5587361a8735dcae98729a73ac",
        "id": "PKljolX2uu1l",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def clean_text(text, remove_stopwords = True):\n",
        "    # Convert words to lower case\n",
        "    text = text.lower()\n",
        "    if True:\n",
        "        text = text.split()\n",
        "        new_text = []\n",
        "        for word in text:\n",
        "            if word in contractions:\n",
        "                new_text.append(contractions[word])\n",
        "            else:\n",
        "                new_text.append(word)\n",
        "        text = \" \".join(new_text)\n",
        "        text = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', text, flags=re.MULTILINE)\n",
        "        text = re.sub(r'\\<a href', ' ', text)\n",
        "        text = re.sub(r'</code>&<code>', '', text)\n",
        "        text = re.sub(r'[_\"\\-;%()|+&=*%!?:#$@\\[\\]/]', ' ', text)\n",
        "        text = re.sub(r'</code><br /><code>', ' ', text)\n",
        "        text = re.sub(r'\\'', ' ', text)\n",
        "        if remove_stopwords:\n",
        "            text = text.split()\n",
        "            stops = set(stopwords.words(\"english\"))\n",
        "            text = [w for w in text if not w in stops]\n",
        "            text = \" \".join(text)\n",
        "        return text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "414921f6201e0f11526be71e752ff832ffe8aee4",
        "id": "zin16tNquu1n",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "clean_summaries = []\n",
        "for summary in reviews.Summary:\n",
        "    clean_summaries.append(clean_text(summary, remove_stopwords=False))\n",
        "print(\"Summaries are complete.\")\n",
        "clean_texts = []\n",
        "for text in reviews.Text:\n",
        "    clean_texts.append(clean_text(text , remove_stopwords = False))\n",
        "print(\"Texts are complete.\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "2bdf20304debc5eb52457092c02813cba3e49fdd",
        "id": "KxYzuHl2uu1q",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "stories = list()\n",
        "for i, text in enumerate(clean_texts):\n",
        "    stories.append({'story': text, 'highlights': clean_summaries[i]})\n",
        "# save to file\n",
        "dump(stories, open('review_dataset2.pkl', 'wb'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "caccdd6968f4e9acf6a309f848881ad82abb02de",
        "id": "6_hJpgyIuu1t",
        "colab_type": "code",
        "colab": {},
        "outputId": "267002e3-9e87-4eae-bc06-882ecb27f4a7"
      },
      "cell_type": "code",
      "source": [
        "stories = load(open('review_dataset2.pkl', 'rb'))\n",
        "print('Loaded Stories %d' % len(stories))\n",
        "print(type(stories))\n",
        "print(stories[3])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loaded Stories 568411\n",
            "<class 'list'>\n",
            "{'story': 'if you are looking for the secret ingredient in robitussin i believe i have found it. i got this in addition to the root beer extract i ordered  which was good  and made some cherry soda. the flavor is very medicinal.', 'highlights': 'cough medicine'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "73582ce9a5d32f177ad92225046ddb1f0d65bea0",
        "id": "MHNwwnlmuu10",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "batch_size = 64\n",
        "epochs = 110\n",
        "latent_dim = 256\n",
        "num_samples = 10000"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "e5446ec6699d41c3c776ec48ff1c95e430ba1a1f",
        "id": "2YOasTbvuu11",
        "colab_type": "code",
        "colab": {},
        "outputId": "dc3c08dd-b2f1-4990-fcf3-4d23006f97ad"
      },
      "cell_type": "code",
      "source": [
        "input_texts = []\n",
        "target_texts = []\n",
        "input_characters = set()\n",
        "target_characters = set()\n",
        "for story in stories[:50]:\n",
        "    input_text = story['story']\n",
        "    target_text = story['highlights']\n",
        "    # We use \"tab\" as the \"start sequence\" character\n",
        "    # for the targets, and \"\\n\" as \"end sequence\" character.\n",
        "    target_text = '\\t' + target_text + '\\n'\n",
        "    input_texts.append(input_text)\n",
        "    target_texts.append(target_text)\n",
        "    for char in input_text:\n",
        "        if char not in input_characters:\n",
        "            input_characters.add(char)\n",
        "    for char in target_text:\n",
        "        if char not in target_characters:\n",
        "            target_characters.add(char)\n",
        "input_characters = sorted(list(input_characters))\n",
        "target_characters = sorted(list(target_characters))\n",
        "num_encoder_tokens = len(input_characters)\n",
        "num_decoder_tokens = len(target_characters)\n",
        "max_encoder_seq_length = max([len(txt) for txt in input_texts])\n",
        "max_decoder_seq_length = max([len(txt) for txt in target_texts])\n",
        "print('Number of samples:', len(input_texts))\n",
        "print('Number of unique input tokens:', num_encoder_tokens)\n",
        "print('Number of unique output tokens:', num_decoder_tokens)\n",
        "print('Max sequence length for inputs:', max_encoder_seq_length)\n",
        "print('Max sequence length for outputs:', max_decoder_seq_length)\n",
        "print(len(input_texts))\n",
        "print(len(target_texts))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of samples: 50\n",
            "Number of unique input tokens: 41\n",
            "Number of unique output tokens: 31\n",
            "Max sequence length for inputs: 1221\n",
            "Max sequence length for outputs: 62\n",
            "50\n",
            "50\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "DRfyAIiMuu15",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "8d1f484667466453bf5152c3613e0c0863e81a14",
        "id": "Mj-WHaUWuu17",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "input_token_index = dict([(char, i) for i, char in enumerate(input_characters)])\n",
        "\n",
        "target_token_index = dict([(char, i) for i, char in enumerate(target_characters)])\n",
        "reverse_target_char_index = dict([(i, char) for i, char in enumerate(target_characters)])\n",
        "\n",
        "\n",
        "encoder_input_data = np.zeros((len(input_texts), max_encoder_seq_length, num_encoder_tokens), dtype='float32')\n",
        "\n",
        "decoder_input_data = np.zeros((len(input_texts), max_decoder_seq_length, num_decoder_tokens), dtype='float32')\n",
        "\n",
        "decoder_target_data = np.zeros((len(input_texts), max_decoder_seq_length, num_decoder_tokens), dtype='float32')\n",
        "\n",
        "\n",
        "\n",
        "for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
        "    for t, char in enumerate(input_text):\n",
        "        encoder_input_data[i, t, input_token_index[char]] = 1.\n",
        "        \n",
        "    for t, char in enumerate(target_text):\n",
        "\n",
        "        # decoder_target_data is ahead of decoder_input_data by one timestep\n",
        "        decoder_input_data[i, t, target_token_index[char]] = 1.\n",
        "        if t > 0:\n",
        "            # decoder_target_data will be ahead by one timestep\n",
        "            # and will not include the start character.\n",
        "            decoder_target_data[i, t - 1, target_token_index[char]] = 1."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "elxsCT1Wuu19",
        "colab_type": "code",
        "colab": {},
        "outputId": "dd89a476-fc11-4c71-cd8d-822c02246454"
      },
      "cell_type": "code",
      "source": [
        "print(encoder_input_data.shape)\n",
        "print(decoder_input_data.shape)\n",
        "print(decoder_target_data.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(50, 1221, 41)\n",
            "(50, 62, 31)\n",
            "(50, 62, 31)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "lqkvzwOouu2A",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "repeator = RepeatVector(2297)\n",
        "concatenator = Concatenate(axis=-1)\n",
        "densor1 = Dense(10, activation = \"tanh\")\n",
        "densor2 = Dense(1, activation = \"relu\")\n",
        "activator = Activation(\"softmax\", name='attention_weights') # We are using a custom softmax(axis = 1) loaded in this notebook\n",
        "dotor = Dot(axes = 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "ca2a387507631fcbda21570948fc77d10884c773",
        "id": "D-yO8Fijuu2D",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def Character_Model(num_encoder_tokens, num_decoder_tokens, latent_dim):\n",
        "    # Define an input sequence and process it.\n",
        "\n",
        "    encoder_inputs = Input(shape=(None, num_encoder_tokens), name = \"encoder_input\")\n",
        "    #encoder_output = Bidirectional(LSTM(latent_dim,\n",
        "    #                                        return_sequences=True,\n",
        "    #                                        kernel_initializer = glorot_uniform(seed = 0),\n",
        "    #                                        bias_initializer ='zeros',\n",
        "    #                                        name = \"Bi-LSTM_output\"),\n",
        "    #                              name = \"Bidirectional\")(encoder_inputs)\n",
        "    \n",
        "    encoder_output,state_h, state_c = LSTM(256,\n",
        "                                            return_sequences=True,\n",
        "                                            return_state = True,\n",
        "                                            kernel_initializer = glorot_uniform(seed = 0),\n",
        "                                            bias_initializer ='zeros',\n",
        "                                            name = \"encoder_output\")(encoder_inputs)\n",
        "    \n",
        "    # We discard `encoder_outputs` and only keep the states.\n",
        "    encoder_states = [state_h, state_c]\n",
        "\n",
        "    # Set up the decoder, using `encoder_states` as initial state.\n",
        "    decoder_inputs = Input(shape=(None, num_decoder_tokens), name = \"decoder_input\")\n",
        "\n",
        "    # We set up our decoder to return full output sequences,\n",
        "    # and to return internal states as well. We don't use the\n",
        "    # return states in the training model, but we will use them in inference.1\n",
        "    decoder_outputs,_,_ = LSTM(256,return_sequences=True,return_state=True,\n",
        "                               kernel_initializer = glorot_uniform(seed = 0),\n",
        "                               bias_initializer ='zeros',\n",
        "                               name = \"decoder_LSTM_1\")(decoder_inputs,initial_state=encoder_states)\n",
        "    decoder_outputs = Dropout(0.8)(decoder_outputs)\n",
        "    \n",
        "    decoder_outputs,_,_ = LSTM(256,return_sequences=True,return_state=True,\n",
        "                               kernel_initializer = glorot_uniform(seed = 0),\n",
        "                               bias_initializer ='zeros',\n",
        "                               name = \"decoder_LSTM_2\")(decoder_outputs)\n",
        "    decoder_outputs = Dropout(0.8)(decoder_outputs)\n",
        "    \n",
        "    decoder_outputs = Dense(num_decoder_tokens, activation='softmax',\n",
        "    #                        kernel_initializer= glorot_uniform(seed = 0),\n",
        "    #                        bias_initializer='zeros',\n",
        "                            name = \"decoder_Dense_Output\")(decoder_outputs)\n",
        "\n",
        "    \n",
        "\n",
        "\n",
        "    # Define the model that will turn\n",
        "    # `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
        "    model = Model(inputs = [encoder_inputs, decoder_inputs], outputs = decoder_outputs)\n",
        "    \n",
        "    \n",
        "    # Encoder Model for Inference\n",
        "    encoder_model = Model(encoder_inputs, encoder_states)\n",
        "\n",
        "    \n",
        "    # Decoder Model for Inference\n",
        "    decoder_state_input_h = Input(shape=(latent_dim,), name = \"Inference_decoder_input_hidden_state\")\n",
        "    decoder_state_input_c = Input(shape=(latent_dim,), name = \"Inference_decoder_input_cell_state\")\n",
        "    decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "    \n",
        "    decoder_outputs_1, state_h, state_c = LSTM(latent_dim,\n",
        "                                               return_sequences=True, \n",
        "                                               return_state=True,\n",
        "                                               name = \"Inference_decoder_LSTM\")(decoder_inputs, initial_state=decoder_states_inputs)\n",
        "    \n",
        "    decoder_states = [state_h, state_c]\n",
        "    \n",
        "    decoder_outputs_1 = Dense(num_decoder_tokens, \n",
        "                              activation='softmax',\n",
        "                              kernel_initializer= glorot_uniform(seed = 0),\n",
        "                              bias_initializer='zeros',\n",
        "                              name = \"decoder_Dense_Output\")(decoder_outputs_1)\n",
        "    decoder_model = Model([decoder_inputs] + decoder_states_inputs,\n",
        "                          [decoder_outputs_1] + decoder_states)\n",
        "\n",
        "    return model, encoder_model, decoder_model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2wqj6UzBuu2F",
        "colab_type": "code",
        "colab": {},
        "outputId": "1d013319-bc85-4d01-89de-5edcd03e78b8"
      },
      "cell_type": "code",
      "source": [
        "model, encoder_model, decoder_model = Character_Model(num_encoder_tokens, num_decoder_tokens, latent_dim)\n",
        "\n",
        "rmsprop = optimizers.RMSprop(lr=0.002)\n",
        "\n",
        "# Compile Model\n",
        "model.compile(optimizer=rmsprop, loss='categorical_crossentropy', metrics = ['accuracy'])\n",
        "# Print Model Summary\n",
        "print(model.summary())\n",
        "plot_model(model, to_file='Models/Character_Level_Model.png')\n",
        "SVG(model_to_dot(model).create(prog='dot', format='svg'))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "encoder_input (InputLayer)      (None, None, 41)     0                                            \n",
            "__________________________________________________________________________________________________\n",
            "decoder_input (InputLayer)      (None, None, 31)     0                                            \n",
            "__________________________________________________________________________________________________\n",
            "encoder_output (LSTM)           [(None, None, 256),  305152      encoder_input[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "decoder_LSTM_1 (LSTM)           [(None, None, 256),  294912      decoder_input[0][0]              \n",
            "                                                                 encoder_output[0][1]             \n",
            "                                                                 encoder_output[0][2]             \n",
            "__________________________________________________________________________________________________\n",
            "dropout_29 (Dropout)            (None, None, 256)    0           decoder_LSTM_1[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "decoder_LSTM_2 (LSTM)           [(None, None, 256),  525312      dropout_29[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_30 (Dropout)            (None, None, 256)    0           decoder_LSTM_2[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "decoder_Dense_Output (Dense)    (None, None, 31)     7967        dropout_30[0][0]                 \n",
            "==================================================================================================\n",
            "Total params: 1,133,343\n",
            "Trainable params: 1,133,343\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "image/svg+xml": "<svg height=\"483pt\" viewBox=\"0.00 0.00 347.50 483.00\" width=\"348pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g class=\"graph\" id=\"graph0\" transform=\"scale(1 1) rotate(0) translate(4 479)\">\n<title>G</title>\n<polygon fill=\"white\" points=\"-4,4 -4,-479 343.5,-479 343.5,4 -4,4\" stroke=\"none\"/>\n<!-- 1711854496904 -->\n<g class=\"node\" id=\"node1\"><title>1711854496904</title>\n<polygon fill=\"none\" points=\"0,-438.5 0,-474.5 164,-474.5 164,-438.5 0,-438.5\" stroke=\"black\"/>\n<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"82\" y=\"-452.8\">encoder_input: InputLayer</text>\n</g>\n<!-- 1707788501288 -->\n<g class=\"node\" id=\"node3\"><title>1707788501288</title>\n<polygon fill=\"none\" points=\"7.5,-365.5 7.5,-401.5 156.5,-401.5 156.5,-365.5 7.5,-365.5\" stroke=\"black\"/>\n<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"82\" y=\"-379.8\">encoder_output: LSTM</text>\n</g>\n<!-- 1711854496904&#45;&gt;1707788501288 -->\n<g class=\"edge\" id=\"edge1\"><title>1711854496904-&gt;1707788501288</title>\n<path d=\"M82,-438.313C82,-430.289 82,-420.547 82,-411.569\" fill=\"none\" stroke=\"black\"/>\n<polygon fill=\"black\" points=\"85.5001,-411.529 82,-401.529 78.5001,-411.529 85.5001,-411.529\" stroke=\"black\"/>\n</g>\n<!-- 1710419945624 -->\n<g class=\"node\" id=\"node2\"><title>1710419945624</title>\n<polygon fill=\"none\" points=\"174.5,-365.5 174.5,-401.5 339.5,-401.5 339.5,-365.5 174.5,-365.5\" stroke=\"black\"/>\n<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"257\" y=\"-379.8\">decoder_input: InputLayer</text>\n</g>\n<!-- 1711853847776 -->\n<g class=\"node\" id=\"node4\"><title>1711853847776</title>\n<polygon fill=\"none\" points=\"86,-292.5 86,-328.5 252,-328.5 252,-292.5 86,-292.5\" stroke=\"black\"/>\n<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"169\" y=\"-306.8\">decoder_LSTM_1: LSTM</text>\n</g>\n<!-- 1710419945624&#45;&gt;1711853847776 -->\n<g class=\"edge\" id=\"edge2\"><title>1710419945624-&gt;1711853847776</title>\n<path d=\"M235.698,-365.313C224.348,-356.156 210.224,-344.76 197.899,-334.816\" fill=\"none\" stroke=\"black\"/>\n<polygon fill=\"black\" points=\"200.087,-332.084 190.106,-328.529 195.691,-337.532 200.087,-332.084\" stroke=\"black\"/>\n</g>\n<!-- 1707788501288&#45;&gt;1711853847776 -->\n<g class=\"edge\" id=\"edge3\"><title>1707788501288-&gt;1711853847776</title>\n<path d=\"M103.06,-365.313C114.174,-356.243 127.979,-344.977 140.081,-335.1\" fill=\"none\" stroke=\"black\"/>\n<polygon fill=\"black\" points=\"142.599,-337.563 148.134,-328.529 138.173,-332.14 142.599,-337.563\" stroke=\"black\"/>\n</g>\n<!-- 1711854082032 -->\n<g class=\"node\" id=\"node5\"><title>1711854082032</title>\n<polygon fill=\"none\" points=\"100.5,-219.5 100.5,-255.5 237.5,-255.5 237.5,-219.5 100.5,-219.5\" stroke=\"black\"/>\n<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"169\" y=\"-233.8\">dropout_29: Dropout</text>\n</g>\n<!-- 1711853847776&#45;&gt;1711854082032 -->\n<g class=\"edge\" id=\"edge5\"><title>1711853847776-&gt;1711854082032</title>\n<path d=\"M169,-292.313C169,-284.289 169,-274.547 169,-265.569\" fill=\"none\" stroke=\"black\"/>\n<polygon fill=\"black\" points=\"172.5,-265.529 169,-255.529 165.5,-265.529 172.5,-265.529\" stroke=\"black\"/>\n</g>\n<!-- 1711854308656 -->\n<g class=\"node\" id=\"node6\"><title>1711854308656</title>\n<polygon fill=\"none\" points=\"86,-146.5 86,-182.5 252,-182.5 252,-146.5 86,-146.5\" stroke=\"black\"/>\n<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"169\" y=\"-160.8\">decoder_LSTM_2: LSTM</text>\n</g>\n<!-- 1711854082032&#45;&gt;1711854308656 -->\n<g class=\"edge\" id=\"edge6\"><title>1711854082032-&gt;1711854308656</title>\n<path d=\"M169,-219.313C169,-211.289 169,-201.547 169,-192.569\" fill=\"none\" stroke=\"black\"/>\n<polygon fill=\"black\" points=\"172.5,-192.529 169,-182.529 165.5,-192.529 172.5,-192.529\" stroke=\"black\"/>\n</g>\n<!-- 1707791781672 -->\n<g class=\"node\" id=\"node7\"><title>1707791781672</title>\n<polygon fill=\"none\" points=\"100.5,-73.5 100.5,-109.5 237.5,-109.5 237.5,-73.5 100.5,-73.5\" stroke=\"black\"/>\n<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"169\" y=\"-87.8\">dropout_30: Dropout</text>\n</g>\n<!-- 1711854308656&#45;&gt;1707791781672 -->\n<g class=\"edge\" id=\"edge7\"><title>1711854308656-&gt;1707791781672</title>\n<path d=\"M169,-146.313C169,-138.289 169,-128.547 169,-119.569\" fill=\"none\" stroke=\"black\"/>\n<polygon fill=\"black\" points=\"172.5,-119.529 169,-109.529 165.5,-119.529 172.5,-119.529\" stroke=\"black\"/>\n</g>\n<!-- 1707791779096 -->\n<g class=\"node\" id=\"node8\"><title>1707791779096</title>\n<polygon fill=\"none\" points=\"73,-0.5 73,-36.5 265,-36.5 265,-0.5 73,-0.5\" stroke=\"black\"/>\n<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"169\" y=\"-14.8\">decoder_Dense_Output: Dense</text>\n</g>\n<!-- 1707791781672&#45;&gt;1707791779096 -->\n<g class=\"edge\" id=\"edge8\"><title>1707791781672-&gt;1707791779096</title>\n<path d=\"M169,-73.3129C169,-65.2895 169,-55.5475 169,-46.5691\" fill=\"none\" stroke=\"black\"/>\n<polygon fill=\"black\" points=\"172.5,-46.5288 169,-36.5288 165.5,-46.5289 172.5,-46.5288\" stroke=\"black\"/>\n</g>\n</g>\n</svg>",
            "text/plain": [
              "<IPython.core.display.SVG object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 224
        }
      ]
    },
    {
      "metadata": {
        "scrolled": true,
        "id": "NRovJtYYuu2K",
        "colab_type": "code",
        "colab": {},
        "outputId": "be4b7d32-b0ae-458d-d995-6bcbedb85c55"
      },
      "cell_type": "code",
      "source": [
        "print()\n",
        "print(\"Encoder Model\")\n",
        "plot_model(encoder_model, to_file='Models/Inference_Encoder_Character_Level_Model.png')\n",
        "SVG(model_to_dot(encoder_model).create(prog='dot', format='svg'))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Encoder Model\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "image/svg+xml": "<svg height=\"118pt\" viewBox=\"0.00 0.00 172.00 118.00\" width=\"172pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g class=\"graph\" id=\"graph0\" transform=\"scale(1 1) rotate(0) translate(4 114)\">\n<title>G</title>\n<polygon fill=\"white\" points=\"-4,4 -4,-114 168,-114 168,4 -4,4\" stroke=\"none\"/>\n<!-- 1707971282200 -->\n<g class=\"node\" id=\"node1\"><title>1707971282200</title>\n<polygon fill=\"none\" points=\"0,-73.5 0,-109.5 164,-109.5 164,-73.5 0,-73.5\" stroke=\"black\"/>\n<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"82\" y=\"-87.8\">encoder_input: InputLayer</text>\n</g>\n<!-- 1707971257288 -->\n<g class=\"node\" id=\"node2\"><title>1707971257288</title>\n<polygon fill=\"none\" points=\"7.5,-0.5 7.5,-36.5 156.5,-36.5 156.5,-0.5 7.5,-0.5\" stroke=\"black\"/>\n<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"82\" y=\"-14.8\">encoder_output: LSTM</text>\n</g>\n<!-- 1707971282200&#45;&gt;1707971257288 -->\n<g class=\"edge\" id=\"edge1\"><title>1707971282200-&gt;1707971257288</title>\n<path d=\"M82,-73.3129C82,-65.2895 82,-55.5475 82,-46.5691\" fill=\"none\" stroke=\"black\"/>\n<polygon fill=\"black\" points=\"85.5001,-46.5288 82,-36.5288 78.5001,-46.5289 85.5001,-46.5288\" stroke=\"black\"/>\n</g>\n</g>\n</svg>",
            "text/plain": [
              "<IPython.core.display.SVG object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 213
        }
      ]
    },
    {
      "metadata": {
        "id": "mMBZuLLluu2N",
        "colab_type": "code",
        "colab": {},
        "outputId": "70b8068b-a15e-452f-e362-9d4c76bae0a6"
      },
      "cell_type": "code",
      "source": [
        "print()\n",
        "print(\"Decoder Model\")\n",
        "plot_model(decoder_model, to_file='Models/Inference_Decoder_Character_Level_Model.png')\n",
        "SVG(model_to_dot(decoder_model).create(prog='dot', format='svg'))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Decoder Model\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "image/svg+xml": "<svg height=\"191pt\" viewBox=\"0.00 0.00 795.50 191.00\" width=\"796pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g class=\"graph\" id=\"graph0\" transform=\"scale(1 1) rotate(0) translate(4 187)\">\n<title>G</title>\n<polygon fill=\"white\" points=\"-4,4 -4,-187 791.5,-187 791.5,4 -4,4\" stroke=\"none\"/>\n<!-- 1707971257568 -->\n<g class=\"node\" id=\"node1\"><title>1707971257568</title>\n<polygon fill=\"none\" points=\"0,-146.5 0,-182.5 165,-182.5 165,-146.5 0,-146.5\" stroke=\"black\"/>\n<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"82.5\" y=\"-160.8\">decoder_input: InputLayer</text>\n</g>\n<!-- 1707938444736 -->\n<g class=\"node\" id=\"node4\"><title>1707938444736</title>\n<polygon fill=\"none\" points=\"229,-73.5 229,-109.5 440,-109.5 440,-73.5 229,-73.5\" stroke=\"black\"/>\n<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"334.5\" y=\"-87.8\">Inference_decoder_LSTM: LSTM</text>\n</g>\n<!-- 1707971257568&#45;&gt;1707938444736 -->\n<g class=\"edge\" id=\"edge1\"><title>1707971257568-&gt;1707938444736</title>\n<path d=\"M142.86,-146.494C179.217,-136.25 225.69,-123.157 263.766,-112.429\" fill=\"none\" stroke=\"black\"/>\n<polygon fill=\"black\" points=\"265.083,-115.694 273.759,-109.614 263.184,-108.957 265.083,-115.694\" stroke=\"black\"/>\n</g>\n<!-- 1712455688320 -->\n<g class=\"node\" id=\"node2\"><title>1712455688320</title>\n<polygon fill=\"none\" points=\"183.5,-146.5 183.5,-182.5 485.5,-182.5 485.5,-146.5 183.5,-146.5\" stroke=\"black\"/>\n<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"334.5\" y=\"-160.8\">Inference_decoder_input_hidden_state: InputLayer</text>\n</g>\n<!-- 1712455688320&#45;&gt;1707938444736 -->\n<g class=\"edge\" id=\"edge2\"><title>1712455688320-&gt;1707938444736</title>\n<path d=\"M334.5,-146.313C334.5,-138.289 334.5,-128.547 334.5,-119.569\" fill=\"none\" stroke=\"black\"/>\n<polygon fill=\"black\" points=\"338,-119.529 334.5,-109.529 331,-119.529 338,-119.529\" stroke=\"black\"/>\n</g>\n<!-- 1707938446976 -->\n<g class=\"node\" id=\"node3\"><title>1707938446976</title>\n<polygon fill=\"none\" points=\"503.5,-146.5 503.5,-182.5 787.5,-182.5 787.5,-146.5 503.5,-146.5\" stroke=\"black\"/>\n<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"645.5\" y=\"-160.8\">Inference_decoder_input_cell_state: InputLayer</text>\n</g>\n<!-- 1707938446976&#45;&gt;1707938444736 -->\n<g class=\"edge\" id=\"edge3\"><title>1707938446976-&gt;1707938444736</title>\n<path d=\"M571.008,-146.494C525.291,-136.057 466.614,-122.661 419.143,-111.824\" fill=\"none\" stroke=\"black\"/>\n<polygon fill=\"black\" points=\"419.721,-108.366 409.192,-109.552 418.163,-115.19 419.721,-108.366\" stroke=\"black\"/>\n</g>\n<!-- 1707938444064 -->\n<g class=\"node\" id=\"node5\"><title>1707938444064</title>\n<polygon fill=\"none\" points=\"238.5,-0.5 238.5,-36.5 430.5,-36.5 430.5,-0.5 238.5,-0.5\" stroke=\"black\"/>\n<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"334.5\" y=\"-14.8\">decoder_Dense_Output: Dense</text>\n</g>\n<!-- 1707938444736&#45;&gt;1707938444064 -->\n<g class=\"edge\" id=\"edge4\"><title>1707938444736-&gt;1707938444064</title>\n<path d=\"M334.5,-73.3129C334.5,-65.2895 334.5,-55.5475 334.5,-46.5691\" fill=\"none\" stroke=\"black\"/>\n<polygon fill=\"black\" points=\"338,-46.5288 334.5,-36.5288 331,-46.5289 338,-46.5288\" stroke=\"black\"/>\n</g>\n</g>\n</svg>",
            "text/plain": [
              "<IPython.core.display.SVG object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 214
        }
      ]
    },
    {
      "metadata": {
        "id": "ZdYKq7RDuu2Q",
        "colab_type": "code",
        "colab": {},
        "outputId": "42ed7d9a-ed84-4f00-d63b-67db69139856"
      },
      "cell_type": "code",
      "source": [
        "history = model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
        "                    batch_size=16,\n",
        "                    epochs=10,\n",
        "                    validation_split=0.01)\n",
        "\n",
        "# Save model\n",
        "model.save('s2s.h5')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 49 samples, validate on 1 samples\n",
            "Epoch 1/10\n",
            "49/49 [==============================] - ETA: 13s - loss: 1.3563 - acc: 0.04 - ETA: 5s - loss: 1.3344 - acc: 0.0393 - ETA: 0s - loss: 1.3245 - acc: 0.035 - 17s 357ms/step - loss: 1.3096 - acc: 0.0346 - val_loss: 0.5845 - val_acc: 0.0161\n",
            "Epoch 2/10\n",
            "49/49 [==============================] - ETA: 6s - loss: 1.0620 - acc: 0.025 - ETA: 3s - loss: 1.1545 - acc: 0.035 - ETA: 0s - loss: 1.1839 - acc: 0.040 - 15s 303ms/step - loss: 1.1716 - acc: 0.0392 - val_loss: 0.5399 - val_acc: 0.0161\n",
            "Epoch 3/10\n",
            "49/49 [==============================] - ETA: 6s - loss: 1.2283 - acc: 0.037 - ETA: 3s - loss: 1.2739 - acc: 0.042 - ETA: 0s - loss: 1.1645 - acc: 0.042 - 15s 306ms/step - loss: 1.1715 - acc: 0.0428 - val_loss: 0.5360 - val_acc: 0.0161\n",
            "Epoch 4/10\n",
            "49/49 [==============================] - ETA: 6s - loss: 1.3453 - acc: 0.055 - ETA: 3s - loss: 1.1443 - acc: 0.043 - ETA: 0s - loss: 1.1511 - acc: 0.040 - 14s 287ms/step - loss: 1.1403 - acc: 0.0395 - val_loss: 0.5387 - val_acc: 0.0161\n",
            "Epoch 5/10\n",
            "49/49 [==============================] - ETA: 5s - loss: 1.3772 - acc: 0.061 - ETA: 3s - loss: 1.1202 - acc: 0.044 - ETA: 0s - loss: 1.1342 - acc: 0.043 - 14s 285ms/step - loss: 1.1356 - acc: 0.0434 - val_loss: 0.5436 - val_acc: 0.0161\n",
            "Epoch 6/10\n",
            "49/49 [==============================] - ETA: 6s - loss: 1.2838 - acc: 0.054 - ETA: 3s - loss: 1.1818 - acc: 0.052 - ETA: 0s - loss: 1.1336 - acc: 0.047 - 14s 289ms/step - loss: 1.1214 - acc: 0.0464 - val_loss: 0.5412 - val_acc: 0.0161\n",
            "Epoch 7/10\n",
            "49/49 [==============================] - ETA: 6s - loss: 0.9798 - acc: 0.035 - ETA: 3s - loss: 1.0573 - acc: 0.044 - ETA: 0s - loss: 1.1235 - acc: 0.046 - 15s 296ms/step - loss: 1.1154 - acc: 0.0458 - val_loss: 0.5350 - val_acc: 0.0161\n",
            "Epoch 8/10\n",
            "49/49 [==============================] - ETA: 5s - loss: 1.3515 - acc: 0.056 - ETA: 3s - loss: 1.1634 - acc: 0.049 - ETA: 0s - loss: 1.1057 - acc: 0.046 - 14s 280ms/step - loss: 1.1150 - acc: 0.0471 - val_loss: 0.5449 - val_acc: 0.0161\n",
            "Epoch 9/10\n",
            "49/49 [==============================] - ETA: 5s - loss: 0.9822 - acc: 0.043 - ETA: 3s - loss: 1.0266 - acc: 0.044 - ETA: 0s - loss: 1.1038 - acc: 0.048 - 14s 285ms/step - loss: 1.1135 - acc: 0.0490 - val_loss: 0.5337 - val_acc: 0.0161\n",
            "Epoch 10/10\n",
            "49/49 [==============================] - ETA: 6s - loss: 1.1538 - acc: 0.053 - ETA: 3s - loss: 1.0867 - acc: 0.051 - ETA: 0s - loss: 1.0954 - acc: 0.053 - 14s 295ms/step - loss: 1.1030 - acc: 0.0540 - val_loss: 0.5593 - val_acc: 0.0161\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "c:\\python36\\lib\\site-packages\\keras\\engine\\network.py:888: UserWarning: Layer decoder_LSTM_1 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'encoder_output_32/while/Exit_2:0' shape=(?, 256) dtype=float32>, <tf.Tensor 'encoder_output_32/while/Exit_3:0' shape=(?, 256) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
            "  '. They will not be included '\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "FSMjlRNouu2T",
        "colab_type": "code",
        "colab": {},
        "outputId": "666468b5-e73f-4812-de2e-588a677d5c0f"
      },
      "cell_type": "code",
      "source": [
        "while True:\n",
        "    try:\n",
        "        run_again = int(input(\"Do you want to run the Model again for 1 epoch more? (1 - Yes | 0 - No) : \"))\n",
        "        if run_again == 1:\n",
        "            model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
        "                    batch_size=16,\n",
        "                    epochs=100,\n",
        "                    validation_split=0.2)\n",
        "        else:\n",
        "            break\n",
        "    except Exception as e:\n",
        "        break"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Do you want to run the Model again for 1 epoch more? (1 - Yes | 0 - No) : 1\n",
            "Train on 40 samples, validate on 10 samples\n",
            "Epoch 1/100\n",
            "40/40 [==============================] - ETA: 4s - loss: 1.2780 - acc: 0.060 - ETA: 1s - loss: 1.1554 - acc: 0.052 - 22s 543ms/step - loss: 1.0949 - acc: 0.0500 - val_loss: 1.1057 - val_acc: 0.0565\n",
            "Epoch 2/100\n",
            "40/40 [==============================] - ETA: 4s - loss: 1.1453 - acc: 0.052 - ETA: 1s - loss: 1.1272 - acc: 0.052 - 21s 526ms/step - loss: 1.0781 - acc: 0.0484 - val_loss: 1.1000 - val_acc: 0.0597\n",
            "Epoch 3/100\n",
            "40/40 [==============================] - ETA: 4s - loss: 1.0062 - acc: 0.048 - ETA: 1s - loss: 1.0850 - acc: 0.050 - 21s 514ms/step - loss: 1.0817 - acc: 0.0516 - val_loss: 1.1030 - val_acc: 0.0597\n",
            "Epoch 4/100\n",
            "40/40 [==============================] - ETA: 4s - loss: 1.1486 - acc: 0.056 - ETA: 1s - loss: 1.1219 - acc: 0.052 - 21s 524ms/step - loss: 1.0769 - acc: 0.0512 - val_loss: 1.0993 - val_acc: 0.0613\n",
            "Epoch 5/100\n",
            "40/40 [==============================] - ETA: 4s - loss: 1.0720 - acc: 0.059 - ETA: 1s - loss: 1.1125 - acc: 0.057 - 23s 577ms/step - loss: 1.0713 - acc: 0.0532 - val_loss: 1.0972 - val_acc: 0.0613\n",
            "Epoch 6/100\n",
            "40/40 [==============================] - ETA: 5s - loss: 1.0198 - acc: 0.053 - ETA: 1s - loss: 1.0940 - acc: 0.048 - 23s 579ms/step - loss: 1.0752 - acc: 0.0468 - val_loss: 1.1133 - val_acc: 0.0597\n",
            "Epoch 7/100\n",
            "40/40 [==============================] - ETA: 4s - loss: 0.9202 - acc: 0.048 - ETA: 1s - loss: 1.0464 - acc: 0.053 - 21s 524ms/step - loss: 1.0594 - acc: 0.0560 - val_loss: 1.0994 - val_acc: 0.0613\n",
            "Epoch 8/100\n",
            "40/40 [==============================] - ETA: 4s - loss: 1.0475 - acc: 0.050 - ETA: 1s - loss: 1.0749 - acc: 0.057 - 21s 520ms/step - loss: 1.0618 - acc: 0.0560 - val_loss: 1.1104 - val_acc: 0.0645\n",
            "Epoch 9/100\n",
            "40/40 [==============================] - ETA: 4s - loss: 1.0784 - acc: 0.047 - ETA: 1s - loss: 1.0832 - acc: 0.051 - 21s 521ms/step - loss: 1.0670 - acc: 0.0540 - val_loss: 1.0994 - val_acc: 0.0597\n",
            "Epoch 10/100\n",
            "40/40 [==============================] - ETA: 4s - loss: 0.8304 - acc: 0.037 - ETA: 1s - loss: 1.0200 - acc: 0.053 - 21s 521ms/step - loss: 1.0542 - acc: 0.0565 - val_loss: 1.0956 - val_acc: 0.0613\n",
            "Epoch 11/100\n",
            "40/40 [==============================] - ETA: 5s - loss: 1.1257 - acc: 0.063 - ETA: 1s - loss: 1.1140 - acc: 0.061 - 21s 526ms/step - loss: 1.0625 - acc: 0.0552 - val_loss: 1.1066 - val_acc: 0.0581\n",
            "Epoch 12/100\n",
            "40/40 [==============================] - ETA: 4s - loss: 1.0439 - acc: 0.057 - ETA: 1s - loss: 1.0351 - acc: 0.057 - 21s 519ms/step - loss: 1.0588 - acc: 0.0569 - val_loss: 1.0924 - val_acc: 0.0613\n",
            "Epoch 13/100\n",
            "40/40 [==============================] - ETA: 4s - loss: 0.8995 - acc: 0.038 - ETA: 1s - loss: 1.0216 - acc: 0.049 - 21s 520ms/step - loss: 1.0510 - acc: 0.0536 - val_loss: 1.1146 - val_acc: 0.0565\n",
            "Epoch 14/100\n",
            "40/40 [==============================] - ETA: 7s - loss: 1.2273 - acc: 0.071 - ETA: 2s - loss: 1.0604 - acc: 0.058 - 25s 636ms/step - loss: 1.0544 - acc: 0.0581 - val_loss: 1.0871 - val_acc: 0.0645\n",
            "Epoch 15/100\n",
            "40/40 [==============================] - ETA: 4s - loss: 1.0836 - acc: 0.075 - ETA: 1s - loss: 1.0950 - acc: 0.065 - 22s 562ms/step - loss: 1.0393 - acc: 0.0706 - val_loss: 1.1367 - val_acc: 0.1565\n",
            "Epoch 16/100\n",
            "40/40 [==============================] - ETA: 5s - loss: 0.9787 - acc: 0.242 - ETA: 1s - loss: 1.0298 - acc: 0.149 - 27s 675ms/step - loss: 1.0577 - acc: 0.1315 - val_loss: 1.0879 - val_acc: 0.0629\n",
            "Epoch 17/100\n",
            "40/40 [==============================] - ETA: 6s - loss: 1.0998 - acc: 0.062 - ETA: 1s - loss: 1.0620 - acc: 0.058 - 24s 602ms/step - loss: 1.0520 - acc: 0.0581 - val_loss: 1.0901 - val_acc: 0.0629\n",
            "Epoch 18/100\n",
            "40/40 [==============================] - ETA: 5s - loss: 1.1525 - acc: 0.071 - ETA: 1s - loss: 1.0525 - acc: 0.061 - 26s 645ms/step - loss: 1.0354 - acc: 0.0629 - val_loss: 1.0911 - val_acc: 0.0645\n",
            "Epoch 19/100\n",
            "40/40 [==============================] - ETA: 5s - loss: 1.0061 - acc: 0.062 - ETA: 1s - loss: 1.0737 - acc: 0.064 - 25s 629ms/step - loss: 1.0302 - acc: 0.0597 - val_loss: 1.1175 - val_acc: 0.0629\n",
            "Epoch 20/100\n",
            "40/40 [==============================] - ETA: 5s - loss: 1.1880 - acc: 0.068 - ETA: 1s - loss: 1.0475 - acc: 0.054 - 24s 590ms/step - loss: 1.0459 - acc: 0.0556 - val_loss: 1.0891 - val_acc: 0.0645\n",
            "Epoch 21/100\n",
            "40/40 [==============================] - ETA: 5s - loss: 1.0130 - acc: 0.052 - ETA: 1s - loss: 1.0451 - acc: 0.063 - 23s 563ms/step - loss: 1.0281 - acc: 0.0617 - val_loss: 1.0848 - val_acc: 0.0661\n",
            "Epoch 22/100\n",
            "40/40 [==============================] - ETA: 4s - loss: 1.0104 - acc: 0.056 - ETA: 1s - loss: 0.9509 - acc: 0.054 - 22s 542ms/step - loss: 1.0242 - acc: 0.0605 - val_loss: 1.1161 - val_acc: 0.0532\n",
            "Epoch 23/100\n",
            "40/40 [==============================] - ETA: 5s - loss: 0.8425 - acc: 0.051 - ETA: 1s - loss: 0.9861 - acc: 0.055 - 24s 610ms/step - loss: 1.0143 - acc: 0.0597 - val_loss: 1.0842 - val_acc: 0.0613\n",
            "Epoch 24/100\n",
            "40/40 [==============================] - ETA: 5s - loss: 1.1893 - acc: 0.076 - ETA: 1s - loss: 1.0344 - acc: 0.068 - 24s 603ms/step - loss: 1.0165 - acc: 0.0698 - val_loss: 1.1141 - val_acc: 0.0613\n",
            "Epoch 25/100\n",
            "40/40 [==============================] - ETA: 5s - loss: 1.0179 - acc: 0.057 - ETA: 1s - loss: 1.0172 - acc: 0.065 - 26s 644ms/step - loss: 1.0091 - acc: 0.0637 - val_loss: 1.1570 - val_acc: 0.0532\n",
            "Epoch 26/100\n",
            "40/40 [==============================] - ETA: 5s - loss: 0.8783 - acc: 0.058 - ETA: 1s - loss: 0.9665 - acc: 0.064 - 27s 663ms/step - loss: 1.0111 - acc: 0.0694 - val_loss: 1.0951 - val_acc: 0.0597\n",
            "Epoch 27/100\n",
            "40/40 [==============================] - ETA: 6s - loss: 1.0247 - acc: 0.064 - ETA: 2s - loss: 1.0524 - acc: 0.073 - 27s 663ms/step - loss: 0.9958 - acc: 0.0669 - val_loss: 1.1209 - val_acc: 0.0548\n",
            "Epoch 28/100\n",
            "40/40 [==============================] - ETA: 4s - loss: 0.9750 - acc: 0.069 - ETA: 1s - loss: 0.9760 - acc: 0.067 - 24s 612ms/step - loss: 0.9901 - acc: 0.0702 - val_loss: 1.0957 - val_acc: 0.0565\n",
            "Epoch 29/100\n",
            "40/40 [==============================] - ETA: 5s - loss: 1.1899 - acc: 0.082 - ETA: 1s - loss: 1.0435 - acc: 0.068 - 22s 543ms/step - loss: 1.0076 - acc: 0.0637 - val_loss: 1.0818 - val_acc: 0.0645\n",
            "Epoch 30/100\n",
            "40/40 [==============================] - ETA: 4s - loss: 1.0928 - acc: 0.080 - ETA: 1s - loss: 1.0308 - acc: 0.076 - 22s 558ms/step - loss: 0.9873 - acc: 0.0730 - val_loss: 1.0892 - val_acc: 0.0565\n",
            "Epoch 31/100\n",
            "40/40 [==============================] - ETA: 5s - loss: 1.0015 - acc: 0.084 - ETA: 1s - loss: 0.9313 - acc: 0.068 - 23s 579ms/step - loss: 0.9717 - acc: 0.0734 - val_loss: 1.3976 - val_acc: 0.0484\n",
            "Epoch 32/100\n",
            "40/40 [==============================] - ETA: 4s - loss: 0.9758 - acc: 0.058 - ETA: 1s - loss: 1.0165 - acc: 0.068 - 23s 583ms/step - loss: 1.0275 - acc: 0.0661 - val_loss: 1.0649 - val_acc: 0.0677\n",
            "Epoch 33/100\n",
            "40/40 [==============================] - ETA: 5s - loss: 1.0474 - acc: 0.077 - ETA: 1s - loss: 0.9715 - acc: 0.071 - 26s 644ms/step - loss: 0.9804 - acc: 0.0762 - val_loss: 1.0723 - val_acc: 0.0710\n",
            "Epoch 34/100\n",
            "40/40 [==============================] - ETA: 5s - loss: 0.9508 - acc: 0.069 - ETA: 1s - loss: 0.9060 - acc: 0.066 - 25s 613ms/step - loss: 0.9696 - acc: 0.0726 - val_loss: 1.0707 - val_acc: 0.0629\n",
            "Epoch 35/100\n",
            "40/40 [==============================] - ETA: 5s - loss: 0.9547 - acc: 0.081 - ETA: 1s - loss: 0.9908 - acc: 0.082 - 25s 627ms/step - loss: 0.9592 - acc: 0.0778 - val_loss: 1.0724 - val_acc: 0.0661\n",
            "Epoch 36/100\n",
            "40/40 [==============================] - ETA: 5s - loss: 0.9317 - acc: 0.068 - ETA: 1s - loss: 0.9956 - acc: 0.073 - 26s 640ms/step - loss: 0.9733 - acc: 0.0706 - val_loss: 1.0671 - val_acc: 0.0677\n",
            "Epoch 37/100\n",
            "40/40 [==============================] - ETA: 5s - loss: 1.0081 - acc: 0.086 - ETA: 1s - loss: 0.9630 - acc: 0.084 - 26s 655ms/step - loss: 0.9458 - acc: 0.0815 - val_loss: 1.0999 - val_acc: 0.0629\n",
            "Epoch 38/100\n",
            "40/40 [==============================] - ETA: 6s - loss: 0.9046 - acc: 0.069 - ETA: 2s - loss: 0.9510 - acc: 0.074 - 29s 715ms/step - loss: 0.9512 - acc: 0.0726 - val_loss: 1.0789 - val_acc: 0.0613\n",
            "Epoch 39/100\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "40/40 [==============================] - ETA: 5s - loss: 0.9976 - acc: 0.101 - ETA: 1s - loss: 0.9589 - acc: 0.086 - 24s 597ms/step - loss: 0.9329 - acc: 0.0831 - val_loss: 1.0877 - val_acc: 0.0694\n",
            "Epoch 40/100\n",
            "40/40 [==============================] - ETA: 5s - loss: 0.8890 - acc: 0.080 - ETA: 1s - loss: 0.9412 - acc: 0.081 - 24s 593ms/step - loss: 0.9343 - acc: 0.0770 - val_loss: 1.0806 - val_acc: 0.0597\n",
            "Epoch 41/100\n",
            "40/40 [==============================] - ETA: 5s - loss: 0.8885 - acc: 0.069 - ETA: 1s - loss: 0.9356 - acc: 0.077 - 28s 706ms/step - loss: 0.9511 - acc: 0.0794 - val_loss: 1.0674 - val_acc: 0.0694\n",
            "Epoch 42/100\n",
            "40/40 [==============================] - ETA: 6s - loss: 0.9107 - acc: 0.083 - ETA: 2s - loss: 0.9024 - acc: 0.078 - 28s 708ms/step - loss: 0.9271 - acc: 0.0815 - val_loss: 1.0687 - val_acc: 0.0726\n",
            "Epoch 43/100\n",
            "40/40 [==============================] - ETA: 7s - loss: 0.8367 - acc: 0.079 - ETA: 2s - loss: 0.9290 - acc: 0.090 - 30s 754ms/step - loss: 0.9229 - acc: 0.0847 - val_loss: 1.1017 - val_acc: 0.0710\n",
            "Epoch 44/100\n",
            "40/40 [==============================] - ETA: 6s - loss: 1.0757 - acc: 0.102 - ETA: 2s - loss: 0.9823 - acc: 0.091 - 27s 671ms/step - loss: 0.9116 - acc: 0.0883 - val_loss: 1.1110 - val_acc: 0.0758\n",
            "Epoch 45/100\n",
            "40/40 [==============================] - ETA: 5s - loss: 0.8437 - acc: 0.089 - ETA: 1s - loss: 0.9085 - acc: 0.090 - 24s 589ms/step - loss: 0.9072 - acc: 0.0855 - val_loss: 1.0807 - val_acc: 0.0710\n",
            "Epoch 46/100\n",
            "40/40 [==============================] - ETA: 4s - loss: 0.8457 - acc: 0.071 - ETA: 1s - loss: 0.9618 - acc: 0.086 - 22s 559ms/step - loss: 0.9207 - acc: 0.0847 - val_loss: 1.0881 - val_acc: 0.0694\n",
            "Epoch 47/100\n",
            "40/40 [==============================] - ETA: 4s - loss: 1.0994 - acc: 0.101 - ETA: 1s - loss: 0.9555 - acc: 0.085 - 24s 609ms/step - loss: 0.9118 - acc: 0.0871 - val_loss: 1.0677 - val_acc: 0.0855\n",
            "Epoch 48/100\n",
            "40/40 [==============================] - ETA: 5s - loss: 0.9335 - acc: 0.084 - ETA: 1s - loss: 0.9218 - acc: 0.090 - 27s 666ms/step - loss: 0.8897 - acc: 0.0952 - val_loss: 1.1006 - val_acc: 0.0661\n",
            "Epoch 49/100\n",
            "40/40 [==============================] - ETA: 7s - loss: 0.6869 - acc: 0.075 - ETA: 2s - loss: 0.8239 - acc: 0.083 - 28s 698ms/step - loss: 0.8992 - acc: 0.0952 - val_loss: 1.1290 - val_acc: 0.0742\n",
            "Epoch 50/100\n",
            "40/40 [==============================] - ETA: 5s - loss: 0.8636 - acc: 0.074 - ETA: 1s - loss: 0.8650 - acc: 0.087 - 26s 658ms/step - loss: 0.9006 - acc: 0.0895 - val_loss: 1.1087 - val_acc: 0.0758\n",
            "Epoch 51/100\n",
            "40/40 [==============================] - ETA: 5s - loss: 0.8847 - acc: 0.090 - ETA: 1s - loss: 0.8746 - acc: 0.095 - 23s 576ms/step - loss: 0.8787 - acc: 0.0952 - val_loss: 1.1603 - val_acc: 0.0613\n",
            "Epoch 52/100\n",
            "40/40 [==============================] - ETA: 5s - loss: 0.9145 - acc: 0.086 - ETA: 1s - loss: 0.8821 - acc: 0.092 - 25s 622ms/step - loss: 0.8912 - acc: 0.0952 - val_loss: 1.0769 - val_acc: 0.0758\n",
            "Epoch 53/100\n",
            "40/40 [==============================] - ETA: 6s - loss: 0.8383 - acc: 0.094 - ETA: 1s - loss: 0.8880 - acc: 0.101 - 26s 653ms/step - loss: 0.8765 - acc: 0.1000 - val_loss: 1.1680 - val_acc: 0.0823\n",
            "Epoch 54/100\n",
            "40/40 [==============================] - ETA: 5s - loss: 0.8623 - acc: 0.088 - ETA: 1s - loss: 0.8501 - acc: 0.090 - 23s 582ms/step - loss: 0.8808 - acc: 0.0956 - val_loss: 1.1574 - val_acc: 0.0806\n",
            "Epoch 55/100\n",
            "40/40 [==============================] - ETA: 5s - loss: 0.8426 - acc: 0.103 - ETA: 1s - loss: 0.8743 - acc: 0.108 - 27s 680ms/step - loss: 0.8577 - acc: 0.1040 - val_loss: 1.1747 - val_acc: 0.0806\n",
            "Epoch 56/100\n",
            "40/40 [==============================] - ETA: 6s - loss: 0.7789 - acc: 0.098 - ETA: 1s - loss: 0.8376 - acc: 0.104 - 24s 594ms/step - loss: 0.8546 - acc: 0.1048 - val_loss: 1.0676 - val_acc: 0.0823\n",
            "Epoch 57/100\n",
            "40/40 [==============================] - ETA: 5s - loss: 1.0624 - acc: 0.121 - ETA: 1s - loss: 0.9005 - acc: 0.107 - 24s 589ms/step - loss: 0.8603 - acc: 0.1060 - val_loss: 1.0848 - val_acc: 0.0823\n",
            "Epoch 58/100\n",
            "40/40 [==============================] - ETA: 4s - loss: 0.9985 - acc: 0.110 - ETA: 1s - loss: 0.8769 - acc: 0.102 - 24s 612ms/step - loss: 0.8547 - acc: 0.0980 - val_loss: 1.0606 - val_acc: 0.0871\n",
            "Epoch 59/100\n",
            "40/40 [==============================] - ETA: 5s - loss: 0.8453 - acc: 0.095 - ETA: 1s - loss: 0.8543 - acc: 0.098 - 30s 744ms/step - loss: 0.8563 - acc: 0.1016 - val_loss: 1.0989 - val_acc: 0.0871\n",
            "Epoch 60/100\n",
            "40/40 [==============================] - ETA: 6s - loss: 0.8290 - acc: 0.107 - ETA: 2s - loss: 0.8167 - acc: 0.100 - 32s 796ms/step - loss: 0.8537 - acc: 0.0984 - val_loss: 1.0709 - val_acc: 0.0758\n",
            "Epoch 61/100\n",
            "40/40 [==============================] - ETA: 6s - loss: 0.9177 - acc: 0.110 - ETA: 2s - loss: 0.8501 - acc: 0.108 - 31s 787ms/step - loss: 0.8449 - acc: 0.1113 - val_loss: 1.1382 - val_acc: 0.0806\n",
            "Epoch 62/100\n",
            "40/40 [==============================] - ETA: 5s - loss: 0.7608 - acc: 0.093 - ETA: 1s - loss: 0.8211 - acc: 0.106 - 28s 711ms/step - loss: 0.8326 - acc: 0.1085 - val_loss: 1.1227 - val_acc: 0.0758\n",
            "Epoch 63/100\n",
            "40/40 [==============================] - ETA: 6s - loss: 0.8281 - acc: 0.086 - ETA: 2s - loss: 0.8639 - acc: 0.100 - 26s 645ms/step - loss: 0.8398 - acc: 0.1012 - val_loss: 1.0797 - val_acc: 0.0774\n",
            "Epoch 64/100\n",
            "40/40 [==============================] - ETA: 5s - loss: 0.9456 - acc: 0.131 - ETA: 1s - loss: 0.8808 - acc: 0.120 - 25s 621ms/step - loss: 0.8161 - acc: 0.1113 - val_loss: 1.1862 - val_acc: 0.0806\n",
            "Epoch 65/100\n",
            "40/40 [==============================] - ETA: 5s - loss: 0.7658 - acc: 0.107 - ETA: 1s - loss: 0.7776 - acc: 0.099 - 25s 634ms/step - loss: 0.8363 - acc: 0.1089 - val_loss: 1.0567 - val_acc: 0.0887\n",
            "Epoch 66/100\n",
            "40/40 [==============================] - ETA: 5s - loss: 0.7115 - acc: 0.098 - ETA: 1s - loss: 0.7850 - acc: 0.101 - 25s 623ms/step - loss: 0.8310 - acc: 0.1060 - val_loss: 1.1208 - val_acc: 0.0806\n",
            "Epoch 67/100\n",
            "40/40 [==============================] - ETA: 5s - loss: 0.9010 - acc: 0.132 - ETA: 1s - loss: 0.8975 - acc: 0.123 - 28s 688ms/step - loss: 0.8179 - acc: 0.1077 - val_loss: 1.1070 - val_acc: 0.0806\n",
            "Epoch 68/100\n",
            "40/40 [==============================] - ETA: 5s - loss: 0.9054 - acc: 0.119 - ETA: 1s - loss: 0.8515 - acc: 0.115 - 24s 590ms/step - loss: 0.8157 - acc: 0.1137 - val_loss: 1.0977 - val_acc: 0.0790\n",
            "Epoch 69/100\n",
            "40/40 [==============================] - ETA: 6s - loss: 0.8290 - acc: 0.119 - ETA: 2s - loss: 0.7940 - acc: 0.111 - 42s 1s/step - loss: 0.8236 - acc: 0.1073 - val_loss: 1.1513 - val_acc: 0.0823\n",
            "Epoch 70/100\n",
            "40/40 [==============================] - ETA: 8s - loss: 0.9538 - acc: 0.128 - ETA: 2s - loss: 0.8198 - acc: 0.114 - 42s 1s/step - loss: 0.8092 - acc: 0.1085 - val_loss: 1.1831 - val_acc: 0.0790\n",
            "Epoch 71/100\n",
            "40/40 [==============================] - ETA: 8s - loss: 0.8963 - acc: 0.125 - ETA: 2s - loss: 0.8530 - acc: 0.107 - 42s 1s/step - loss: 0.8206 - acc: 0.1065 - val_loss: 1.1266 - val_acc: 0.0790\n",
            "Epoch 72/100\n",
            "40/40 [==============================] - ETA: 8s - loss: 0.6770 - acc: 0.115 - ETA: 2s - loss: 0.7450 - acc: 0.121 - 42s 1s/step - loss: 0.7868 - acc: 0.1234 - val_loss: 1.2232 - val_acc: 0.0871\n",
            "Epoch 73/100\n",
            "40/40 [==============================] - ETA: 8s - loss: 0.6949 - acc: 0.100 - ETA: 2s - loss: 0.8640 - acc: 0.118 - 40s 1s/step - loss: 0.8181 - acc: 0.1105 - val_loss: 1.0936 - val_acc: 0.0774\n",
            "Epoch 74/100\n",
            "40/40 [==============================] - ETA: 8s - loss: 0.7563 - acc: 0.126 - ETA: 2s - loss: 0.7511 - acc: 0.119 - 41s 1s/step - loss: 0.7986 - acc: 0.1230 - val_loss: 1.1242 - val_acc: 0.0839\n",
            "Epoch 75/100\n",
            "40/40 [==============================] - ETA: 8s - loss: 0.7274 - acc: 0.125 - ETA: 2s - loss: 0.7323 - acc: 0.116 - 42s 1s/step - loss: 0.7758 - acc: 0.1226 - val_loss: 1.2076 - val_acc: 0.0823\n",
            "Epoch 76/100\n",
            "40/40 [==============================] - ETA: 9s - loss: 0.7936 - acc: 0.124 - ETA: 3s - loss: 0.7897 - acc: 0.119 - 47s 1s/step - loss: 0.7884 - acc: 0.1165 - val_loss: 1.1289 - val_acc: 0.0871\n",
            "Epoch 77/100\n",
            "40/40 [==============================] - ETA: 8s - loss: 0.9690 - acc: 0.156 - ETA: 2s - loss: 0.7909 - acc: 0.127 - 42s 1s/step - loss: 0.7811 - acc: 0.1198 - val_loss: 1.1061 - val_acc: 0.0823\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 78/100\n",
            "40/40 [==============================] - ETA: 8s - loss: 0.6342 - acc: 0.100 - ETA: 2s - loss: 0.7486 - acc: 0.127 - 39s 964ms/step - loss: 0.7679 - acc: 0.1290 - val_loss: 1.2029 - val_acc: 0.0855\n",
            "Epoch 79/100\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.9333 - acc: 0.132 - ETA: 1s - loss: 0.8372 - acc: 0.119 - 17s 417ms/step - loss: 0.7861 - acc: 0.1157 - val_loss: 1.1570 - val_acc: 0.0839\n",
            "Epoch 80/100\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.6976 - acc: 0.109 - ETA: 1s - loss: 0.7609 - acc: 0.129 - 17s 418ms/step - loss: 0.7674 - acc: 0.1246 - val_loss: 1.2365 - val_acc: 0.0742\n",
            "Epoch 81/100\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.9802 - acc: 0.155 - ETA: 1s - loss: 0.8128 - acc: 0.141 - 16s 392ms/step - loss: 0.7736 - acc: 0.1379 - val_loss: 1.1507 - val_acc: 0.0790\n",
            "Epoch 82/100\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.9531 - acc: 0.133 - ETA: 1s - loss: 0.7869 - acc: 0.107 - 16s 408ms/step - loss: 0.7950 - acc: 0.1161 - val_loss: 1.1875 - val_acc: 0.0806\n",
            "Epoch 83/100\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.8016 - acc: 0.137 - ETA: 1s - loss: 0.7603 - acc: 0.126 - 16s 404ms/step - loss: 0.7651 - acc: 0.1246 - val_loss: 1.2491 - val_acc: 0.0839\n",
            "Epoch 84/100\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.8968 - acc: 0.142 - ETA: 1s - loss: 0.8367 - acc: 0.132 - 16s 400ms/step - loss: 0.7689 - acc: 0.1226 - val_loss: 1.0888 - val_acc: 0.0919\n",
            "Epoch 85/100\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.6898 - acc: 0.110 - ETA: 1s - loss: 0.7783 - acc: 0.127 - 16s 403ms/step - loss: 0.7753 - acc: 0.1278 - val_loss: 1.0824 - val_acc: 0.0790\n",
            "Epoch 86/100\n",
            "40/40 [==============================] - ETA: 2s - loss: 0.8196 - acc: 0.143 - ETA: 1s - loss: 0.7814 - acc: 0.121 - 17s 430ms/step - loss: 0.7562 - acc: 0.1250 - val_loss: 1.1481 - val_acc: 0.0871\n",
            "Epoch 87/100\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.7311 - acc: 0.129 - ETA: 1s - loss: 0.7200 - acc: 0.114 - 17s 430ms/step - loss: 0.7585 - acc: 0.1262 - val_loss: 1.0886 - val_acc: 0.0790\n",
            "Epoch 88/100\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.6046 - acc: 0.121 - ETA: 1s - loss: 0.6906 - acc: 0.137 - 17s 418ms/step - loss: 0.7332 - acc: 0.1403 - val_loss: 1.1215 - val_acc: 0.0871\n",
            "Epoch 89/100\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.6265 - acc: 0.132 - ETA: 1s - loss: 0.6840 - acc: 0.125 - 18s 441ms/step - loss: 0.7377 - acc: 0.1419 - val_loss: 1.1347 - val_acc: 0.0919\n",
            "Epoch 90/100\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.7426 - acc: 0.122 - ETA: 1s - loss: 0.7675 - acc: 0.133 - 17s 436ms/step - loss: 0.7442 - acc: 0.1290 - val_loss: 1.1208 - val_acc: 0.0871\n",
            "Epoch 91/100\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.8120 - acc: 0.155 - ETA: 1s - loss: 0.7627 - acc: 0.144 - 18s 444ms/step - loss: 0.7201 - acc: 0.1371 - val_loss: 1.1171 - val_acc: 0.0871\n",
            "Epoch 92/100\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.8341 - acc: 0.175 - ETA: 1s - loss: 0.7451 - acc: 0.142 - 18s 447ms/step - loss: 0.7220 - acc: 0.1411 - val_loss: 1.1126 - val_acc: 0.0871\n",
            "Epoch 93/100\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.7311 - acc: 0.126 - ETA: 1s - loss: 0.7361 - acc: 0.143 - 19s 480ms/step - loss: 0.7225 - acc: 0.1411 - val_loss: 1.1166 - val_acc: 0.0774\n",
            "Epoch 94/100\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.6113 - acc: 0.132 - ETA: 1s - loss: 0.7145 - acc: 0.145 - 18s 460ms/step - loss: 0.7057 - acc: 0.1448 - val_loss: 1.1618 - val_acc: 0.0823\n",
            "Epoch 95/100\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.7398 - acc: 0.138 - ETA: 1s - loss: 0.7152 - acc: 0.135 - 18s 455ms/step - loss: 0.7178 - acc: 0.1407 - val_loss: 1.1403 - val_acc: 0.0903\n",
            "Epoch 96/100\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.6907 - acc: 0.152 - ETA: 1s - loss: 0.6802 - acc: 0.146 - 20s 497ms/step - loss: 0.6825 - acc: 0.1500 - val_loss: 1.1852 - val_acc: 0.0855\n",
            "Epoch 97/100\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.6301 - acc: 0.129 - ETA: 1s - loss: 0.7403 - acc: 0.156 - 18s 459ms/step - loss: 0.6953 - acc: 0.1423 - val_loss: 1.1584 - val_acc: 0.0823\n",
            "Epoch 98/100\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.6073 - acc: 0.106 - ETA: 1s - loss: 0.7132 - acc: 0.129 - 17s 436ms/step - loss: 0.7238 - acc: 0.1367 - val_loss: 1.1646 - val_acc: 0.0823\n",
            "Epoch 99/100\n",
            "40/40 [==============================] - ETA: 4s - loss: 0.8455 - acc: 0.150 - ETA: 1s - loss: 0.7248 - acc: 0.136 - 19s 478ms/step - loss: 0.7139 - acc: 0.1359 - val_loss: 1.2297 - val_acc: 0.0806\n",
            "Epoch 100/100\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.6654 - acc: 0.141 - ETA: 1s - loss: 0.6824 - acc: 0.152 - 18s 461ms/step - loss: 0.6901 - acc: 0.1500 - val_loss: 1.1145 - val_acc: 0.0935\n",
            "Do you want to run the Model again for 1 epoch more? (1 - Yes | 0 - No) : 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Lo5XVENIuu2X",
        "colab_type": "code",
        "colab": {},
        "outputId": "0981b7c5-a653-4dfa-85b2-c9d97419e8f1"
      },
      "cell_type": "code",
      "source": [
        "while True:\n",
        "    try:\n",
        "        run_again = int(input(\"Do you want to run the Model again for 1 epoch more? (1 - Yes | 0 - No) : \"))\n",
        "        if run_again == 1:\n",
        "            history = model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
        "                    batch_size=16,\n",
        "                    epochs=1000,\n",
        "                    validation_split=0.2)\n",
        "        else:\n",
        "            break\n",
        "    except Exception as e:\n",
        "        break"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Do you want to run the Model again for 1 epoch more? (1 - Yes | 0 - No) : 1\n",
            "Train on 40 samples, validate on 10 samples\n",
            "Epoch 1/1000\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.6921 - acc: 0.122 - ETA: 1s - loss: 0.6983 - acc: 0.151 - 20s 494ms/step - loss: 0.6957 - acc: 0.1452 - val_loss: 1.1281 - val_acc: 0.0790\n",
            "Epoch 2/1000\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.7279 - acc: 0.129 - ETA: 1s - loss: 0.6723 - acc: 0.139 - 17s 420ms/step - loss: 0.6947 - acc: 0.1427 - val_loss: 1.2058 - val_acc: 0.0855\n",
            "Epoch 3/1000\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.7233 - acc: 0.148 - ETA: 1s - loss: 0.6817 - acc: 0.146 - 16s 409ms/step - loss: 0.6864 - acc: 0.1496 - val_loss: 1.1574 - val_acc: 0.0887\n",
            "Epoch 4/1000\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.5748 - acc: 0.128 - ETA: 1s - loss: 0.6294 - acc: 0.128 - 16s 394ms/step - loss: 0.7106 - acc: 0.1347 - val_loss: 1.1828 - val_acc: 0.0839\n",
            "Epoch 5/1000\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.8630 - acc: 0.191 - ETA: 1s - loss: 0.7354 - acc: 0.164 - 17s 413ms/step - loss: 0.7011 - acc: 0.1492 - val_loss: 1.1130 - val_acc: 0.0952\n",
            "Epoch 6/1000\n",
            "40/40 [==============================] - ETA: 2s - loss: 0.6416 - acc: 0.145 - ETA: 1s - loss: 0.6914 - acc: 0.153 - 17s 413ms/step - loss: 0.6762 - acc: 0.1460 - val_loss: 1.1974 - val_acc: 0.0919\n",
            "Epoch 7/1000\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.7043 - acc: 0.161 - ETA: 1s - loss: 0.7176 - acc: 0.149 - 16s 393ms/step - loss: 0.6781 - acc: 0.1444 - val_loss: 1.1404 - val_acc: 0.0871\n",
            "Epoch 8/1000\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.6783 - acc: 0.158 - ETA: 1s - loss: 0.6896 - acc: 0.146 - 18s 442ms/step - loss: 0.6780 - acc: 0.1452 - val_loss: 1.1934 - val_acc: 0.0790\n",
            "Epoch 9/1000\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.6120 - acc: 0.141 - ETA: 1s - loss: 0.6849 - acc: 0.167 - 15s 384ms/step - loss: 0.6570 - acc: 0.1565 - val_loss: 1.1805 - val_acc: 0.0855\n",
            "Epoch 10/1000\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.6820 - acc: 0.170 - ETA: 1s - loss: 0.6722 - acc: 0.157 - 15s 382ms/step - loss: 0.6393 - acc: 0.1585 - val_loss: 1.1781 - val_acc: 0.0887\n",
            "Epoch 11/1000\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.6197 - acc: 0.149 - ETA: 1s - loss: 0.6308 - acc: 0.160 - 15s 387ms/step - loss: 0.6411 - acc: 0.1577 - val_loss: 1.2438 - val_acc: 0.0887\n",
            "Epoch 12/1000\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.6418 - acc: 0.156 - ETA: 1s - loss: 0.6410 - acc: 0.148 - 16s 400ms/step - loss: 0.6632 - acc: 0.1496 - val_loss: 1.1728 - val_acc: 0.0919\n",
            "Epoch 13/1000\n",
            "40/40 [==============================] - ETA: 2s - loss: 0.6679 - acc: 0.176 - ETA: 1s - loss: 0.6665 - acc: 0.159 - 16s 401ms/step - loss: 0.6583 - acc: 0.1552 - val_loss: 1.1781 - val_acc: 0.0871\n",
            "Epoch 14/1000\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.7544 - acc: 0.198 - ETA: 1s - loss: 0.6295 - acc: 0.170 - 15s 380ms/step - loss: 0.6314 - acc: 0.1625 - val_loss: 1.1431 - val_acc: 0.0887\n",
            "Epoch 15/1000\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.5956 - acc: 0.154 - ETA: 1s - loss: 0.6382 - acc: 0.161 - 16s 396ms/step - loss: 0.6252 - acc: 0.1601 - val_loss: 1.2064 - val_acc: 0.0919\n",
            "Epoch 16/1000\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.6958 - acc: 0.163 - ETA: 1s - loss: 0.6783 - acc: 0.176 - 15s 385ms/step - loss: 0.6245 - acc: 0.1581 - val_loss: 1.1940 - val_acc: 0.0855\n",
            "Epoch 17/1000\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.6654 - acc: 0.194 - ETA: 1s - loss: 0.6487 - acc: 0.159 - 16s 402ms/step - loss: 0.6300 - acc: 0.1681 - val_loss: 1.2452 - val_acc: 0.0871\n",
            "Epoch 18/1000\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.5929 - acc: 0.189 - ETA: 1s - loss: 0.5804 - acc: 0.166 - 16s 395ms/step - loss: 0.6242 - acc: 0.1657 - val_loss: 1.1646 - val_acc: 0.0935\n",
            "Epoch 19/1000\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.5849 - acc: 0.140 - ETA: 1s - loss: 0.6096 - acc: 0.152 - 16s 388ms/step - loss: 0.6269 - acc: 0.1621 - val_loss: 1.2031 - val_acc: 0.0839\n",
            "Epoch 20/1000\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.6982 - acc: 0.186 - ETA: 1s - loss: 0.6730 - acc: 0.180 - 16s 390ms/step - loss: 0.6185 - acc: 0.1621 - val_loss: 1.1814 - val_acc: 0.0903\n",
            "Epoch 21/1000\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.5845 - acc: 0.157 - ETA: 1s - loss: 0.5926 - acc: 0.172 - 15s 386ms/step - loss: 0.6002 - acc: 0.1770 - val_loss: 1.3427 - val_acc: 0.0871\n",
            "Epoch 22/1000\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.6694 - acc: 0.183 - ETA: 1s - loss: 0.6132 - acc: 0.171 - 16s 395ms/step - loss: 0.6185 - acc: 0.1685 - val_loss: 1.1788 - val_acc: 0.0887\n",
            "Epoch 23/1000\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.5510 - acc: 0.161 - ETA: 1s - loss: 0.6324 - acc: 0.180 - 15s 385ms/step - loss: 0.6019 - acc: 0.1706 - val_loss: 1.1720 - val_acc: 0.0984\n",
            "Epoch 24/1000\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.5939 - acc: 0.153 - ETA: 1s - loss: 0.6008 - acc: 0.161 - 16s 392ms/step - loss: 0.6135 - acc: 0.1710 - val_loss: 1.3072 - val_acc: 0.0919\n",
            "Epoch 25/1000\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.6225 - acc: 0.181 - ETA: 1s - loss: 0.6071 - acc: 0.172 - 16s 406ms/step - loss: 0.6073 - acc: 0.1706 - val_loss: 1.2252 - val_acc: 0.0952\n",
            "Epoch 26/1000\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.6469 - acc: 0.146 - ETA: 1s - loss: 0.6185 - acc: 0.160 - 16s 399ms/step - loss: 0.5999 - acc: 0.1669 - val_loss: 1.4097 - val_acc: 0.0823\n",
            "Epoch 27/1000\n",
            "40/40 [==============================] - ETA: 2s - loss: 0.5441 - acc: 0.148 - ETA: 0s - loss: 0.5609 - acc: 0.167 - 15s 387ms/step - loss: 0.5931 - acc: 0.1754 - val_loss: 1.2304 - val_acc: 0.1000\n",
            "Epoch 28/1000\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.5412 - acc: 0.175 - ETA: 1s - loss: 0.6175 - acc: 0.191 - 15s 387ms/step - loss: 0.5959 - acc: 0.1746 - val_loss: 1.2082 - val_acc: 0.1016\n",
            "Epoch 29/1000\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.5538 - acc: 0.168 - ETA: 1s - loss: 0.5728 - acc: 0.168 - 15s 385ms/step - loss: 0.5889 - acc: 0.1750 - val_loss: 1.2580 - val_acc: 0.0952\n",
            "Epoch 30/1000\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.5756 - acc: 0.179 - ETA: 1s - loss: 0.5837 - acc: 0.176 - 15s 382ms/step - loss: 0.5786 - acc: 0.1766 - val_loss: 1.2704 - val_acc: 0.0919\n",
            "Epoch 31/1000\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.5247 - acc: 0.177 - ETA: 1s - loss: 0.5850 - acc: 0.189 - 16s 389ms/step - loss: 0.5762 - acc: 0.1879 - val_loss: 1.2031 - val_acc: 0.0823\n",
            "Epoch 32/1000\n",
            "40/40 [==============================] - ETA: 2s - loss: 0.6040 - acc: 0.214 - ETA: 1s - loss: 0.5704 - acc: 0.191 - 15s 384ms/step - loss: 0.5752 - acc: 0.1847 - val_loss: 1.2125 - val_acc: 0.1000\n",
            "Epoch 33/1000\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.5612 - acc: 0.172 - ETA: 1s - loss: 0.5610 - acc: 0.177 - 16s 395ms/step - loss: 0.5647 - acc: 0.1839 - val_loss: 1.2535 - val_acc: 0.0968\n",
            "Epoch 34/1000\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.6000 - acc: 0.205 - ETA: 1s - loss: 0.5777 - acc: 0.187 - 16s 391ms/step - loss: 0.5598 - acc: 0.1802 - val_loss: 1.1902 - val_acc: 0.0984\n",
            "Epoch 35/1000\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.4948 - acc: 0.166 - ETA: 1s - loss: 0.5556 - acc: 0.188 - 16s 388ms/step - loss: 0.5588 - acc: 0.1903 - val_loss: 1.3249 - val_acc: 0.0984\n",
            "Epoch 36/1000\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.5520 - acc: 0.177 - ETA: 1s - loss: 0.5243 - acc: 0.162 - 16s 399ms/step - loss: 0.5483 - acc: 0.1806 - val_loss: 1.3234 - val_acc: 0.0855\n",
            "Epoch 37/1000\n",
            "40/40 [==============================] - ETA: 2s - loss: 0.5659 - acc: 0.166 - ETA: 1s - loss: 0.5689 - acc: 0.179 - 16s 389ms/step - loss: 0.5686 - acc: 0.1782 - val_loss: 1.2534 - val_acc: 0.0903\n",
            "Epoch 38/1000\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.6135 - acc: 0.196 - ETA: 1s - loss: 0.5693 - acc: 0.176 - 16s 391ms/step - loss: 0.5601 - acc: 0.1831 - val_loss: 1.2661 - val_acc: 0.0968\n",
            "Epoch 39/1000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "40/40 [==============================] - ETA: 2s - loss: 0.6393 - acc: 0.226 - ETA: 0s - loss: 0.5662 - acc: 0.186 - 16s 389ms/step - loss: 0.5562 - acc: 0.1859 - val_loss: 1.2161 - val_acc: 0.1000\n",
            "Epoch 40/1000\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.5451 - acc: 0.164 - ETA: 1s - loss: 0.5739 - acc: 0.179 - 16s 398ms/step - loss: 0.5654 - acc: 0.1823 - val_loss: 1.2972 - val_acc: 0.0984\n",
            "Epoch 41/1000\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.5797 - acc: 0.206 - ETA: 1s - loss: 0.5846 - acc: 0.205 - 15s 386ms/step - loss: 0.5464 - acc: 0.1871 - val_loss: 1.2656 - val_acc: 0.1000\n",
            "Epoch 42/1000\n",
            "40/40 [==============================] - ETA: 2s - loss: 0.4163 - acc: 0.126 - ETA: 1s - loss: 0.5370 - acc: 0.174 - 16s 398ms/step - loss: 0.5418 - acc: 0.1859 - val_loss: 1.3117 - val_acc: 0.1000\n",
            "Epoch 43/1000\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.5192 - acc: 0.157 - ETA: 1s - loss: 0.5322 - acc: 0.183 - 15s 385ms/step - loss: 0.5379 - acc: 0.1911 - val_loss: 1.1888 - val_acc: 0.1000\n",
            "Epoch 44/1000\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.5596 - acc: 0.212 - ETA: 1s - loss: 0.5333 - acc: 0.186 - 16s 394ms/step - loss: 0.5462 - acc: 0.1847 - val_loss: 1.3012 - val_acc: 0.0903\n",
            "Epoch 45/1000\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.6110 - acc: 0.208 - ETA: 1s - loss: 0.5319 - acc: 0.182 - 15s 387ms/step - loss: 0.5347 - acc: 0.1948 - val_loss: 1.2234 - val_acc: 0.0952\n",
            "Epoch 46/1000\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.4695 - acc: 0.153 - ETA: 1s - loss: 0.4941 - acc: 0.185 - 15s 382ms/step - loss: 0.5322 - acc: 0.1919 - val_loss: 1.2259 - val_acc: 0.0968\n",
            "Epoch 47/1000\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.5996 - acc: 0.245 - ETA: 0s - loss: 0.5163 - acc: 0.204 - 15s 381ms/step - loss: 0.5208 - acc: 0.1948 - val_loss: 1.1962 - val_acc: 0.0968\n",
            "Epoch 48/1000\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.4557 - acc: 0.174 - ETA: 1s - loss: 0.4722 - acc: 0.185 - 16s 396ms/step - loss: 0.5048 - acc: 0.2036 - val_loss: 1.3305 - val_acc: 0.0806\n",
            "Epoch 49/1000\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.6351 - acc: 0.201 - ETA: 1s - loss: 0.5384 - acc: 0.180 - 16s 397ms/step - loss: 0.5293 - acc: 0.1960 - val_loss: 1.1967 - val_acc: 0.0871\n",
            "Epoch 50/1000\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.5626 - acc: 0.178 - ETA: 1s - loss: 0.5121 - acc: 0.189 - 16s 392ms/step - loss: 0.5107 - acc: 0.1976 - val_loss: 1.2352 - val_acc: 0.0984\n",
            "Epoch 51/1000\n",
            "40/40 [==============================] - ETA: 2s - loss: 0.4947 - acc: 0.190 - ETA: 0s - loss: 0.4900 - acc: 0.191 - 15s 387ms/step - loss: 0.4982 - acc: 0.2004 - val_loss: 1.1842 - val_acc: 0.0968\n",
            "Epoch 52/1000\n",
            "40/40 [==============================] - ETA: 2s - loss: 0.6079 - acc: 0.226 - ETA: 1s - loss: 0.5420 - acc: 0.197 - 15s 386ms/step - loss: 0.5234 - acc: 0.1895 - val_loss: 1.2689 - val_acc: 0.0919\n",
            "Epoch 53/1000\n",
            "40/40 [==============================] - ETA: 2s - loss: 0.4691 - acc: 0.165 - ETA: 0s - loss: 0.5153 - acc: 0.191 - 16s 389ms/step - loss: 0.5000 - acc: 0.1964 - val_loss: 1.3086 - val_acc: 0.0887\n",
            "Epoch 54/1000\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.5539 - acc: 0.209 - ETA: 0s - loss: 0.5310 - acc: 0.193 - 15s 386ms/step - loss: 0.5157 - acc: 0.1984 - val_loss: 1.3213 - val_acc: 0.0855\n",
            "Epoch 55/1000\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.5073 - acc: 0.199 - ETA: 0s - loss: 0.5059 - acc: 0.200 - 16s 389ms/step - loss: 0.4978 - acc: 0.1992 - val_loss: 1.3313 - val_acc: 0.0903\n",
            "Epoch 56/1000\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.4720 - acc: 0.170 - ETA: 0s - loss: 0.5018 - acc: 0.202 - 15s 387ms/step - loss: 0.4976 - acc: 0.1952 - val_loss: 1.1833 - val_acc: 0.0984\n",
            "Epoch 57/1000\n",
            "40/40 [==============================] - ETA: 2s - loss: 0.4930 - acc: 0.225 - ETA: 0s - loss: 0.4770 - acc: 0.202 - 16s 391ms/step - loss: 0.4926 - acc: 0.2081 - val_loss: 1.2149 - val_acc: 0.1000\n",
            "Epoch 58/1000\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.4804 - acc: 0.196 - ETA: 1s - loss: 0.5087 - acc: 0.217 - 15s 385ms/step - loss: 0.4902 - acc: 0.2077 - val_loss: 1.3169 - val_acc: 0.0903\n",
            "Epoch 59/1000\n",
            "40/40 [==============================] - ETA: 2s - loss: 0.3883 - acc: 0.143 - ETA: 0s - loss: 0.4737 - acc: 0.206 - 16s 389ms/step - loss: 0.4793 - acc: 0.2069 - val_loss: 1.2679 - val_acc: 0.0968\n",
            "Epoch 60/1000\n",
            "40/40 [==============================] - ETA: 2s - loss: 0.5120 - acc: 0.242 - ETA: 0s - loss: 0.4443 - acc: 0.208 - 15s 384ms/step - loss: 0.4630 - acc: 0.2161 - val_loss: 1.2892 - val_acc: 0.0952\n",
            "Epoch 61/1000\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.4327 - acc: 0.169 - ETA: 1s - loss: 0.4460 - acc: 0.195 - 16s 401ms/step - loss: 0.4646 - acc: 0.2125 - val_loss: 1.2625 - val_acc: 0.0887\n",
            "Epoch 62/1000\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.5113 - acc: 0.198 - ETA: 1s - loss: 0.5579 - acc: 0.200 - 15s 385ms/step - loss: 0.5162 - acc: 0.1964 - val_loss: 1.2484 - val_acc: 0.1048\n",
            "Epoch 63/1000\n",
            "40/40 [==============================] - ETA: 2s - loss: 0.3816 - acc: 0.220 - ETA: 1s - loss: 0.4200 - acc: 0.208 - 16s 394ms/step - loss: 0.4569 - acc: 0.2169 - val_loss: 1.3828 - val_acc: 0.0984\n",
            "Epoch 64/1000\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.5119 - acc: 0.251 - ETA: 1s - loss: 0.4691 - acc: 0.218 - 16s 393ms/step - loss: 0.4670 - acc: 0.2173 - val_loss: 1.2755 - val_acc: 0.0952\n",
            "Epoch 65/1000\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.5610 - acc: 0.244 - ETA: 1s - loss: 0.5071 - acc: 0.215 - 16s 395ms/step - loss: 0.4773 - acc: 0.2121 - val_loss: 1.2501 - val_acc: 0.1048\n",
            "Epoch 66/1000\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.5314 - acc: 0.246 - ETA: 1s - loss: 0.4804 - acc: 0.211 - 15s 384ms/step - loss: 0.4725 - acc: 0.2113 - val_loss: 1.3526 - val_acc: 0.1016\n",
            "Epoch 67/1000\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.3849 - acc: 0.176 - ETA: 1s - loss: 0.4413 - acc: 0.211 - 15s 380ms/step - loss: 0.4450 - acc: 0.2165 - val_loss: 1.3369 - val_acc: 0.0935\n",
            "Epoch 68/1000\n",
            "40/40 [==============================] - ETA: 2s - loss: 0.4191 - acc: 0.231 - ETA: 0s - loss: 0.4512 - acc: 0.225 - 15s 383ms/step - loss: 0.4379 - acc: 0.2226 - val_loss: 1.2737 - val_acc: 0.1065\n",
            "Epoch 69/1000\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.4538 - acc: 0.190 - ETA: 0s - loss: 0.4855 - acc: 0.226 - 15s 379ms/step - loss: 0.4617 - acc: 0.2137 - val_loss: 1.5000 - val_acc: 0.0903\n",
            "Epoch 70/1000\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.4762 - acc: 0.195 - ETA: 1s - loss: 0.5050 - acc: 0.227 - 16s 392ms/step - loss: 0.4782 - acc: 0.2097 - val_loss: 1.2782 - val_acc: 0.0968\n",
            "Epoch 71/1000\n",
            "40/40 [==============================] - ETA: 2s - loss: 0.3815 - acc: 0.234 - ETA: 1s - loss: 0.4267 - acc: 0.213 - 16s 388ms/step - loss: 0.4346 - acc: 0.2210 - val_loss: 1.3523 - val_acc: 0.1000\n",
            "Epoch 72/1000\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.4826 - acc: 0.197 - ETA: 1s - loss: 0.4773 - acc: 0.214 - 16s 390ms/step - loss: 0.4655 - acc: 0.2141 - val_loss: 1.3278 - val_acc: 0.0984\n",
            "Epoch 73/1000\n",
            "40/40 [==============================] - ETA: 2s - loss: 0.4392 - acc: 0.178 - ETA: 1s - loss: 0.4669 - acc: 0.218 - 16s 394ms/step - loss: 0.4444 - acc: 0.2141 - val_loss: 1.3135 - val_acc: 0.0968\n",
            "Epoch 74/1000\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.4263 - acc: 0.228 - ETA: 1s - loss: 0.4197 - acc: 0.219 - 16s 401ms/step - loss: 0.4273 - acc: 0.2198 - val_loss: 1.3890 - val_acc: 0.0984\n",
            "Epoch 75/1000\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.4528 - acc: 0.191 - ETA: 1s - loss: 0.4843 - acc: 0.221 - 15s 386ms/step - loss: 0.4539 - acc: 0.2129 - val_loss: 1.3641 - val_acc: 0.1000\n",
            "Epoch 76/1000\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.3722 - acc: 0.213 - ETA: 1s - loss: 0.4352 - acc: 0.232 - 16s 393ms/step - loss: 0.4360 - acc: 0.2266 - val_loss: 1.3314 - val_acc: 0.0952\n",
            "Epoch 77/1000\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.4267 - acc: 0.229 - ETA: 1s - loss: 0.4269 - acc: 0.236 - 16s 393ms/step - loss: 0.4237 - acc: 0.2262 - val_loss: 1.3890 - val_acc: 0.0887\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 78/1000\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.3761 - acc: 0.179 - ETA: 0s - loss: 0.4224 - acc: 0.208 - 15s 386ms/step - loss: 0.4361 - acc: 0.2226 - val_loss: 1.3255 - val_acc: 0.1000\n",
            "Epoch 79/1000\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.3423 - acc: 0.180 - ETA: 1s - loss: 0.4039 - acc: 0.221 - 16s 389ms/step - loss: 0.4154 - acc: 0.2290 - val_loss: 1.3037 - val_acc: 0.1000\n",
            "Epoch 80/1000\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.4296 - acc: 0.227 - ETA: 1s - loss: 0.4079 - acc: 0.240 - 15s 386ms/step - loss: 0.4011 - acc: 0.2347 - val_loss: 1.3582 - val_acc: 0.1000\n",
            "Epoch 81/1000\n",
            "40/40 [==============================] - ETA: 2s - loss: 0.3997 - acc: 0.207 - ETA: 0s - loss: 0.3967 - acc: 0.229 - 16s 392ms/step - loss: 0.4180 - acc: 0.2262 - val_loss: 1.3810 - val_acc: 0.1032\n",
            "Epoch 82/1000\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.4688 - acc: 0.262 - ETA: 1s - loss: 0.4191 - acc: 0.238 - 16s 398ms/step - loss: 0.4110 - acc: 0.2278 - val_loss: 1.3224 - val_acc: 0.1000\n",
            "Epoch 83/1000\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.3592 - acc: 0.246 - ETA: 1s - loss: 0.3846 - acc: 0.216 - 16s 392ms/step - loss: 0.4075 - acc: 0.2298 - val_loss: 1.4088 - val_acc: 0.0935\n",
            "Epoch 84/1000\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.5173 - acc: 0.245 - ETA: 1s - loss: 0.4383 - acc: 0.230 - 15s 386ms/step - loss: 0.4144 - acc: 0.2242 - val_loss: 1.3709 - val_acc: 0.0968\n",
            "Epoch 85/1000\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.4228 - acc: 0.222 - ETA: 1s - loss: 0.3987 - acc: 0.225 - 16s 391ms/step - loss: 0.4137 - acc: 0.2290 - val_loss: 1.3263 - val_acc: 0.0984\n",
            "Epoch 86/1000\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.3835 - acc: 0.192 - ETA: 1s - loss: 0.3653 - acc: 0.214 - 16s 390ms/step - loss: 0.3981 - acc: 0.2355 - val_loss: 1.3576 - val_acc: 0.1032\n",
            "Epoch 87/1000\n",
            "40/40 [==============================] - ETA: 2s - loss: 0.4781 - acc: 0.246 - ETA: 0s - loss: 0.3833 - acc: 0.219 - 16s 392ms/step - loss: 0.3934 - acc: 0.2383 - val_loss: 1.3953 - val_acc: 0.0968\n",
            "Epoch 88/1000\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.3508 - acc: 0.214 - ETA: 1s - loss: 0.3709 - acc: 0.239 - 16s 411ms/step - loss: 0.3852 - acc: 0.2339 - val_loss: 1.3128 - val_acc: 0.0984\n",
            "Epoch 89/1000\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.4019 - acc: 0.244 - ETA: 1s - loss: 0.3866 - acc: 0.224 - 15s 387ms/step - loss: 0.4012 - acc: 0.2359 - val_loss: 1.3075 - val_acc: 0.0968\n",
            "Epoch 90/1000\n",
            "40/40 [==============================] - ETA: 2s - loss: 0.3621 - acc: 0.181 - ETA: 1s - loss: 0.3949 - acc: 0.221 - 16s 398ms/step - loss: 0.3934 - acc: 0.2351 - val_loss: 1.2737 - val_acc: 0.1081\n",
            "Epoch 91/1000\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.3851 - acc: 0.224 - ETA: 1s - loss: 0.3600 - acc: 0.222 - 16s 400ms/step - loss: 0.3925 - acc: 0.2331 - val_loss: 1.3542 - val_acc: 0.1016\n",
            "Epoch 92/1000\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.3905 - acc: 0.259 - ETA: 1s - loss: 0.3818 - acc: 0.257 - 15s 384ms/step - loss: 0.3711 - acc: 0.2448 - val_loss: 1.3083 - val_acc: 0.1016\n",
            "Epoch 93/1000\n",
            "40/40 [==============================] - ETA: 2s - loss: 0.3019 - acc: 0.216 - ETA: 1s - loss: 0.3531 - acc: 0.251 - 16s 394ms/step - loss: 0.3610 - acc: 0.2480 - val_loss: 1.3419 - val_acc: 0.1016\n",
            "Epoch 94/1000\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.3992 - acc: 0.202 - ETA: 1s - loss: 0.4186 - acc: 0.246 - 16s 398ms/step - loss: 0.4075 - acc: 0.2290 - val_loss: 1.4155 - val_acc: 0.0935\n",
            "Epoch 95/1000\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.4142 - acc: 0.224 - ETA: 1s - loss: 0.3828 - acc: 0.235 - 16s 393ms/step - loss: 0.3841 - acc: 0.2347 - val_loss: 1.3273 - val_acc: 0.1032\n",
            "Epoch 96/1000\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.3583 - acc: 0.189 - ETA: 1s - loss: 0.3909 - acc: 0.239 - 16s 396ms/step - loss: 0.3747 - acc: 0.2435 - val_loss: 1.3827 - val_acc: 0.1032\n",
            "Epoch 97/1000\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.3563 - acc: 0.207 - ETA: 1s - loss: 0.3380 - acc: 0.232 - 15s 385ms/step - loss: 0.3551 - acc: 0.2468 - val_loss: 1.3141 - val_acc: 0.1065\n",
            "Epoch 98/1000\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.4112 - acc: 0.257 - ETA: 1s - loss: 0.3854 - acc: 0.238 - 16s 394ms/step - loss: 0.3800 - acc: 0.2363 - val_loss: 1.3630 - val_acc: 0.1000\n",
            "Epoch 99/1000\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.4053 - acc: 0.210 - ETA: 1s - loss: 0.3638 - acc: 0.226 - 15s 377ms/step - loss: 0.3803 - acc: 0.2383 - val_loss: 1.4425 - val_acc: 0.1032\n",
            "Epoch 100/1000\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.3412 - acc: 0.251 - ETA: 0s - loss: 0.3515 - acc: 0.252 - 15s 381ms/step - loss: 0.3683 - acc: 0.2492 - val_loss: 1.3548 - val_acc: 0.1081\n",
            "Epoch 101/1000\n",
            "40/40 [==============================] - ETA: 2s - loss: 0.4275 - acc: 0.284 - ETA: 0s - loss: 0.3784 - acc: 0.260 - 15s 374ms/step - loss: 0.3613 - acc: 0.2492 - val_loss: 1.3596 - val_acc: 0.1000\n",
            "Epoch 102/1000\n",
            "40/40 [==============================] - ETA: 2s - loss: 0.3965 - acc: 0.302 - ETA: 1s - loss: 0.3508 - acc: 0.248 - 16s 390ms/step - loss: 0.3501 - acc: 0.2440 - val_loss: 1.4665 - val_acc: 0.0968\n",
            "Epoch 103/1000\n",
            "40/40 [==============================] - ETA: 2s - loss: 0.3544 - acc: 0.231 - ETA: 1s - loss: 0.3670 - acc: 0.246 - 15s 387ms/step - loss: 0.3613 - acc: 0.2427 - val_loss: 1.3756 - val_acc: 0.0968\n",
            "Epoch 104/1000\n",
            "40/40 [==============================] - ETA: 2s - loss: 0.3868 - acc: 0.274 - ETA: 1s - loss: 0.3561 - acc: 0.252 - 15s 380ms/step - loss: 0.3584 - acc: 0.2464 - val_loss: 1.4237 - val_acc: 0.0919\n",
            "Epoch 105/1000\n",
            "40/40 [==============================] - ETA: 2s - loss: 0.3556 - acc: 0.255 - ETA: 0s - loss: 0.3681 - acc: 0.255 - 15s 378ms/step - loss: 0.3540 - acc: 0.2468 - val_loss: 1.3926 - val_acc: 0.1016\n",
            "Epoch 106/1000\n",
            "40/40 [==============================] - ETA: 2s - loss: 0.3305 - acc: 0.209 - ETA: 0s - loss: 0.3344 - acc: 0.242 - 15s 381ms/step - loss: 0.3590 - acc: 0.2540 - val_loss: 1.2927 - val_acc: 0.0952\n",
            "Epoch 107/1000\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.3969 - acc: 0.259 - ETA: 1s - loss: 0.3855 - acc: 0.272 - 15s 384ms/step - loss: 0.3620 - acc: 0.2472 - val_loss: 1.3359 - val_acc: 0.0952\n",
            "Epoch 108/1000\n",
            "40/40 [==============================] - ETA: 2s - loss: 0.3599 - acc: 0.255 - ETA: 1s - loss: 0.3479 - acc: 0.250 - 15s 385ms/step - loss: 0.3437 - acc: 0.2532 - val_loss: 1.3763 - val_acc: 0.1048\n",
            "Epoch 109/1000\n",
            "40/40 [==============================] - ETA: 2s - loss: 0.2770 - acc: 0.216 - ETA: 0s - loss: 0.3103 - acc: 0.225 - 15s 384ms/step - loss: 0.3418 - acc: 0.2536 - val_loss: 1.3085 - val_acc: 0.1032\n",
            "Epoch 110/1000\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.3405 - acc: 0.195 - ETA: 1s - loss: 0.3465 - acc: 0.236 - 16s 401ms/step - loss: 0.3491 - acc: 0.2476 - val_loss: 1.3597 - val_acc: 0.0984\n",
            "Epoch 111/1000\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.3647 - acc: 0.270 - ETA: 0s - loss: 0.3545 - acc: 0.263 - 16s 391ms/step - loss: 0.3446 - acc: 0.2472 - val_loss: 1.3829 - val_acc: 0.1016\n",
            "Epoch 112/1000\n",
            "40/40 [==============================] - ETA: 2s - loss: 0.3287 - acc: 0.259 - ETA: 0s - loss: 0.3318 - acc: 0.266 - 16s 388ms/step - loss: 0.3281 - acc: 0.2556 - val_loss: 1.4885 - val_acc: 0.0968\n",
            "Epoch 113/1000\n",
            "40/40 [==============================] - ETA: 2s - loss: 0.3204 - acc: 0.226 - ETA: 0s - loss: 0.3243 - acc: 0.231 - 15s 386ms/step - loss: 0.3369 - acc: 0.2468 - val_loss: 1.3671 - val_acc: 0.0984\n",
            "Epoch 114/1000\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.3645 - acc: 0.255 - ETA: 1s - loss: 0.3364 - acc: 0.242 - 15s 386ms/step - loss: 0.3475 - acc: 0.2472 - val_loss: 1.4022 - val_acc: 0.0968\n",
            "Epoch 115/1000\n",
            "40/40 [==============================] - ETA: 2s - loss: 0.3217 - acc: 0.211 - ETA: 1s - loss: 0.3347 - acc: 0.265 - 15s 385ms/step - loss: 0.3222 - acc: 0.2528 - val_loss: 1.3920 - val_acc: 0.0903\n",
            "Epoch 116/1000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "40/40 [==============================] - ETA: 2s - loss: 0.3494 - acc: 0.279 - ETA: 0s - loss: 0.3305 - acc: 0.265 - 15s 378ms/step - loss: 0.3211 - acc: 0.2597 - val_loss: 1.3437 - val_acc: 0.1097\n",
            "Epoch 117/1000\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.3404 - acc: 0.282 - ETA: 0s - loss: 0.3252 - acc: 0.274 - 16s 389ms/step - loss: 0.3134 - acc: 0.2597 - val_loss: 1.3924 - val_acc: 0.1048\n",
            "Epoch 118/1000\n",
            "40/40 [==============================] - ETA: 2s - loss: 0.2658 - acc: 0.263 - ETA: 0s - loss: 0.2876 - acc: 0.258 - 16s 390ms/step - loss: 0.3146 - acc: 0.2629 - val_loss: 1.3417 - val_acc: 0.0952\n",
            "Epoch 119/1000\n",
            "40/40 [==============================] - ETA: 2s - loss: 0.3319 - acc: 0.255 - ETA: 0s - loss: 0.3330 - acc: 0.249 - 15s 387ms/step - loss: 0.3439 - acc: 0.2488 - val_loss: 1.4393 - val_acc: 0.0887\n",
            "Epoch 120/1000\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.3870 - acc: 0.285 - ETA: 1s - loss: 0.3443 - acc: 0.260 - 15s 381ms/step - loss: 0.3274 - acc: 0.2512 - val_loss: 1.4381 - val_acc: 0.0984\n",
            "Epoch 121/1000\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.2983 - acc: 0.274 - ETA: 1s - loss: 0.3065 - acc: 0.275 - 16s 400ms/step - loss: 0.3028 - acc: 0.2645 - val_loss: 1.4434 - val_acc: 0.0935\n",
            "Epoch 122/1000\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.2760 - acc: 0.257 - ETA: 0s - loss: 0.3098 - acc: 0.269 - 15s 387ms/step - loss: 0.3158 - acc: 0.2597 - val_loss: 1.4196 - val_acc: 0.0968\n",
            "Epoch 123/1000\n",
            "40/40 [==============================] - ETA: 2s - loss: 0.3543 - acc: 0.267 - ETA: 1s - loss: 0.3161 - acc: 0.245 - 16s 391ms/step - loss: 0.3248 - acc: 0.2637 - val_loss: 1.5684 - val_acc: 0.0935\n",
            "Epoch 124/1000\n",
            "40/40 [==============================] - ETA: 2s - loss: 0.2911 - acc: 0.295 - ETA: 1s - loss: 0.2858 - acc: 0.267 - 16s 391ms/step - loss: 0.2983 - acc: 0.2677 - val_loss: 1.4911 - val_acc: 0.0952\n",
            "Epoch 125/1000\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.2994 - acc: 0.238 - ETA: 1s - loss: 0.3141 - acc: 0.247 - 16s 396ms/step - loss: 0.3300 - acc: 0.2484 - val_loss: 1.3959 - val_acc: 0.1000\n",
            "Epoch 126/1000\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.3254 - acc: 0.259 - ETA: 1s - loss: 0.3403 - acc: 0.273 - 16s 404ms/step - loss: 0.3248 - acc: 0.2605 - val_loss: 1.4624 - val_acc: 0.0919\n",
            "Epoch 127/1000\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.2800 - acc: 0.236 - ETA: 1s - loss: 0.3057 - acc: 0.251 - 15s 385ms/step - loss: 0.3080 - acc: 0.2589 - val_loss: 1.5288 - val_acc: 0.0919\n",
            "Epoch 128/1000\n",
            "40/40 [==============================] - ETA: 2s - loss: 0.3111 - acc: 0.270 - ETA: 0s - loss: 0.2903 - acc: 0.290 - 16s 394ms/step - loss: 0.2842 - acc: 0.2710 - val_loss: 1.4470 - val_acc: 0.0919\n",
            "Epoch 129/1000\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.2709 - acc: 0.248 - ETA: 1s - loss: 0.2898 - acc: 0.264 - 16s 388ms/step - loss: 0.3014 - acc: 0.2653 - val_loss: 1.4561 - val_acc: 0.0919\n",
            "Epoch 130/1000\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.3254 - acc: 0.277 - ETA: 1s - loss: 0.3180 - acc: 0.248 - 15s 387ms/step - loss: 0.3207 - acc: 0.2589 - val_loss: 1.4627 - val_acc: 0.1000\n",
            "Epoch 131/1000\n",
            "40/40 [==============================] - ETA: 2s - loss: 0.3116 - acc: 0.223 - ETA: 0s - loss: 0.3111 - acc: 0.234 - 16s 390ms/step - loss: 0.3122 - acc: 0.2556 - val_loss: 1.4800 - val_acc: 0.0968\n",
            "Epoch 132/1000\n",
            "40/40 [==============================] - ETA: 2s - loss: 0.3139 - acc: 0.283 - ETA: 1s - loss: 0.2832 - acc: 0.257 - 15s 377ms/step - loss: 0.2987 - acc: 0.2649 - val_loss: 1.4563 - val_acc: 0.1016\n",
            "Epoch 133/1000\n",
            "40/40 [==============================] - ETA: 2s - loss: 0.2535 - acc: 0.274 - ETA: 0s - loss: 0.2798 - acc: 0.284 - 15s 376ms/step - loss: 0.2881 - acc: 0.2649 - val_loss: 1.3569 - val_acc: 0.0887\n",
            "Epoch 134/1000\n",
            "40/40 [==============================] - ETA: 2s - loss: 0.3048 - acc: 0.293 - ETA: 1s - loss: 0.3161 - acc: 0.280 - 15s 381ms/step - loss: 0.3142 - acc: 0.2645 - val_loss: 1.4715 - val_acc: 0.0871\n",
            "Epoch 135/1000\n",
            "40/40 [==============================] - ETA: 2s - loss: 0.2956 - acc: 0.203 - ETA: 1s - loss: 0.2939 - acc: 0.250 - 15s 381ms/step - loss: 0.2992 - acc: 0.2577 - val_loss: 1.4533 - val_acc: 0.0839\n",
            "Epoch 136/1000\n",
            "40/40 [==============================] - ETA: 2s - loss: 0.3257 - acc: 0.344 - ETA: 0s - loss: 0.2827 - acc: 0.261 - 15s 377ms/step - loss: 0.2893 - acc: 0.2673 - val_loss: 1.4686 - val_acc: 0.0919\n",
            "Epoch 137/1000\n",
            "40/40 [==============================] - ETA: 2s - loss: 0.3153 - acc: 0.266 - ETA: 0s - loss: 0.2995 - acc: 0.255 - 15s 378ms/step - loss: 0.3046 - acc: 0.2577 - val_loss: 1.4731 - val_acc: 0.0984\n",
            "Epoch 138/1000\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.2873 - acc: 0.309 - ETA: 1s - loss: 0.2742 - acc: 0.261 - 16s 393ms/step - loss: 0.2802 - acc: 0.2726 - val_loss: 1.4231 - val_acc: 0.1032\n",
            "Epoch 139/1000\n",
            "40/40 [==============================] - ETA: 2s - loss: 0.2855 - acc: 0.310 - ETA: 0s - loss: 0.2617 - acc: 0.265 - 15s 386ms/step - loss: 0.2682 - acc: 0.2742 - val_loss: 1.4936 - val_acc: 0.0919\n",
            "Epoch 140/1000\n",
            "40/40 [==============================] - ETA: 2s - loss: 0.3125 - acc: 0.309 - ETA: 1s - loss: 0.2905 - acc: 0.278 - 15s 383ms/step - loss: 0.2869 - acc: 0.2706 - val_loss: 1.5074 - val_acc: 0.1048\n",
            "Epoch 141/1000\n",
            "40/40 [==============================] - ETA: 2s - loss: 0.2679 - acc: 0.264 - ETA: 0s - loss: 0.2598 - acc: 0.263 - 15s 379ms/step - loss: 0.2740 - acc: 0.2754 - val_loss: 1.4749 - val_acc: 0.0903\n",
            "Epoch 142/1000\n",
            "40/40 [==============================] - ETA: 2s - loss: 0.3434 - acc: 0.278 - ETA: 1s - loss: 0.3064 - acc: 0.269 - 15s 386ms/step - loss: 0.3110 - acc: 0.2649 - val_loss: 1.4700 - val_acc: 0.0968\n",
            "Epoch 143/1000\n",
            "40/40 [==============================] - ETA: 2s - loss: 0.3049 - acc: 0.285 - ETA: 0s - loss: 0.2831 - acc: 0.283 - 15s 376ms/step - loss: 0.2790 - acc: 0.2706 - val_loss: 1.5809 - val_acc: 0.0871\n",
            "Epoch 144/1000\n",
            "40/40 [==============================] - ETA: 2s - loss: 0.2585 - acc: 0.206 - ETA: 1s - loss: 0.2918 - acc: 0.254 - 15s 386ms/step - loss: 0.2834 - acc: 0.2673 - val_loss: 1.5932 - val_acc: 0.0903\n",
            "Epoch 145/1000\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.2612 - acc: 0.239 - ETA: 1s - loss: 0.2735 - acc: 0.274 - 15s 375ms/step - loss: 0.2762 - acc: 0.2754 - val_loss: 1.5089 - val_acc: 0.1048\n",
            "Epoch 146/1000\n",
            "40/40 [==============================] - ETA: 2s - loss: 0.2695 - acc: 0.327 - ETA: 1s - loss: 0.2650 - acc: 0.270 - 15s 387ms/step - loss: 0.2775 - acc: 0.2754 - val_loss: 1.5449 - val_acc: 0.0968\n",
            "Epoch 147/1000\n",
            "40/40 [==============================] - ETA: 2s - loss: 0.3332 - acc: 0.317 - ETA: 0s - loss: 0.2773 - acc: 0.279 - 15s 378ms/step - loss: 0.2763 - acc: 0.2730 - val_loss: 1.6071 - val_acc: 0.1000\n",
            "Epoch 148/1000\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.2801 - acc: 0.335 - ETA: 0s - loss: 0.2644 - acc: 0.299 - 15s 375ms/step - loss: 0.2640 - acc: 0.2770 - val_loss: 1.5609 - val_acc: 0.0968\n",
            "Epoch 149/1000\n",
            "40/40 [==============================] - ETA: 2s - loss: 0.2556 - acc: 0.270 - ETA: 0s - loss: 0.2659 - acc: 0.294 - 15s 383ms/step - loss: 0.2641 - acc: 0.2774 - val_loss: 1.5215 - val_acc: 0.1113\n",
            "Epoch 150/1000\n",
            "40/40 [==============================] - ETA: 2s - loss: 0.2507 - acc: 0.248 - ETA: 0s - loss: 0.2498 - acc: 0.276 - 15s 381ms/step - loss: 0.2511 - acc: 0.2819 - val_loss: 1.5834 - val_acc: 0.1032\n",
            "Epoch 151/1000\n",
            "40/40 [==============================] - ETA: 2s - loss: 0.2601 - acc: 0.324 - ETA: 0s - loss: 0.2792 - acc: 0.307 - 15s 380ms/step - loss: 0.2676 - acc: 0.2827 - val_loss: 1.4810 - val_acc: 0.0984\n",
            "Epoch 152/1000\n",
            "40/40 [==============================] - ETA: 2s - loss: 0.2233 - acc: 0.261 - ETA: 0s - loss: 0.2505 - acc: 0.291 - 15s 384ms/step - loss: 0.2529 - acc: 0.2843 - val_loss: 1.4959 - val_acc: 0.1032\n",
            "Epoch 153/1000\n",
            "40/40 [==============================] - ETA: 2s - loss: 0.2901 - acc: 0.298 - ETA: 0s - loss: 0.2583 - acc: 0.278 - 15s 385ms/step - loss: 0.2602 - acc: 0.2794 - val_loss: 1.5819 - val_acc: 0.0935\n",
            "Epoch 154/1000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "40/40 [==============================] - ETA: 2s - loss: 0.2625 - acc: 0.248 - ETA: 1s - loss: 0.2652 - acc: 0.264 - 16s 407ms/step - loss: 0.2613 - acc: 0.2774 - val_loss: 1.5514 - val_acc: 0.0919\n",
            "Epoch 155/1000\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.2466 - acc: 0.268 - ETA: 1s - loss: 0.2582 - acc: 0.273 - 16s 404ms/step - loss: 0.2538 - acc: 0.2819 - val_loss: 1.5071 - val_acc: 0.0935\n",
            "Epoch 156/1000\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.2435 - acc: 0.284 - ETA: 1s - loss: 0.2527 - acc: 0.287 - 18s 450ms/step - loss: 0.2538 - acc: 0.2823 - val_loss: 1.6060 - val_acc: 0.0919\n",
            "Epoch 157/1000\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.3292 - acc: 0.348 - ETA: 1s - loss: 0.2909 - acc: 0.284 - 18s 444ms/step - loss: 0.2691 - acc: 0.2710 - val_loss: 1.6272 - val_acc: 0.0887\n",
            "Epoch 158/1000\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.2600 - acc: 0.278 - ETA: 1s - loss: 0.2844 - acc: 0.286 - 17s 437ms/step - loss: 0.2712 - acc: 0.2782 - val_loss: 1.5345 - val_acc: 0.0919\n",
            "Epoch 159/1000\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.2363 - acc: 0.261 - ETA: 1s - loss: 0.2432 - acc: 0.262 - 17s 433ms/step - loss: 0.2557 - acc: 0.2810 - val_loss: 1.5009 - val_acc: 0.0952\n",
            "Epoch 160/1000\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.2187 - acc: 0.216 - ETA: 1s - loss: 0.2403 - acc: 0.257 - 16s 389ms/step - loss: 0.2531 - acc: 0.2738 - val_loss: 1.5357 - val_acc: 0.0903\n",
            "Epoch 161/1000\n",
            "40/40 [==============================] - ETA: 2s - loss: 0.2363 - acc: 0.229 - ETA: 0s - loss: 0.2752 - acc: 0.286 - 16s 392ms/step - loss: 0.2729 - acc: 0.2726 - val_loss: 1.5093 - val_acc: 0.0952\n",
            "Epoch 162/1000\n",
            "40/40 [==============================] - ETA: 2s - loss: 0.2770 - acc: 0.316 - ETA: 0s - loss: 0.2696 - acc: 0.289 - 17s 417ms/step - loss: 0.2676 - acc: 0.2754 - val_loss: 1.6174 - val_acc: 0.1000\n",
            "Epoch 163/1000\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.2536 - acc: 0.281 - ETA: 1s - loss: 0.2694 - acc: 0.297 - 18s 455ms/step - loss: 0.2552 - acc: 0.2827 - val_loss: 1.5334 - val_acc: 0.1000\n",
            "Epoch 164/1000\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.2622 - acc: 0.294 - ETA: 1s - loss: 0.2562 - acc: 0.295 - 16s 392ms/step - loss: 0.2494 - acc: 0.2847 - val_loss: 1.5599 - val_acc: 0.1081\n",
            "Epoch 165/1000\n",
            "40/40 [==============================] - ETA: 2s - loss: 0.2298 - acc: 0.289 - ETA: 1s - loss: 0.2260 - acc: 0.288 - 15s 385ms/step - loss: 0.2354 - acc: 0.2895 - val_loss: 1.5057 - val_acc: 0.1016\n",
            "Epoch 166/1000\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.2590 - acc: 0.316 - ETA: 1s - loss: 0.2393 - acc: 0.279 - 16s 403ms/step - loss: 0.2403 - acc: 0.2863 - val_loss: 1.4778 - val_acc: 0.1000\n",
            "Epoch 167/1000\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.2146 - acc: 0.265 - ETA: 0s - loss: 0.2222 - acc: 0.265 - 16s 390ms/step - loss: 0.2408 - acc: 0.2903 - val_loss: 1.5488 - val_acc: 0.0984\n",
            "Epoch 168/1000\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.2832 - acc: 0.247 - ETA: 1s - loss: 0.2696 - acc: 0.300 - 16s 402ms/step - loss: 0.2515 - acc: 0.2867 - val_loss: 1.5786 - val_acc: 0.1000\n",
            "Epoch 169/1000\n",
            "40/40 [==============================] - ETA: 2s - loss: 0.2567 - acc: 0.344 - ETA: 0s - loss: 0.2454 - acc: 0.290 - 15s 384ms/step - loss: 0.2381 - acc: 0.2903 - val_loss: 1.5793 - val_acc: 0.0952\n",
            "Epoch 170/1000\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.2755 - acc: 0.279 - ETA: 1s - loss: 0.2523 - acc: 0.271 - 16s 398ms/step - loss: 0.2552 - acc: 0.2770 - val_loss: 1.5262 - val_acc: 0.0919\n",
            "Epoch 171/1000\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.2943 - acc: 0.214 - ETA: 1s - loss: 0.2963 - acc: 0.286 - 18s 462ms/step - loss: 0.2723 - acc: 0.2762 - val_loss: 1.5403 - val_acc: 0.0968\n",
            "Epoch 172/1000\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.2520 - acc: 0.273 - ETA: 1s - loss: 0.2514 - acc: 0.299 - 16s 396ms/step - loss: 0.2518 - acc: 0.2798 - val_loss: 1.5794 - val_acc: 0.1016\n",
            "Epoch 173/1000\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.2232 - acc: 0.300 - ETA: 1s - loss: 0.2352 - acc: 0.284 - 16s 401ms/step - loss: 0.2260 - acc: 0.2851 - val_loss: 1.5975 - val_acc: 0.1000\n",
            "Epoch 174/1000\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.2851 - acc: 0.283 - ETA: 1s - loss: 0.2625 - acc: 0.279 - 16s 391ms/step - loss: 0.2612 - acc: 0.2794 - val_loss: 1.4664 - val_acc: 0.1016\n",
            "Epoch 175/1000\n",
            "40/40 [==============================] - ETA: 2s - loss: 0.2529 - acc: 0.306 - ETA: 1s - loss: 0.2429 - acc: 0.289 - 16s 389ms/step - loss: 0.2311 - acc: 0.2843 - val_loss: 1.5685 - val_acc: 0.0984\n",
            "Epoch 176/1000\n",
            "40/40 [==============================] - ETA: 2s - loss: 0.2432 - acc: 0.303 - ETA: 1s - loss: 0.2318 - acc: 0.287 - 16s 406ms/step - loss: 0.2267 - acc: 0.2907 - val_loss: 1.5973 - val_acc: 0.0968\n",
            "Epoch 177/1000\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.2194 - acc: 0.201 - ETA: 1s - loss: 0.2225 - acc: 0.277 - 18s 459ms/step - loss: 0.2252 - acc: 0.2839 - val_loss: 1.6434 - val_acc: 0.0952\n",
            "Epoch 178/1000\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.2193 - acc: 0.272 - ETA: 1s - loss: 0.2204 - acc: 0.290 - 17s 413ms/step - loss: 0.2294 - acc: 0.2810 - val_loss: 1.6435 - val_acc: 0.0952\n",
            "Epoch 179/1000\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.2584 - acc: 0.316 - ETA: 1s - loss: 0.2472 - acc: 0.277 - 18s 440ms/step - loss: 0.2444 - acc: 0.2835 - val_loss: 1.6804 - val_acc: 0.0919\n",
            "Epoch 180/1000\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.2612 - acc: 0.319 - ETA: 1s - loss: 0.2545 - acc: 0.278 - 16s 408ms/step - loss: 0.2550 - acc: 0.2758 - val_loss: 1.6742 - val_acc: 0.1000\n",
            "Epoch 181/1000\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.2542 - acc: 0.372 - ETA: 1s - loss: 0.2274 - acc: 0.283 - 16s 400ms/step - loss: 0.2270 - acc: 0.2883 - val_loss: 1.6467 - val_acc: 0.1032\n",
            "Epoch 182/1000\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.2108 - acc: 0.281 - ETA: 1s - loss: 0.2334 - acc: 0.260 - 16s 398ms/step - loss: 0.2359 - acc: 0.2839 - val_loss: 1.5901 - val_acc: 0.0984\n",
            "Epoch 183/1000\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.2447 - acc: 0.268 - ETA: 1s - loss: 0.2440 - acc: 0.278 - 16s 408ms/step - loss: 0.2415 - acc: 0.2806 - val_loss: 1.6338 - val_acc: 0.0968\n",
            "Epoch 184/1000\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.2151 - acc: 0.259 - ETA: 1s - loss: 0.2182 - acc: 0.268 - 16s 395ms/step - loss: 0.2195 - acc: 0.2899 - val_loss: 1.6567 - val_acc: 0.1016\n",
            "Epoch 185/1000\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.2152 - acc: 0.262 - ETA: 1s - loss: 0.2197 - acc: 0.280 - 16s 405ms/step - loss: 0.2198 - acc: 0.2859 - val_loss: 1.6085 - val_acc: 0.0984\n",
            "Epoch 186/1000\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.2307 - acc: 0.253 - ETA: 1s - loss: 0.2591 - acc: 0.283 - 15s 387ms/step - loss: 0.2547 - acc: 0.2835 - val_loss: 1.5517 - val_acc: 0.0984\n",
            "Epoch 187/1000\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.2315 - acc: 0.237 - ETA: 1s - loss: 0.2514 - acc: 0.274 - 16s 403ms/step - loss: 0.2537 - acc: 0.2794 - val_loss: 1.4847 - val_acc: 0.0968\n",
            "Epoch 188/1000\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.2248 - acc: 0.311 - ETA: 1s - loss: 0.2146 - acc: 0.293 - 19s 475ms/step - loss: 0.2170 - acc: 0.2931 - val_loss: 1.5443 - val_acc: 0.0984\n",
            "Epoch 189/1000\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.1998 - acc: 0.270 - ETA: 1s - loss: 0.2021 - acc: 0.260 - 19s 477ms/step - loss: 0.2164 - acc: 0.2899 - val_loss: 1.5953 - val_acc: 0.1048\n",
            "Epoch 190/1000\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.2176 - acc: 0.289 - ETA: 1s - loss: 0.2204 - acc: 0.294 - 18s 439ms/step - loss: 0.2197 - acc: 0.2903 - val_loss: 1.5729 - val_acc: 0.0968\n",
            "Epoch 191/1000\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.2372 - acc: 0.345 - ETA: 1s - loss: 0.2203 - acc: 0.296 - 18s 450ms/step - loss: 0.2168 - acc: 0.2907 - val_loss: 1.5864 - val_acc: 0.1000\n",
            "Epoch 192/1000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "40/40 [==============================] - ETA: 3s - loss: 0.1970 - acc: 0.279 - ETA: 1s - loss: 0.2101 - acc: 0.297 - 18s 438ms/step - loss: 0.2100 - acc: 0.2907 - val_loss: 1.5525 - val_acc: 0.1032\n",
            "Epoch 193/1000\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.2387 - acc: 0.283 - ETA: 1s - loss: 0.2194 - acc: 0.298 - 18s 450ms/step - loss: 0.2206 - acc: 0.2899 - val_loss: 1.7148 - val_acc: 0.0871\n",
            "Epoch 194/1000\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.2042 - acc: 0.290 - ETA: 1s - loss: 0.2154 - acc: 0.290 - 18s 454ms/step - loss: 0.2184 - acc: 0.2919 - val_loss: 1.6146 - val_acc: 0.0968\n",
            "Epoch 195/1000\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.1960 - acc: 0.263 - ETA: 1s - loss: 0.1960 - acc: 0.270 - 18s 457ms/step - loss: 0.1959 - acc: 0.2992 - val_loss: 1.6598 - val_acc: 0.0968\n",
            "Epoch 196/1000\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.1984 - acc: 0.217 - ETA: 1s - loss: 0.1994 - acc: 0.248 - 18s 440ms/step - loss: 0.2195 - acc: 0.2863 - val_loss: 1.6717 - val_acc: 0.1000\n",
            "Epoch 197/1000\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.2365 - acc: 0.274 - ETA: 1s - loss: 0.2240 - acc: 0.296 - 18s 444ms/step - loss: 0.2165 - acc: 0.2851 - val_loss: 1.6338 - val_acc: 0.1032\n",
            "Epoch 198/1000\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.2116 - acc: 0.282 - ETA: 1s - loss: 0.2071 - acc: 0.294 - 17s 434ms/step - loss: 0.2067 - acc: 0.2923 - val_loss: 1.6840 - val_acc: 0.0839\n",
            "Epoch 199/1000\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.2032 - acc: 0.273 - ETA: 1s - loss: 0.2056 - acc: 0.308 - 18s 444ms/step - loss: 0.2046 - acc: 0.2956 - val_loss: 1.6828 - val_acc: 0.0984\n",
            "Epoch 200/1000\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.2004 - acc: 0.258 - ETA: 1s - loss: 0.2008 - acc: 0.294 - 18s 442ms/step - loss: 0.2078 - acc: 0.2976 - val_loss: 1.6318 - val_acc: 0.0984\n",
            "Epoch 201/1000\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.1929 - acc: 0.305 - ETA: 1s - loss: 0.1972 - acc: 0.314 - 18s 447ms/step - loss: 0.2002 - acc: 0.2992 - val_loss: 1.6811 - val_acc: 0.0919\n",
            "Epoch 202/1000\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.1991 - acc: 0.240 - ETA: 1s - loss: 0.2023 - acc: 0.295 - 18s 445ms/step - loss: 0.2041 - acc: 0.2964 - val_loss: 1.6587 - val_acc: 0.0903\n",
            "Epoch 203/1000\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.2201 - acc: 0.292 - ETA: 1s - loss: 0.2197 - acc: 0.275 - 18s 445ms/step - loss: 0.2170 - acc: 0.2891 - val_loss: 1.6249 - val_acc: 0.0919\n",
            "Epoch 204/1000\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.2026 - acc: 0.277 - ETA: 1s - loss: 0.2055 - acc: 0.291 - 18s 458ms/step - loss: 0.2134 - acc: 0.2923 - val_loss: 1.5762 - val_acc: 0.1000\n",
            "Epoch 205/1000\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.1835 - acc: 0.334 - ETA: 1s - loss: 0.1933 - acc: 0.320 - 18s 442ms/step - loss: 0.1901 - acc: 0.2976 - val_loss: 1.6438 - val_acc: 0.1032\n",
            "Epoch 206/1000\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.2014 - acc: 0.329 - ETA: 1s - loss: 0.1972 - acc: 0.312 - 16s 407ms/step - loss: 0.1925 - acc: 0.2988 - val_loss: 1.6104 - val_acc: 0.0968\n",
            "Epoch 207/1000\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.2055 - acc: 0.294 - ETA: 1s - loss: 0.2037 - acc: 0.281 - 15s 383ms/step - loss: 0.1984 - acc: 0.2944 - val_loss: 1.6024 - val_acc: 0.0919\n",
            "Epoch 208/1000\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.1750 - acc: 0.270 - ETA: 0s - loss: 0.1906 - acc: 0.294 - 15s 387ms/step - loss: 0.1895 - acc: 0.3020 - val_loss: 1.6736 - val_acc: 0.1000\n",
            "Epoch 209/1000\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.1982 - acc: 0.318 - ETA: 1s - loss: 0.1942 - acc: 0.302 - 16s 393ms/step - loss: 0.1976 - acc: 0.2996 - val_loss: 1.6655 - val_acc: 0.0855\n",
            "Epoch 210/1000\n",
            "40/40 [==============================] - ETA: 2s - loss: 0.1955 - acc: 0.281 - ETA: 1s - loss: 0.2049 - acc: 0.310 - 16s 392ms/step - loss: 0.1968 - acc: 0.3000 - val_loss: 1.7444 - val_acc: 0.0919\n",
            "Epoch 211/1000\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.2008 - acc: 0.266 - ETA: 1s - loss: 0.1897 - acc: 0.277 - 16s 390ms/step - loss: 0.1987 - acc: 0.2972 - val_loss: 1.6977 - val_acc: 0.0871\n",
            "Epoch 212/1000\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.2033 - acc: 0.271 - ETA: 1s - loss: 0.2058 - acc: 0.280 - 15s 384ms/step - loss: 0.2173 - acc: 0.2891 - val_loss: 1.6237 - val_acc: 0.0952\n",
            "Epoch 213/1000\n",
            "40/40 [==============================] - ETA: 2s - loss: 0.2272 - acc: 0.332 - ETA: 1s - loss: 0.2127 - acc: 0.295 - 16s 410ms/step - loss: 0.2126 - acc: 0.2948 - val_loss: 1.9055 - val_acc: 0.0758\n",
            "Epoch 214/1000\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.2419 - acc: 0.313 - ETA: 1s - loss: 0.2236 - acc: 0.291 - 16s 392ms/step - loss: 0.2271 - acc: 0.2855 - val_loss: 1.6847 - val_acc: 0.1016\n",
            "Epoch 215/1000\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.1897 - acc: 0.293 - ETA: 1s - loss: 0.1911 - acc: 0.299 - 16s 391ms/step - loss: 0.1950 - acc: 0.3004 - val_loss: 1.6491 - val_acc: 0.0968\n",
            "Epoch 216/1000\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.1914 - acc: 0.277 - ETA: 1s - loss: 0.1804 - acc: 0.320 - 17s 414ms/step - loss: 0.1840 - acc: 0.3036 - val_loss: 1.6312 - val_acc: 0.0919\n",
            "Epoch 217/1000\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.1926 - acc: 0.341 - ETA: 1s - loss: 0.1882 - acc: 0.316 - 16s 406ms/step - loss: 0.1874 - acc: 0.3008 - val_loss: 1.6413 - val_acc: 0.0903\n",
            "Epoch 218/1000\n",
            "40/40 [==============================] - ETA: 2s - loss: 0.1857 - acc: 0.280 - ETA: 1s - loss: 0.1858 - acc: 0.294 - 15s 378ms/step - loss: 0.1869 - acc: 0.3024 - val_loss: 1.6479 - val_acc: 0.0839\n",
            "Epoch 219/1000\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.2350 - acc: 0.328 - ETA: 1s - loss: 0.2166 - acc: 0.309 - 15s 380ms/step - loss: 0.2130 - acc: 0.2927 - val_loss: 1.6678 - val_acc: 0.0968\n",
            "Epoch 220/1000\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.2060 - acc: 0.353 - ETA: 1s - loss: 0.1951 - acc: 0.300 - 15s 387ms/step - loss: 0.1926 - acc: 0.3000 - val_loss: 1.6137 - val_acc: 0.1000\n",
            "Epoch 221/1000\n",
            "40/40 [==============================] - ETA: 2s - loss: 0.1697 - acc: 0.290 - ETA: 0s - loss: 0.1766 - acc: 0.300 - 16s 388ms/step - loss: 0.1829 - acc: 0.3060 - val_loss: 1.7047 - val_acc: 0.0968\n",
            "Epoch 222/1000\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.2082 - acc: 0.342 - ETA: 1s - loss: 0.1897 - acc: 0.301 - 16s 392ms/step - loss: 0.1935 - acc: 0.3000 - val_loss: 1.6741 - val_acc: 0.0952\n",
            "Epoch 223/1000\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.2181 - acc: 0.303 - ETA: 1s - loss: 0.1931 - acc: 0.295 - 16s 402ms/step - loss: 0.1870 - acc: 0.3048 - val_loss: 1.6729 - val_acc: 0.0935\n",
            "Epoch 224/1000\n",
            "40/40 [==============================] - ETA: 2s - loss: 0.1782 - acc: 0.264 - ETA: 0s - loss: 0.1837 - acc: 0.299 - 15s 382ms/step - loss: 0.1918 - acc: 0.3008 - val_loss: 1.7058 - val_acc: 0.0871\n",
            "Epoch 225/1000\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.2162 - acc: 0.333 - ETA: 1s - loss: 0.1934 - acc: 0.308 - 15s 378ms/step - loss: 0.1865 - acc: 0.3020 - val_loss: 1.7192 - val_acc: 0.0919\n",
            "Epoch 226/1000\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.2149 - acc: 0.340 - ETA: 1s - loss: 0.1911 - acc: 0.313 - 15s 382ms/step - loss: 0.1892 - acc: 0.3000 - val_loss: 1.7723 - val_acc: 0.0871\n",
            "Epoch 227/1000\n",
            "40/40 [==============================] - ETA: 2s - loss: 0.1935 - acc: 0.238 - ETA: 0s - loss: 0.1970 - acc: 0.296 - 15s 387ms/step - loss: 0.2013 - acc: 0.2980 - val_loss: 1.6243 - val_acc: 0.0903\n",
            "Epoch 228/1000\n",
            "40/40 [==============================] - ETA: 2s - loss: 0.1777 - acc: 0.330 - ETA: 0s - loss: 0.1709 - acc: 0.305 - 15s 387ms/step - loss: 0.1729 - acc: 0.3073 - val_loss: 1.6805 - val_acc: 0.0952\n",
            "Epoch 229/1000\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.2014 - acc: 0.279 - ETA: 1s - loss: 0.1927 - acc: 0.320 - 15s 382ms/step - loss: 0.1825 - acc: 0.3020 - val_loss: 1.6439 - val_acc: 0.0935\n",
            "Epoch 230/1000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "40/40 [==============================] - ETA: 3s - loss: 0.1679 - acc: 0.349 - ETA: 1s - loss: 0.1661 - acc: 0.301 - 15s 385ms/step - loss: 0.1737 - acc: 0.3000 - val_loss: 1.6306 - val_acc: 0.0871\n",
            "Epoch 231/1000\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.1816 - acc: 0.283 - ETA: 1s - loss: 0.1946 - acc: 0.313 - 15s 378ms/step - loss: 0.1870 - acc: 0.3024 - val_loss: 1.6215 - val_acc: 0.0952\n",
            "Epoch 232/1000\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.1685 - acc: 0.295 - ETA: 1s - loss: 0.1765 - acc: 0.298 - 15s 387ms/step - loss: 0.1818 - acc: 0.3008 - val_loss: 1.7276 - val_acc: 0.0790\n",
            "Epoch 233/1000\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.1903 - acc: 0.313 - ETA: 0s - loss: 0.1849 - acc: 0.310 - 15s 378ms/step - loss: 0.1824 - acc: 0.3024 - val_loss: 1.6347 - val_acc: 0.0984\n",
            "Epoch 234/1000\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.1790 - acc: 0.294 - ETA: 1s - loss: 0.1812 - acc: 0.302 - 15s 385ms/step - loss: 0.1829 - acc: 0.3020 - val_loss: 1.6885 - val_acc: 0.0903\n",
            "Epoch 235/1000\n",
            "40/40 [==============================] - ETA: 2s - loss: 0.1586 - acc: 0.240 - ETA: 1s - loss: 0.1662 - acc: 0.319 - 16s 391ms/step - loss: 0.1754 - acc: 0.3048 - val_loss: 1.6793 - val_acc: 0.0968\n",
            "Epoch 236/1000\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.1592 - acc: 0.267 - ETA: 1s - loss: 0.1694 - acc: 0.296 - 16s 391ms/step - loss: 0.1711 - acc: 0.3048 - val_loss: 1.6418 - val_acc: 0.0903\n",
            "Epoch 237/1000\n",
            "40/40 [==============================] - ETA: 2s - loss: 0.1811 - acc: 0.324 - ETA: 0s - loss: 0.1907 - acc: 0.319 - 16s 396ms/step - loss: 0.1877 - acc: 0.3048 - val_loss: 1.7547 - val_acc: 0.0871\n",
            "Epoch 238/1000\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.2047 - acc: 0.389 - ETA: 1s - loss: 0.1936 - acc: 0.305 - 15s 384ms/step - loss: 0.1848 - acc: 0.3020 - val_loss: 1.6502 - val_acc: 0.0903\n",
            "Epoch 239/1000\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.1777 - acc: 0.353 - ETA: 1s - loss: 0.1815 - acc: 0.331 - 16s 393ms/step - loss: 0.1705 - acc: 0.3028 - val_loss: 1.6864 - val_acc: 0.0984\n",
            "Epoch 240/1000\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.1859 - acc: 0.317 - ETA: 1s - loss: 0.1686 - acc: 0.318 - 16s 402ms/step - loss: 0.1747 - acc: 0.3048 - val_loss: 1.7166 - val_acc: 0.0935\n",
            "Epoch 241/1000\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.1833 - acc: 0.274 - ETA: 1s - loss: 0.1825 - acc: 0.297 - 16s 391ms/step - loss: 0.1843 - acc: 0.3016 - val_loss: 1.6746 - val_acc: 0.0968\n",
            "Epoch 242/1000\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.1815 - acc: 0.290 - ETA: 1s - loss: 0.1780 - acc: 0.339 - 16s 389ms/step - loss: 0.1755 - acc: 0.3060 - val_loss: 1.6599 - val_acc: 0.0952\n",
            "Epoch 243/1000\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.1782 - acc: 0.311 - ETA: 1s - loss: 0.1713 - acc: 0.291 - 16s 389ms/step - loss: 0.1708 - acc: 0.3101 - val_loss: 1.7084 - val_acc: 0.0919\n",
            "Epoch 244/1000\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.1776 - acc: 0.326 - ETA: 1s - loss: 0.1731 - acc: 0.313 - 16s 388ms/step - loss: 0.1732 - acc: 0.3020 - val_loss: 1.7123 - val_acc: 0.0935\n",
            "Epoch 245/1000\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.2208 - acc: 0.274 - ETA: 0s - loss: 0.1964 - acc: 0.316 - 16s 388ms/step - loss: 0.1875 - acc: 0.3020 - val_loss: 1.6822 - val_acc: 0.0855\n",
            "Epoch 246/1000\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.1890 - acc: 0.293 - ETA: 1s - loss: 0.1862 - acc: 0.304 - 16s 391ms/step - loss: 0.1786 - acc: 0.3040 - val_loss: 1.7582 - val_acc: 0.1016\n",
            "Epoch 247/1000\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.1795 - acc: 0.268 - ETA: 1s - loss: 0.1789 - acc: 0.283 - 16s 390ms/step - loss: 0.1803 - acc: 0.3020 - val_loss: 1.6589 - val_acc: 0.0935\n",
            "Epoch 248/1000\n",
            "40/40 [==============================] - ETA: 2s - loss: 0.1647 - acc: 0.302 - ETA: 0s - loss: 0.1700 - acc: 0.309 - 16s 389ms/step - loss: 0.1705 - acc: 0.3073 - val_loss: 1.7075 - val_acc: 0.0935\n",
            "Epoch 249/1000\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.1790 - acc: 0.288 - ETA: 1s - loss: 0.1785 - acc: 0.289 - 18s 438ms/step - loss: 0.1806 - acc: 0.3008 - val_loss: 1.6946 - val_acc: 0.0935\n",
            "Epoch 250/1000\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.1519 - acc: 0.230 - ETA: 1s - loss: 0.1851 - acc: 0.293 - 16s 403ms/step - loss: 0.1823 - acc: 0.3040 - val_loss: 1.7093 - val_acc: 0.0935\n",
            "Epoch 251/1000\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.1552 - acc: 0.291 - ETA: 1s - loss: 0.1704 - acc: 0.279 - 15s 382ms/step - loss: 0.1750 - acc: 0.3060 - val_loss: 1.6810 - val_acc: 0.1032\n",
            "Epoch 252/1000\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.1927 - acc: 0.338 - ETA: 1s - loss: 0.1835 - acc: 0.299 - 16s 393ms/step - loss: 0.1814 - acc: 0.3016 - val_loss: 1.7780 - val_acc: 0.1000\n",
            "Epoch 253/1000\n",
            "40/40 [==============================] - ETA: 2s - loss: 0.1790 - acc: 0.281 - ETA: 1s - loss: 0.1742 - acc: 0.328 - 15s 382ms/step - loss: 0.1685 - acc: 0.3065 - val_loss: 1.7214 - val_acc: 0.0968\n",
            "Epoch 254/1000\n",
            "40/40 [==============================] - ETA: 2s - loss: 0.1605 - acc: 0.335 - ETA: 0s - loss: 0.1625 - acc: 0.326 - 15s 383ms/step - loss: 0.1638 - acc: 0.3101 - val_loss: 1.7099 - val_acc: 0.0984\n",
            "Epoch 255/1000\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.1801 - acc: 0.301 - ETA: 1s - loss: 0.1675 - acc: 0.305 - 15s 387ms/step - loss: 0.1621 - acc: 0.3097 - val_loss: 1.7022 - val_acc: 0.0935\n",
            "Epoch 256/1000\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.1554 - acc: 0.323 - ETA: 1s - loss: 0.1655 - acc: 0.313 - 15s 386ms/step - loss: 0.1629 - acc: 0.3081 - val_loss: 1.6848 - val_acc: 0.1000\n",
            "Epoch 257/1000\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.1525 - acc: 0.290 - ETA: 1s - loss: 0.1717 - acc: 0.323 - 15s 382ms/step - loss: 0.1673 - acc: 0.3109 - val_loss: 1.6862 - val_acc: 0.0952\n",
            "Epoch 258/1000\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.1611 - acc: 0.366 - ETA: 1s - loss: 0.1614 - acc: 0.320 - 16s 394ms/step - loss: 0.1666 - acc: 0.3069 - val_loss: 1.7675 - val_acc: 0.0903\n",
            "Epoch 259/1000\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.1730 - acc: 0.259 - ETA: 0s - loss: 0.1603 - acc: 0.269 - 16s 390ms/step - loss: 0.1693 - acc: 0.3024 - val_loss: 1.6821 - val_acc: 0.1097\n",
            "Epoch 260/1000\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.1858 - acc: 0.320 - ETA: 1s - loss: 0.1745 - acc: 0.327 - 16s 390ms/step - loss: 0.1636 - acc: 0.3085 - val_loss: 1.6388 - val_acc: 0.1000\n",
            "Epoch 261/1000\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.1489 - acc: 0.285 - ETA: 1s - loss: 0.1536 - acc: 0.297 - 15s 384ms/step - loss: 0.1561 - acc: 0.3101 - val_loss: 1.7543 - val_acc: 0.0984\n",
            "Epoch 262/1000\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.1553 - acc: 0.326 - ETA: 1s - loss: 0.1520 - acc: 0.312 - 17s 421ms/step - loss: 0.1546 - acc: 0.3113 - val_loss: 1.7666 - val_acc: 0.1016\n",
            "Epoch 263/1000\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.1583 - acc: 0.297 - ETA: 1s - loss: 0.1673 - acc: 0.327 - 18s 455ms/step - loss: 0.1628 - acc: 0.3113 - val_loss: 1.8060 - val_acc: 0.0968\n",
            "Epoch 264/1000\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.1385 - acc: 0.307 - ETA: 1s - loss: 0.1592 - acc: 0.325 - 18s 457ms/step - loss: 0.1600 - acc: 0.3085 - val_loss: 1.7027 - val_acc: 0.0919\n",
            "Epoch 265/1000\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.1502 - acc: 0.245 - ETA: 1s - loss: 0.1630 - acc: 0.309 - 16s 396ms/step - loss: 0.1603 - acc: 0.3056 - val_loss: 1.7531 - val_acc: 0.0968\n",
            "Epoch 266/1000\n",
            "40/40 [==============================] - ETA: 2s - loss: 0.1316 - acc: 0.296 - ETA: 1s - loss: 0.1471 - acc: 0.311 - 16s 402ms/step - loss: 0.1456 - acc: 0.3129 - val_loss: 1.7216 - val_acc: 0.0968\n",
            "Epoch 267/1000\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.1430 - acc: 0.280 - ETA: 1s - loss: 0.1495 - acc: 0.304 - 16s 391ms/step - loss: 0.1517 - acc: 0.3085 - val_loss: 1.7016 - val_acc: 0.1000\n",
            "Epoch 268/1000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "40/40 [==============================] - ETA: 3s - loss: 0.1879 - acc: 0.318 - ETA: 0s - loss: 0.1616 - acc: 0.328 - 15s 385ms/step - loss: 0.1577 - acc: 0.3069 - val_loss: 1.8101 - val_acc: 0.0952\n",
            "Epoch 269/1000\n",
            "40/40 [==============================] - ETA: 2s - loss: 0.1503 - acc: 0.324 - ETA: 0s - loss: 0.1598 - acc: 0.322 - 16s 389ms/step - loss: 0.1600 - acc: 0.3097 - val_loss: 1.7352 - val_acc: 0.1000\n",
            "Epoch 270/1000\n",
            "40/40 [==============================] - ETA: 2s - loss: 0.1724 - acc: 0.324 - ETA: 0s - loss: 0.1604 - acc: 0.343 - 15s 384ms/step - loss: 0.1561 - acc: 0.3101 - val_loss: 1.7610 - val_acc: 0.0919\n",
            "Epoch 271/1000\n",
            "40/40 [==============================] - ETA: 2s - loss: 0.1549 - acc: 0.254 - ETA: 1s - loss: 0.1602 - acc: 0.290 - 16s 389ms/step - loss: 0.1655 - acc: 0.3065 - val_loss: 1.7303 - val_acc: 0.1000\n",
            "Epoch 272/1000\n",
            "40/40 [==============================] - ETA: 2s - loss: 0.1812 - acc: 0.360 - ETA: 0s - loss: 0.1698 - acc: 0.334 - 15s 384ms/step - loss: 0.1619 - acc: 0.3125 - val_loss: 1.7309 - val_acc: 0.0968\n",
            "Epoch 273/1000\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.1611 - acc: 0.298 - ETA: 1s - loss: 0.1611 - acc: 0.315 - 16s 394ms/step - loss: 0.1577 - acc: 0.3081 - val_loss: 1.7934 - val_acc: 0.0952\n",
            "Epoch 274/1000\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.1632 - acc: 0.266 - ETA: 1s - loss: 0.1584 - acc: 0.296 - 16s 409ms/step - loss: 0.1545 - acc: 0.3085 - val_loss: 1.7556 - val_acc: 0.0968\n",
            "Epoch 275/1000\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.1474 - acc: 0.252 - ETA: 1s - loss: 0.1425 - acc: 0.303 - 16s 390ms/step - loss: 0.1493 - acc: 0.3194 - val_loss: 1.7227 - val_acc: 0.0968\n",
            "Epoch 276/1000\n",
            "40/40 [==============================] - ETA: 2s - loss: 0.1555 - acc: 0.351 - ETA: 1s - loss: 0.1572 - acc: 0.313 - 15s 386ms/step - loss: 0.1654 - acc: 0.3085 - val_loss: 1.7339 - val_acc: 0.1065\n",
            "Epoch 277/1000\n",
            "40/40 [==============================] - ETA: 2s - loss: 0.1547 - acc: 0.272 - ETA: 1s - loss: 0.1496 - acc: 0.320 - 16s 389ms/step - loss: 0.1526 - acc: 0.3117 - val_loss: 1.8577 - val_acc: 0.0806\n",
            "Epoch 278/1000\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.1440 - acc: 0.261 - ETA: 1s - loss: 0.1793 - acc: 0.300 - 15s 382ms/step - loss: 0.1732 - acc: 0.3044 - val_loss: 1.6768 - val_acc: 0.0903\n",
            "Epoch 279/1000\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.1603 - acc: 0.286 - ETA: 1s - loss: 0.1545 - acc: 0.320 - 16s 391ms/step - loss: 0.1537 - acc: 0.3109 - val_loss: 1.6443 - val_acc: 0.1000\n",
            "Epoch 280/1000\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.1400 - acc: 0.300 - ETA: 1s - loss: 0.1595 - acc: 0.304 - 16s 399ms/step - loss: 0.1603 - acc: 0.3069 - val_loss: 1.6955 - val_acc: 0.0839\n",
            "Epoch 281/1000\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.1463 - acc: 0.366 - ETA: 1s - loss: 0.1521 - acc: 0.322 - 16s 388ms/step - loss: 0.1528 - acc: 0.3109 - val_loss: 1.7078 - val_acc: 0.0984\n",
            "Epoch 282/1000\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.1369 - acc: 0.303 - ETA: 1s - loss: 0.1485 - acc: 0.325 - 16s 402ms/step - loss: 0.1407 - acc: 0.3169 - val_loss: 1.7620 - val_acc: 0.0903\n",
            "Epoch 283/1000\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.1524 - acc: 0.312 - ETA: 1s - loss: 0.1458 - acc: 0.310 - 16s 402ms/step - loss: 0.1504 - acc: 0.3149 - val_loss: 1.7555 - val_acc: 0.0871\n",
            "Epoch 284/1000\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.1678 - acc: 0.290 - ETA: 1s - loss: 0.1586 - acc: 0.306 - 15s 385ms/step - loss: 0.1582 - acc: 0.3089 - val_loss: 1.7485 - val_acc: 0.0903\n",
            "Epoch 285/1000\n",
            "40/40 [==============================] - ETA: 2s - loss: 0.1399 - acc: 0.288 - ETA: 1s - loss: 0.1518 - acc: 0.309 - 16s 390ms/step - loss: 0.1524 - acc: 0.3073 - val_loss: 1.7463 - val_acc: 0.0855\n",
            "Epoch 286/1000\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.1541 - acc: 0.293 - ETA: 1s - loss: 0.1502 - acc: 0.309 - 15s 380ms/step - loss: 0.1495 - acc: 0.3109 - val_loss: 1.7125 - val_acc: 0.0903\n",
            "Epoch 287/1000\n",
            "40/40 [==============================] - ETA: 2s - loss: 0.1537 - acc: 0.266 - ETA: 0s - loss: 0.1533 - acc: 0.315 - 15s 385ms/step - loss: 0.1586 - acc: 0.3113 - val_loss: 1.7327 - val_acc: 0.0887\n",
            "Epoch 288/1000\n",
            "40/40 [==============================] - ETA: 2s - loss: 0.1567 - acc: 0.249 - ETA: 1s - loss: 0.1539 - acc: 0.296 - 16s 389ms/step - loss: 0.1517 - acc: 0.3141 - val_loss: 1.7061 - val_acc: 0.0968\n",
            "Epoch 289/1000\n",
            "40/40 [==============================] - ETA: 2s - loss: 0.1510 - acc: 0.304 - ETA: 0s - loss: 0.1382 - acc: 0.287 - 16s 390ms/step - loss: 0.1407 - acc: 0.3133 - val_loss: 1.8221 - val_acc: 0.0919\n",
            "Epoch 290/1000\n",
            "40/40 [==============================] - ETA: 2s - loss: 0.1410 - acc: 0.312 - ETA: 0s - loss: 0.1476 - acc: 0.302 - 15s 382ms/step - loss: 0.1478 - acc: 0.3121 - val_loss: 1.9119 - val_acc: 0.0790\n",
            "Epoch 291/1000\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.1376 - acc: 0.269 - ETA: 0s - loss: 0.1400 - acc: 0.314 - 15s 382ms/step - loss: 0.1441 - acc: 0.3133 - val_loss: 1.8575 - val_acc: 0.0871\n",
            "Epoch 292/1000\n",
            "40/40 [==============================] - ETA: 2s - loss: 0.1352 - acc: 0.348 - ETA: 0s - loss: 0.1375 - acc: 0.313 - 15s 379ms/step - loss: 0.1418 - acc: 0.3113 - val_loss: 1.8041 - val_acc: 0.0935\n",
            "Epoch 293/1000\n",
            "40/40 [==============================] - ETA: 2s - loss: 0.1773 - acc: 0.306 - ETA: 1s - loss: 0.1574 - acc: 0.306 - 16s 399ms/step - loss: 0.1581 - acc: 0.3101 - val_loss: 1.8390 - val_acc: 0.0887\n",
            "Epoch 294/1000\n",
            "40/40 [==============================] - ETA: 2s - loss: 0.1530 - acc: 0.329 - ETA: 0s - loss: 0.1459 - acc: 0.323 - 16s 397ms/step - loss: 0.1420 - acc: 0.3149 - val_loss: 1.8356 - val_acc: 0.1000\n",
            "Epoch 295/1000\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.1194 - acc: 0.263 - ETA: 1s - loss: 0.1312 - acc: 0.306 - 15s 383ms/step - loss: 0.1354 - acc: 0.3202 - val_loss: 1.8504 - val_acc: 0.0952\n",
            "Epoch 296/1000\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.1291 - acc: 0.294 - ETA: 1s - loss: 0.1364 - acc: 0.289 - 16s 392ms/step - loss: 0.1448 - acc: 0.3125 - val_loss: 1.8510 - val_acc: 0.0919\n",
            "Epoch 297/1000\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.1454 - acc: 0.382 - ETA: 1s - loss: 0.1442 - acc: 0.320 - 15s 382ms/step - loss: 0.1454 - acc: 0.3137 - val_loss: 1.8653 - val_acc: 0.0903\n",
            "Epoch 298/1000\n",
            "40/40 [==============================] - ETA: 2s - loss: 0.1473 - acc: 0.308 - ETA: 0s - loss: 0.1534 - acc: 0.320 - 16s 392ms/step - loss: 0.1499 - acc: 0.3137 - val_loss: 1.8610 - val_acc: 0.0952\n",
            "Epoch 299/1000\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.1238 - acc: 0.347 - ETA: 1s - loss: 0.1394 - acc: 0.311 - 16s 398ms/step - loss: 0.1435 - acc: 0.3109 - val_loss: 1.9243 - val_acc: 0.0968\n",
            "Epoch 300/1000\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.1409 - acc: 0.351 - ETA: 1s - loss: 0.1489 - acc: 0.350 - 16s 389ms/step - loss: 0.1420 - acc: 0.3173 - val_loss: 1.9220 - val_acc: 0.0984\n",
            "Epoch 301/1000\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.1314 - acc: 0.279 - ETA: 1s - loss: 0.1439 - acc: 0.305 - 15s 382ms/step - loss: 0.1488 - acc: 0.3149 - val_loss: 1.9288 - val_acc: 0.0903\n",
            "Epoch 302/1000\n",
            "40/40 [==============================] - ETA: 2s - loss: 0.1501 - acc: 0.273 - ETA: 1s - loss: 0.1465 - acc: 0.297 - 15s 382ms/step - loss: 0.1467 - acc: 0.3161 - val_loss: 1.9636 - val_acc: 0.0871\n",
            "Epoch 303/1000\n",
            "40/40 [==============================] - ETA: 2s - loss: 0.1480 - acc: 0.342 - ETA: 1s - loss: 0.1453 - acc: 0.280 - 16s 406ms/step - loss: 0.1458 - acc: 0.3109 - val_loss: 1.8349 - val_acc: 0.0919\n",
            "Epoch 304/1000\n",
            "40/40 [==============================] - ETA: 2s - loss: 0.1555 - acc: 0.355 - ETA: 1s - loss: 0.1502 - acc: 0.311 - 15s 381ms/step - loss: 0.1495 - acc: 0.3141 - val_loss: 1.9146 - val_acc: 0.0903\n",
            "Epoch 305/1000\n",
            "40/40 [==============================] - ETA: 2s - loss: 0.1360 - acc: 0.315 - ETA: 1s - loss: 0.1291 - acc: 0.309 - 15s 380ms/step - loss: 0.1322 - acc: 0.3194 - val_loss: 1.8552 - val_acc: 0.0968\n",
            "Epoch 306/1000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "40/40 [==============================] - ETA: 2s - loss: 0.1350 - acc: 0.276 - ETA: 1s - loss: 0.1411 - acc: 0.292 - 16s 391ms/step - loss: 0.1448 - acc: 0.3121 - val_loss: 1.7859 - val_acc: 0.0935\n",
            "Epoch 307/1000\n",
            "40/40 [==============================] - ETA: 2s - loss: 0.1511 - acc: 0.346 - ETA: 0s - loss: 0.1440 - acc: 0.343 - 16s 393ms/step - loss: 0.1411 - acc: 0.3157 - val_loss: 1.8183 - val_acc: 0.0919\n",
            "Epoch 308/1000\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.1433 - acc: 0.294 - ETA: 1s - loss: 0.1512 - acc: 0.347 - 15s 384ms/step - loss: 0.1442 - acc: 0.3121 - val_loss: 1.8648 - val_acc: 0.0935\n",
            "Epoch 309/1000\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.1405 - acc: 0.321 - ETA: 1s - loss: 0.1376 - acc: 0.308 - 15s 386ms/step - loss: 0.1371 - acc: 0.3202 - val_loss: 1.8199 - val_acc: 0.0919\n",
            "Epoch 310/1000\n",
            "40/40 [==============================] - ETA: 2s - loss: 0.1319 - acc: 0.278 - ETA: 0s - loss: 0.1348 - acc: 0.297 - 16s 388ms/step - loss: 0.1370 - acc: 0.3157 - val_loss: 1.8612 - val_acc: 0.0887\n",
            "Epoch 311/1000\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.1295 - acc: 0.305 - ETA: 1s - loss: 0.1281 - acc: 0.282 - 15s 381ms/step - loss: 0.1288 - acc: 0.3218 - val_loss: 1.9151 - val_acc: 0.0839\n",
            "Epoch 312/1000\n",
            "40/40 [==============================] - ETA: 2s - loss: 0.1449 - acc: 0.294 - ETA: 1s - loss: 0.1428 - acc: 0.309 - 16s 394ms/step - loss: 0.1447 - acc: 0.3153 - val_loss: 1.8680 - val_acc: 0.0968\n",
            "Epoch 313/1000\n",
            "40/40 [==============================] - ETA: 2s - loss: 0.1412 - acc: 0.345 - ETA: 1s - loss: 0.1266 - acc: 0.358 - 16s 390ms/step - loss: 0.1276 - acc: 0.3214 - val_loss: 1.8272 - val_acc: 0.0855\n",
            "Epoch 314/1000\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.1408 - acc: 0.337 - ETA: 1s - loss: 0.1391 - acc: 0.307 - 15s 384ms/step - loss: 0.1471 - acc: 0.3153 - val_loss: 1.8555 - val_acc: 0.0968\n",
            "Epoch 315/1000\n",
            "40/40 [==============================] - ETA: 2s - loss: 0.1358 - acc: 0.294 - ETA: 0s - loss: 0.1374 - acc: 0.311 - 15s 381ms/step - loss: 0.1345 - acc: 0.3202 - val_loss: 1.8879 - val_acc: 0.0839\n",
            "Epoch 316/1000\n",
            "40/40 [==============================] - ETA: 2s - loss: 0.1283 - acc: 0.289 - ETA: 0s - loss: 0.1407 - acc: 0.307 - 15s 387ms/step - loss: 0.1460 - acc: 0.3145 - val_loss: 1.8425 - val_acc: 0.0968\n",
            "Epoch 317/1000\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.1274 - acc: 0.255 - ETA: 1s - loss: 0.1347 - acc: 0.322 - 16s 393ms/step - loss: 0.1370 - acc: 0.3177 - val_loss: 1.9784 - val_acc: 0.0903\n",
            "Epoch 318/1000\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.1371 - acc: 0.324 - ETA: 1s - loss: 0.1391 - acc: 0.331 - 15s 387ms/step - loss: 0.1337 - acc: 0.3165 - val_loss: 1.8398 - val_acc: 0.0903\n",
            "Epoch 319/1000\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.1376 - acc: 0.292 - ETA: 1s - loss: 0.1402 - acc: 0.297 - 15s 387ms/step - loss: 0.1433 - acc: 0.3177 - val_loss: 1.8668 - val_acc: 0.0935\n",
            "Epoch 320/1000\n",
            "40/40 [==============================] - ETA: 2s - loss: 0.1614 - acc: 0.325 - ETA: 1s - loss: 0.1473 - acc: 0.316 - 16s 394ms/step - loss: 0.1413 - acc: 0.3153 - val_loss: 1.9005 - val_acc: 0.0935\n",
            "Epoch 321/1000\n",
            "40/40 [==============================] - ETA: 2s - loss: 0.1346 - acc: 0.344 - ETA: 0s - loss: 0.1390 - acc: 0.309 - 16s 391ms/step - loss: 0.1423 - acc: 0.3133 - val_loss: 1.8551 - val_acc: 0.0887\n",
            "Epoch 322/1000\n",
            "40/40 [==============================] - ETA: 2s - loss: 0.1375 - acc: 0.266 - ETA: 1s - loss: 0.1436 - acc: 0.303 - 15s 377ms/step - loss: 0.1404 - acc: 0.3165 - val_loss: 1.8951 - val_acc: 0.0935\n",
            "Epoch 323/1000\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.1294 - acc: 0.365 - ETA: 1s - loss: 0.1264 - acc: 0.332 - 15s 384ms/step - loss: 0.1293 - acc: 0.3177 - val_loss: 1.8618 - val_acc: 0.0919\n",
            "Epoch 324/1000\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.1500 - acc: 0.324 - ETA: 1s - loss: 0.1519 - acc: 0.316 - 16s 390ms/step - loss: 0.1518 - acc: 0.3093 - val_loss: 1.8644 - val_acc: 0.0887\n",
            "Epoch 325/1000\n",
            "40/40 [==============================] - ETA: 2s - loss: 0.1486 - acc: 0.371 - ETA: 0s - loss: 0.1320 - acc: 0.330 - 15s 383ms/step - loss: 0.1381 - acc: 0.3149 - val_loss: 1.8871 - val_acc: 0.0968\n",
            "Epoch 326/1000\n",
            "40/40 [==============================] - ETA: 2s - loss: 0.1128 - acc: 0.319 - ETA: 0s - loss: 0.1287 - acc: 0.302 - 15s 385ms/step - loss: 0.1297 - acc: 0.3157 - val_loss: 1.8984 - val_acc: 0.0984\n",
            "Epoch 327/1000\n",
            "40/40 [==============================] - ETA: 2s - loss: 0.1368 - acc: 0.363 - ETA: 0s - loss: 0.1334 - acc: 0.343 - 16s 389ms/step - loss: 0.1326 - acc: 0.3190 - val_loss: 1.9163 - val_acc: 0.0839\n",
            "Epoch 328/1000\n",
            "40/40 [==============================] - ETA: 2s - loss: 0.1269 - acc: 0.336 - ETA: 1s - loss: 0.1248 - acc: 0.321 - 16s 391ms/step - loss: 0.1270 - acc: 0.3214 - val_loss: 1.8433 - val_acc: 0.1000\n",
            "Epoch 329/1000\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.1330 - acc: 0.319 - ETA: 1s - loss: 0.1314 - acc: 0.305 - 16s 399ms/step - loss: 0.1289 - acc: 0.3185 - val_loss: 1.8793 - val_acc: 0.0919\n",
            "Epoch 330/1000\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.1275 - acc: 0.289 - ETA: 1s - loss: 0.1253 - acc: 0.316 - 16s 393ms/step - loss: 0.1267 - acc: 0.3181 - val_loss: 1.8561 - val_acc: 0.1016\n",
            "Epoch 331/1000\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.1240 - acc: 0.306 - ETA: 1s - loss: 0.1249 - acc: 0.315 - 18s 442ms/step - loss: 0.1272 - acc: 0.3210 - val_loss: 1.8162 - val_acc: 0.1016\n",
            "Epoch 332/1000\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.1281 - acc: 0.344 - ETA: 1s - loss: 0.1254 - acc: 0.315 - 18s 443ms/step - loss: 0.1272 - acc: 0.3198 - val_loss: 1.8649 - val_acc: 0.0903\n",
            "Epoch 333/1000\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.1240 - acc: 0.370 - ETA: 1s - loss: 0.1306 - acc: 0.329 - 17s 432ms/step - loss: 0.1277 - acc: 0.3218 - val_loss: 1.8305 - val_acc: 0.0919\n",
            "Epoch 334/1000\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.1186 - acc: 0.300 - ETA: 1s - loss: 0.1256 - acc: 0.329 - 16s 391ms/step - loss: 0.1292 - acc: 0.3214 - val_loss: 1.8427 - val_acc: 0.0952\n",
            "Epoch 335/1000\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.1415 - acc: 0.337 - ETA: 1s - loss: 0.1353 - acc: 0.327 - 15s 383ms/step - loss: 0.1346 - acc: 0.3169 - val_loss: 1.7854 - val_acc: 0.1048\n",
            "Epoch 336/1000\n",
            "40/40 [==============================] - ETA: 2s - loss: 0.1398 - acc: 0.320 - ETA: 0s - loss: 0.1339 - acc: 0.325 - 16s 393ms/step - loss: 0.1362 - acc: 0.3145 - val_loss: 1.8073 - val_acc: 0.0952\n",
            "Epoch 337/1000\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.1253 - acc: 0.299 - ETA: 1s - loss: 0.1305 - acc: 0.331 - 15s 384ms/step - loss: 0.1274 - acc: 0.3218 - val_loss: 1.8652 - val_acc: 0.0952\n",
            "Epoch 338/1000\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.1143 - acc: 0.247 - ETA: 1s - loss: 0.1217 - acc: 0.302 - 16s 388ms/step - loss: 0.1256 - acc: 0.3185 - val_loss: 1.7648 - val_acc: 0.0952\n",
            "Epoch 339/1000\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.1555 - acc: 0.354 - ETA: 1s - loss: 0.1459 - acc: 0.332 - 16s 395ms/step - loss: 0.1474 - acc: 0.3145 - val_loss: 1.7384 - val_acc: 0.0984\n",
            "Epoch 340/1000\n",
            "40/40 [==============================] - ETA: 2s - loss: 0.1452 - acc: 0.322 - ETA: 0s - loss: 0.1413 - acc: 0.332 - 15s 380ms/step - loss: 0.1372 - acc: 0.3149 - val_loss: 1.7958 - val_acc: 0.0887\n",
            "Epoch 341/1000\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.1256 - acc: 0.282 - ETA: 1s - loss: 0.1237 - acc: 0.311 - 16s 396ms/step - loss: 0.1235 - acc: 0.3194 - val_loss: 1.8368 - val_acc: 0.0903\n",
            "Epoch 342/1000\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.1239 - acc: 0.394 - ETA: 1s - loss: 0.1325 - acc: 0.342 - 15s 386ms/step - loss: 0.1339 - acc: 0.3190 - val_loss: 1.7703 - val_acc: 0.1000\n",
            "Epoch 343/1000\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.1392 - acc: 0.317 - ETA: 1s - loss: 0.1342 - acc: 0.315 - 15s 387ms/step - loss: 0.1326 - acc: 0.3165 - val_loss: 1.8011 - val_acc: 0.0968\n",
            "Epoch 344/1000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "40/40 [==============================] - ETA: 2s - loss: 0.1262 - acc: 0.261 - ETA: 0s - loss: 0.1299 - acc: 0.316 - 15s 384ms/step - loss: 0.1268 - acc: 0.3194 - val_loss: 1.7962 - val_acc: 0.1048\n",
            "Epoch 345/1000\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.1266 - acc: 0.372 - ETA: 1s - loss: 0.1212 - acc: 0.336 - 16s 396ms/step - loss: 0.1202 - acc: 0.3238 - val_loss: 1.8860 - val_acc: 0.1016\n",
            "Epoch 346/1000\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.1299 - acc: 0.261 - ETA: 1s - loss: 0.1242 - acc: 0.302 - 16s 398ms/step - loss: 0.1282 - acc: 0.3202 - val_loss: 1.8198 - val_acc: 0.0984\n",
            "Epoch 347/1000\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.1326 - acc: 0.296 - ETA: 1s - loss: 0.1338 - acc: 0.317 - 16s 392ms/step - loss: 0.1308 - acc: 0.3190 - val_loss: 1.8557 - val_acc: 0.0919\n",
            "Epoch 348/1000\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.1369 - acc: 0.355 - ETA: 1s - loss: 0.1307 - acc: 0.344 - 16s 393ms/step - loss: 0.1306 - acc: 0.3173 - val_loss: 1.8876 - val_acc: 0.0952\n",
            "Epoch 349/1000\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.1258 - acc: 0.319 - ETA: 0s - loss: 0.1206 - acc: 0.325 - 15s 383ms/step - loss: 0.1224 - acc: 0.3190 - val_loss: 1.8304 - val_acc: 0.0903\n",
            "Epoch 350/1000\n",
            "40/40 [==============================] - ETA: 2s - loss: 0.1255 - acc: 0.292 - ETA: 1s - loss: 0.1257 - acc: 0.351 - 16s 391ms/step - loss: 0.1239 - acc: 0.3210 - val_loss: 1.8466 - val_acc: 0.0871\n",
            "Epoch 351/1000\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.1056 - acc: 0.278 - ETA: 1s - loss: 0.1191 - acc: 0.292 - 16s 392ms/step - loss: 0.1257 - acc: 0.3181 - val_loss: 1.8775 - val_acc: 0.0903\n",
            "Epoch 352/1000\n",
            "40/40 [==============================] - ETA: 2s - loss: 0.1501 - acc: 0.327 - ETA: 0s - loss: 0.1366 - acc: 0.321 - 15s 386ms/step - loss: 0.1333 - acc: 0.3161 - val_loss: 1.8721 - val_acc: 0.0887\n",
            "Epoch 353/1000\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.1307 - acc: 0.320 - ETA: 1s - loss: 0.1334 - acc: 0.323 - 15s 385ms/step - loss: 0.1292 - acc: 0.3185 - val_loss: 1.9039 - val_acc: 0.0887\n",
            "Epoch 354/1000\n",
            "40/40 [==============================] - ETA: 2s - loss: 0.1154 - acc: 0.362 - ETA: 1s - loss: 0.1322 - acc: 0.309 - 16s 391ms/step - loss: 0.1351 - acc: 0.3165 - val_loss: 1.8512 - val_acc: 0.0935\n",
            "Epoch 355/1000\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.1125 - acc: 0.320 - ETA: 1s - loss: 0.1336 - acc: 0.319 - 15s 379ms/step - loss: 0.1275 - acc: 0.3206 - val_loss: 1.8499 - val_acc: 0.0919\n",
            "Epoch 356/1000\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.1110 - acc: 0.354 - ETA: 1s - loss: 0.1123 - acc: 0.318 - 15s 376ms/step - loss: 0.1101 - acc: 0.3278 - val_loss: 1.9175 - val_acc: 0.0935\n",
            "Epoch 357/1000\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.1326 - acc: 0.336 - ETA: 0s - loss: 0.1208 - acc: 0.304 - 15s 379ms/step - loss: 0.1229 - acc: 0.3185 - val_loss: 1.8664 - val_acc: 0.0919\n",
            "Epoch 358/1000\n",
            "40/40 [==============================] - ETA: 2s - loss: 0.1203 - acc: 0.236 - ETA: 0s - loss: 0.1173 - acc: 0.284 - 16s 388ms/step - loss: 0.1244 - acc: 0.3206 - val_loss: 1.7879 - val_acc: 0.0968\n",
            "Epoch 359/1000\n",
            "40/40 [==============================] - ETA: 2s - loss: 0.1184 - acc: 0.261 - ETA: 0s - loss: 0.1192 - acc: 0.276 - 15s 386ms/step - loss: 0.1242 - acc: 0.3206 - val_loss: 1.9395 - val_acc: 0.0790\n",
            "Epoch 360/1000\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.1562 - acc: 0.359 - ETA: 1s - loss: 0.1351 - acc: 0.318 - 16s 406ms/step - loss: 0.1301 - acc: 0.3202 - val_loss: 1.8942 - val_acc: 0.0935\n",
            "Epoch 361/1000\n",
            "40/40 [==============================] - ETA: 2s - loss: 0.1141 - acc: 0.397 - ETA: 0s - loss: 0.1091 - acc: 0.321 - 16s 389ms/step - loss: 0.1076 - acc: 0.3274 - val_loss: 1.9364 - val_acc: 0.0887\n",
            "Epoch 362/1000\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.1517 - acc: 0.335 - ETA: 1s - loss: 0.1392 - acc: 0.307 - 18s 450ms/step - loss: 0.1369 - acc: 0.3125 - val_loss: 1.9391 - val_acc: 0.0903\n",
            "Epoch 363/1000\n",
            "40/40 [==============================] - ETA: 3s - loss: 0.1558 - acc: 0.225 - ETA: 1s - loss: 0.1423 - acc: 0.322 - 18s 439ms/step - loss: 0.1318 - acc: 0.3194 - val_loss: 1.8785 - val_acc: 0.0919\n",
            "Epoch 364/1000\n",
            "32/40 [=======================>......] - ETA: 3s - loss: 0.1210 - acc: 0.399 - ETA: 1s - loss: 0.1269 - acc: 0.3422"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "nspbTZlGuu2b",
        "colab_type": "code",
        "colab": {},
        "outputId": "6d2a8d0c-0d19-43d9-c562-6eb6a9d243da"
      },
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Plot training & validation accuracy values\n",
        "plt.plot(history.history['acc'])\n",
        "plt.plot(history.history['val_acc'])\n",
        "plt.title('Model accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Test'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "# Plot training & validation loss values\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Model loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Test'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "X4Dw92qXuu2e",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def decode_sequence(input_seq):\n",
        "    # Encode the input as state vectors.\n",
        "    states_value = encoder_model.predict(input_seq)\n",
        "\n",
        "    # Generate empty target sequence of length 1.\n",
        "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
        "    # Populate the first character of target sequence with the start character.\n",
        "    target_seq[0, 0, target_token_index['\\t']] = 1.\n",
        "\n",
        "    # Sampling loop for a batch of sequences\n",
        "    # (to simplify, here we assume a batch of size 1).\n",
        "    stop_condition = False\n",
        "    decoded_sentence = ''\n",
        "    while not stop_condition:\n",
        "        output_tokens, h, c = decoder_model.predict(\n",
        "            [target_seq] + states_value)\n",
        "\n",
        "        # Sample a token\n",
        "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
        "        decoded_sentence += sampled_char\n",
        "\n",
        "        # Exit condition: either hit max length\n",
        "        # or find stop character.\n",
        "        if (sampled_char == '\\n' or\n",
        "           len(decoded_sentence) > max_decoder_seq_length):\n",
        "            stop_condition = True\n",
        "\n",
        "        # Update the target sequence (of length 1).\n",
        "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
        "        target_seq[0, 0, sampled_token_index] = 1.\n",
        "\n",
        "        # Update states\n",
        "        states_value = [h, c]\n",
        "\n",
        "    return decoded_sentence"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_ICXDpyiuu2h",
        "colab_type": "code",
        "colab": {},
        "outputId": "ec695e65-9be8-4f51-f19c-c8a20f840bc4"
      },
      "cell_type": "code",
      "source": [
        "for seq_index in range(5):\n",
        "\n",
        "    # Take one sequence (part of the training set) for trying out decoding.\n",
        "    input_seq = encoder_input_data[seq_index: seq_index + 1]\n",
        "    decoded_sentence = decode_sequence(input_seq)\n",
        "\n",
        "    print('-')\n",
        "    print('Input sentence:', input_texts[seq_index])\n",
        "    print('Decoded sentence:', decoded_sentence)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-\n",
            "Input sentence: i have bought several of the vitality canned dog food products and have found them all to be of good quality. the product looks more like a stew than a processed meat and it smells better. my labrador is finicky and she appreciates this product better than most.\n",
            "Decoded sentence: ggggggggggyyyyyyyyyuyyyyyyuyyyyyuyyyyyuyyyyyuyyyyyuyyyyyuyyyyyuyyyyy\n",
            "-\n",
            "Input sentence: product arrived labeled as jumbo salted peanuts...the peanuts were actually small sized unsalted. not sure if this was an error or if the vendor intended to represent the product as  jumbo .\n",
            "Decoded sentence: ggggggggggyyyyyyyyyuyyyyyyuyyyyyuyyyyyuyyyyyuyyyyyuyyyyyuyyyyyuyyyyy\n",
            "-\n",
            "Input sentence: this is a confection that has been around a few centuries. it is a light, pillowy citrus gelatin with nuts   in this case filberts. and it is cut into tiny squares and then liberally coated with powdered sugar. and it is a tiny mouthful of heaven. not too chewy, and very flavorful. i highly recommend this yummy treat. if you are familiar with the story of c.s. lewis   the lion, the witch, and the wardrobe    this is the treat that seduces edmund into selling out his brother and sisters to the witch.\n",
            "Decoded sentence: ggggggggggyyyyyyyyyuyyyyyyuyyyyyuyyyyyuyyyyyuyyyyyuyyyyyuyyyyyuyyyyy\n",
            "-\n",
            "Input sentence: if you are looking for the secret ingredient in robitussin i believe i have found it. i got this in addition to the root beer extract i ordered  which was good  and made some cherry soda. the flavor is very medicinal.\n",
            "Decoded sentence: ggggggggggyyyyyyyyyuyyyyyyuyyyyyuyyyyyuyyyyyuyyyyyuyyyyyuyyyyyuyyyyy\n",
            "-\n",
            "Input sentence: great taffy at a great price. there was a wide assortment of yummy taffy. delivery was very quick. if your a taffy lover, this is a deal.\n",
            "Decoded sentence: ggggggggggyyyyyyyyyuyyyyyyuyyyyyuyyyyyuyyyyyuyyyyyuyyyyyuyyyyyuyyyyy\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "46585c7317f64250f9c1846f4d04e2c4907422ee",
        "id": "BYVGjRPEuu2k",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def generic_Model(n_input, n_output, n_units):\n",
        "    # define training encoder\n",
        "    encoder_inputs = Input(shape=(None, n_input))\n",
        "    encoder = LSTM(n_units, return_state=True)\n",
        "    encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
        "    encoder_states = [state_h, state_c]\n",
        "    # define training decoder\n",
        "    decoder_inputs = Input(shape=(None, n_output))\n",
        "    decoder_lstm = LSTM(n_units, return_sequences=True, return_state=True)\n",
        "    decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
        "    decoder_dense = Dense(n_output, activation='softmax')\n",
        "    \n",
        "    decoder_outputs = decoder_dense(decoder_outputs)\n",
        "    model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "    # define inference encoder\n",
        "    encoder_model = Model(encoder_inputs, encoder_states)\n",
        "    # define inference decoder\n",
        "    decoder_state_input_h = Input(shape=(n_units,))\n",
        "    decoder_state_input_c = Input(shape=(n_units,))\n",
        "    decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "    decoder_outputs, state_h, state_c = decoder_lstm(decoder_inputs,  initial_state=decoder_states_inputs)\n",
        "    decoder_states = [state_h, state_c]\n",
        "    decoder_outputs = decoder_dense(decoder_outputs)\n",
        "    decoder_model = Model([decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states)\n",
        "    # return all models\n",
        "    return model, encoder_model, decoder_model\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "ea1e80648de3d72954da93f934d9f2210f7f4738",
        "id": "sLnY_WiBuu2m",
        "colab_type": "code",
        "colab": {},
        "outputId": "9500eb13-45a2-4435-fa2c-175a33051690"
      },
      "cell_type": "code",
      "source": [
        "training_model, encoder_model, decoder_model = generic_Model(max_encoder_seq_length, max_decoder_seq_length, 128)\n",
        "training_model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_7 (InputLayer)            (None, None, 2297)   0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_8 (InputLayer)            (None, None, 67)     0                                            \n",
            "__________________________________________________________________________________________________\n",
            "lstm_4 (LSTM)                   [(None, 128), (None, 1242112     input_7[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lstm_5 (LSTM)                   [(None, None, 128),  100352      input_8[0][0]                    \n",
            "                                                                 lstm_4[0][1]                     \n",
            "                                                                 lstm_4[0][2]                     \n",
            "__________________________________________________________________________________________________\n",
            "dense_13 (Dense)                (None, None, 67)     8643        lstm_5[0][0]                     \n",
            "==================================================================================================\n",
            "Total params: 1,351,107\n",
            "Trainable params: 1,351,107\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "234e7a92e06db16dee5c08bb7699ec8dc03894d7",
        "id": "ilSFp85Guu2p",
        "colab_type": "code",
        "colab": {},
        "outputId": "ebcc95f6-ab53-4e65-c218-d678681ed23b"
      },
      "cell_type": "code",
      "source": [
        "training_model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
        "training_model.fit([input_characters, target_characters], target_characters,\n",
        "batch_size=batch_size,\n",
        "epochs=epochs,\n",
        "validation_split=0.2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Error when checking input: expected input_7 to have 3 dimensions, but got array with shape (42, 1)",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[1;32m<ipython-input-99-e2c7b0ebd1bb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m validation_split=0.2)\n\u001b[0m",
            "\u001b[1;32mc:\\python36\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m    953\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    954\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 955\u001b[1;33m             batch_size=batch_size)\n\u001b[0m\u001b[0;32m    956\u001b[0m         \u001b[1;31m# Prepare validation data.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    957\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\python36\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[0;32m    752\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    753\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# Don't enforce the batch size.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 754\u001b[1;33m             exception_prefix='input')\n\u001b[0m\u001b[0;32m    755\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    756\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\python36\\lib\\site-packages\\keras\\engine\\training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[1;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[0;32m    124\u001b[0m                         \u001b[1;34m': expected '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' to have '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    125\u001b[0m                         \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' dimensions, but got array '\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 126\u001b[1;33m                         'with shape ' + str(data_shape))\n\u001b[0m\u001b[0;32m    127\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    128\u001b[0m                     \u001b[0mdata_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_shape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mValueError\u001b[0m: Error when checking input: expected input_7 to have 3 dimensions, but got array with shape (42, 1)"
          ]
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "aa670a6d30f64756f81c558931d58084044bc307",
        "id": "9RIDdtqXuu2s",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "0f82d087944a99d6e2f2eb7832a3e98ce143cbb1",
        "id": "WpoGSuMAuu2t",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d482a2fa-4999-41e9-aff9-9baacb63806e"
      },
      "cell_type": "code",
      "source": [
        "word_to_index, index_to_word, word_to_vec_map = read_glove_vecs('glove.6B.50d.txt')"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "400000it [00:06, 58737.16it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "523e40ced0dfa29c4ae66de97fb2a07a83296f16",
        "scrolled": true,
        "id": "4dCRMb2Juu2w",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 214
        },
        "outputId": "db73bd02-c92e-49b1-cd5c-5289c52424dc"
      },
      "cell_type": "code",
      "source": [
        "print(len(word_to_vec_map))\n",
        "print(word_to_index[\"didn't\"])\n",
        "print(word_to_vec_map[\"strife-torn\"])"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "400000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-0001a2c272b4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_to_vec_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_to_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"didn't\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_to_vec_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"strife-torn\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: \"didn't\""
          ]
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "b85eee801058d40f9b1967632b6e75b4ad15ef6d",
        "id": "5zm931pBuu2z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "cae940ca-2f0b-4bdc-9948-6d0af233a5f4"
      },
      "cell_type": "code",
      "source": [
        "word = \"cucumber\"\n",
        "index = 289846\n",
        "print(len(word_to_vec_map[word]))\n",
        "print(\"the index of\", word, \"in the vocabulary is\", word_to_index[word])\n",
        "print(\"the\", str(index) + \"th word in the vocabulary is\", index_to_word[index])\n",
        "print(len(word_to_vec_map))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "50\n",
            "the index of cucumber in the vocabulary is 113317\n",
            "the 289846th word in the vocabulary is potatos\n",
            "400000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "36fc694f6d61a67426121d05d5d7ecbe5f6576e8",
        "id": "rGXQQ4QTuu22",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Exercise**: Implement `sentence_to_avg()`. This function performs two steps described as follows:\n",
        "1. Convert every sentence to lower-case, then split the sentence into a list of words.\n",
        "2. For each word in the sentence, access its GloVe representation. Then, average all these values."
      ]
    },
    {
      "metadata": {
        "_uuid": "370e34b8fd3664c187fdf1f24bb718216ef3eb58",
        "id": "Pho7N204uu22",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# GRADED FUNCTION: sentence_to_avg\n",
        "\n",
        "def sentence_to_avg(sentence, word_to_vec_map):\n",
        "    \"\"\"\n",
        "    Converts a sentence (string) into a list of words (strings). Extracts the GloVe representation of each word\n",
        "    and averages its value into a single vector encoding the meaning of the sentence.\n",
        "    \n",
        "    Arguments:\n",
        "    sentence -- string, one training example from X\n",
        "    word_to_vec_map -- dictionary mapping every word in a vocabulary into its 50-dimensional vector representation\n",
        "    \n",
        "    Returns:\n",
        "    avg -- average vector encoding information about the sentence, numpy-array of shape (50,)\n",
        "    \"\"\"\n",
        "    \n",
        "    ### START CODE HERE ###\n",
        "    # Step 1: Split sentence into list of lower case words ( 1 line)\n",
        "    words = [i.lower() for i in sentence.split()]\n",
        "    \n",
        "    # Initialize the average word vector, should have the same shape as your word vectors.\n",
        "    avg = np.zeros((len(word_to_vec_map[\"a\"]),))\n",
        "    \n",
        "    # Step 2: average the word vectors. You can loop over the words in the list \"words\".\n",
        "    for w in words:\n",
        "        try:\n",
        "            avg += word_to_vec_map[w]\n",
        "        except KeyError:\n",
        "            print(w)\n",
        "            continue\n",
        "        \n",
        "    avg = avg / len(words)\n",
        "    \n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    return avg"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "28d98594c056c9c0e584f53f0eeaece1d1fdeb3d",
        "id": "yKrso8TYuu24",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "c6f6899a-cc34-4881-d0ee-a023fd3a9ecd"
      },
      "cell_type": "code",
      "source": [
        "avg = sentence_to_avg(\"at least two people were killed in a suspected bomb attack on a passenger bus in the strife-torn southern philippines on monday , the military said .\\n\", word_to_vec_map)\n",
        "print(\"avg = \", avg)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "strife-torn\n",
            "avg =  [ 6.00406333e-01 -3.32836241e-02  1.87963111e-01  6.77701481e-02\n",
            "  3.54529296e-01  5.80289259e-02 -5.28430741e-01  2.34516926e-02\n",
            " -4.44399430e-02 -4.49052852e-01 -4.94001852e-02 -5.91713370e-01\n",
            " -3.30848889e-01 -3.86380000e-02  2.55290974e-01 -1.00995963e-01\n",
            " -2.50230633e-01  1.57850185e-01 -8.12166667e-01  1.53866296e-02\n",
            "  1.62483900e-01  3.39234815e-01  1.11209556e-01  1.11625963e-02\n",
            " -1.28282022e-01 -1.71446667e+00 -3.79737037e-02  7.29834074e-02\n",
            "  7.51024444e-02 -5.31830370e-02  3.21913704e+00 -1.57129481e-02\n",
            " -3.00656678e-01 -2.03952637e-01  3.13164624e-01  7.13015852e-02\n",
            "  1.96886630e-01 -2.07194259e-01 -1.68992593e-03  3.62324111e-01\n",
            " -3.64241751e-01  2.49083281e-01  2.42211593e-01 -2.78113704e-01\n",
            "  2.00010611e-01  2.41158148e-02 -4.04617674e-01 -2.34434815e-02\n",
            "  1.46426704e-01 -3.46355148e-01]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "fd9716098a859c18687a3761bcc4401916ad81be",
        "id": "3PD7NV2Fuu27",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def preprocess_data(X):\n",
        "    from tqdm import tqdm\n",
        "    \n",
        "    m = len(X)\n",
        "    str_check = [\",\", \".\", '\"', \"'\", \"(\", \")\", \"$\", \"mph\", \"-\", \"_\"]\n",
        "    str_remove = [\"-lrb-\", \"-rrb-\"]\n",
        "    \n",
        "    for i in tqdm(range(m)):\n",
        "        sentence_words = [w for w in X[i].split()]\n",
        "        j = 0\n",
        "        new_string = \"\"\n",
        "        if(i == 0):\n",
        "            print(X[i])\n",
        "        \n",
        "        for w in sentence_words:\n",
        "            try:\n",
        "               _ = word_to_index[w]\n",
        "            except KeyError:\n",
        "                for stri_r in str_remove:\n",
        "                    if(stri_r in w):\n",
        "                        #print(stri_r)\n",
        "                        w = w.replace(stri_r,\" \")\n",
        "                for stri in str_check:\n",
        "                    if(stri in w):\n",
        "                        idx = w.index(stri)\n",
        "                        if(w[idx:idx+2] == \"'s\"):\n",
        "                            stri = \"'s\"\n",
        "                            idx = w.index(stri)\n",
        "                            a = w[:idx]\n",
        "                            b = w[idx + 2:]\n",
        "                            w = a + \" \" + stri + \" \" + b\n",
        "                            continue\n",
        "                        elif(w[idx:idx+3] == \"mph\"):\n",
        "                            stri = \"mph\"\n",
        "                            idx = w.index(stri)\n",
        "                            if(w[idx - 1] == \" \"):\n",
        "                                a = \"\"\n",
        "                            else:\n",
        "                                a = w[:idx]\n",
        "                            b = w[idx + 3:]\n",
        "                            w = a + \" \" + stri + \" \" + b\n",
        "                            continue\n",
        "                        if(w[idx - 1] == \" \"):\n",
        "                            a = \"\"\n",
        "                        else:\n",
        "                            a = w[:idx]\n",
        "\n",
        "                        b = w[idx + 1:]\n",
        "                        w = a + \" \" + stri + \" \" + b\n",
        "            \n",
        "            new_string += w + \" \"\n",
        "        X[i] = new_string\n",
        "        if(i == 0):\n",
        "            print(X[i])\n",
        "    return X"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "b0d1502e9d10aa5eefb9fb45c5fef9578c4f0364",
        "id": "lEK7cl5Kuu29",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# GRADED FUNCTION: sentences_to_indices\n",
        "\n",
        "def sentences_to_indices(X, word_to_index, max_len):\n",
        "    from tqdm import tqdm\n",
        "    \"\"\"\n",
        "    Converts an array of sentences (strings) into an array of indices corresponding to words in the sentences.\n",
        "    The output shape should be such that it can be given to `Embedding()` (described in Figure 4). \n",
        "    \n",
        "    Arguments:\n",
        "    X -- array of sentences (strings), of shape (m, 1)\n",
        "    word_to_index -- a dictionary containing the each word mapped to its index\n",
        "    max_len -- maximum number of words in a sentence. You can assume every sentence in X is no longer than this. \n",
        "    \n",
        "    Returns:\n",
        "    X_indices -- array of indices corresponding to words in the sentences from X, of shape (m, max_len)\n",
        "    \"\"\"\n",
        "    \n",
        "    m = len(X)                                # number of training examples\n",
        "    \n",
        "    ### START CODE HERE ###\n",
        "    # Initialize X_indices as a numpy matrix of zeros and the correct shape ( 1 line)\n",
        "    X_indices = np.zeros((m, max_len))\n",
        "    \n",
        "    for i in tqdm(range(m)):                               # loop over training examples\n",
        "        \n",
        "        # Convert the ith training sentence in lower case and split is into words. You should get a list of words.\n",
        "        sentence_words = [w.lower() for w in X[i].split()]\n",
        "        \n",
        "        # Initialize j to 0\n",
        "        j = 0\n",
        "        \n",
        "        # Loop over the words of sentence_words\n",
        "        for w in sentence_words:\n",
        "            # Set the (i,j)th entry of X_indices to the index of the correct word.\n",
        "            try:\n",
        "                #if(w == \"<unk>\"):\n",
        "                    #X_indices[i, j] = -1\n",
        "                #else:\n",
        "                    #X_indices[i, j] = word_to_index[w]\n",
        "                X_indices[i, j] = word_to_index[w]\n",
        "            except Exception as e:\n",
        "                #print(e)\n",
        "                pass\n",
        "            # Increment j to j + 1\n",
        "            j += 1\n",
        "            \n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    return X_indices"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NtuBTvDTIeKZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# GRADED FUNCTION: sentences_to_indices\n",
        "\n",
        "def sentences_to_indices_reversed(X, word_to_index, max_len):\n",
        "    from tqdm import tqdm\n",
        "    \"\"\"\n",
        "    Converts an array of sentences (strings) into an array of indices corresponding to words in the sentences.\n",
        "    The output shape should be such that it can be given to `Embedding()` (described in Figure 4). \n",
        "    \n",
        "    Arguments:\n",
        "    X -- array of sentences (strings), of shape (m, 1)\n",
        "    word_to_index -- a dictionary containing the each word mapped to its index\n",
        "    max_len -- maximum number of words in a sentence. You can assume every sentence in X is no longer than this. \n",
        "    \n",
        "    Returns:\n",
        "    X_indices -- array of indices corresponding to words in the sentences from X, of shape (m, max_len)\n",
        "    \"\"\"\n",
        "    \n",
        "    m = len(X)                                # number of training examples\n",
        "    \n",
        "    ### START CODE HERE ###\n",
        "    # Initialize X_indices as a numpy matrix of zeros and the correct shape ( 1 line)\n",
        "    X_indices = np.zeros((m, max_len))\n",
        "    \n",
        "    for i in tqdm(range(m)):                               # loop over training examples\n",
        "        \n",
        "        # Convert the ith training sentence in lower case and split is into words. You should get a list of words.\n",
        "        sentence_words = [w.lower() for w in X[i].split()]\n",
        "        \n",
        "        # Initialize j to 0\n",
        "        j = 0\n",
        "        \n",
        "        # Loop over the words of sentence_words\n",
        "        for w in sentence_words:\n",
        "            # Set the (i,j)th entry of X_indices to the index of the correct word.\n",
        "            try:\n",
        "                #if(w == \"<unk>\"):\n",
        "                    #X_indices[i, j] = -1\n",
        "                #else:\n",
        "                    #X_indices[i, j] = word_to_index[w]\n",
        "                X_indices[i, - 1 - j] = word_to_index[w]\n",
        "            except Exception as e:\n",
        "                #print(e)\n",
        "                pass\n",
        "            # Increment j to j + 1\n",
        "            j += 1\n",
        "            \n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    return X_indices"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "6c5e925bca627dc0f6008eeb9cb1cf19672f596e",
        "scrolled": false,
        "id": "kuHQL_FOuu2_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 666
        },
        "outputId": "2aa61957-406e-4cba-e529-50b9a46a7688"
      },
      "cell_type": "code",
      "source": [
        "X = open(\"train.article.10000.txt\").readlines()[:100]\n",
        "X = preprocess_data(X)\n",
        "print(len(X))\n",
        "length = [len(x.split()) for x in X]\n",
        "maxLen = max(length)\n",
        "print(maxLen)\n",
        "X1 = sentences_to_indices_reversed(X, word_to_index, maxLen)\n",
        "print(X1.shape)\n",
        "\n",
        "print(\"X[0] =\", X[9])\n",
        "print(\"X1_indices[0] =\", X1[9])\n",
        "\n",
        "print(\"X[0] =\", X[length.index(max(length))])\n",
        "print(\"X1_indices[0] =\", X1[length.index(max(length))])\n",
        "\n",
        "\n",
        "Y = open(\"train.title.10000.txt\").readlines()[:100]\n",
        "Y = preprocess_data(Y)\n",
        "print(len(Y))\n",
        "length_Y = [len(y.split()) for y in Y]\n",
        "maxLen_Y = max(length_Y)\n",
        "print(maxLen_Y)\n",
        "Y1 = sentences_to_indices(Y, word_to_index, maxLen_Y)\n",
        "print(Y1.shape)\n",
        "print(\"Y[0] =\", Y[0])\n",
        "print(\"Y1_indices[0] =\", Y1[0])\n",
        "print(\"Y[0] =\", Y[length_Y.index(max(length_Y))])\n",
        "print(\"Y1_indices[0] =\", Y1[length_Y.index(max(length_Y))])"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|| 100/100 [00:00<00:00, 16401.94it/s]\n",
            "100%|| 100/100 [00:00<00:00, 35261.07it/s]\n",
            "100%|| 100/100 [00:00<00:00, 21368.98it/s]\n",
            "100%|| 100/100 [00:00<00:00, 31368.66it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "australia 's current account deficit shrunk by a record #.## billion dollars -lrb- #.## billion us -rrb- in the june quarter due to soaring commodity prices , figures released monday showed .\n",
            "\n",
            "australia 's current account deficit shrunk by a record # . ## billion dollars   # . ## billion us   in the june quarter due to soaring commodity prices , figures released monday showed . \n",
            "100\n",
            "41\n",
            "(100, 41)\n",
            "X[0] = the united nations ' humanitarian chief john holmes arrived in ethiopia monday to tour regions affected by drought , which has left some eight million people in need of urgent food aid . \n",
            "X1_indices[0] = [0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00\n",
            " 0.00000e+00 0.00000e+00 8.67000e+02 4.88310e+04 1.51204e+05 3.73814e+05\n",
            " 2.68046e+05 2.58451e+05 1.88481e+05 2.80944e+05 2.44641e+05 1.34390e+05\n",
            " 3.37259e+05 2.19577e+05 1.74032e+05 3.86474e+05 4.52000e+02 1.29635e+05\n",
            " 8.81260e+04 4.74860e+04 3.04268e+05 3.62970e+05 3.60915e+05 2.48403e+05\n",
            " 1.40306e+05 1.88481e+05 6.00960e+04 1.80703e+05 1.98418e+05 9.88340e+04\n",
            " 1.83878e+05 4.80000e+01 2.57214e+05 3.72191e+05 3.57266e+05]\n",
            "X[0] = president george w. bush declared an emergency in texas and state authorities began ordering evacuations as deadly hurricane ike , which strengthened wednesday to a powerful category two storm , churned in the gulf of mexico toward the us state . \n",
            "X1_indices[0] = [   867. 341956. 374021. 357266. 363080. 242556. 268046. 168964. 357266.\n",
            " 188481. 100759.    452. 343914. 368321.  93724. 290201.  43010. 360915.\n",
            " 384674. 344582. 386474.    452. 187122. 184300. 118226.  60665. 141683.\n",
            " 271178.  71917.  63526. 341956.  54718. 356797. 188481. 136330.  54273.\n",
            " 118832.  87571. 381773. 159864. 291804.]\n",
            "australian current account deficit narrows sharply\n",
            "\n",
            "australian current account deficit narrows sharply \n",
            "100\n",
            "13\n",
            "(100, 13)\n",
            "Y[0] = australian current account deficit narrows sharply \n",
            "Y1_indices[0] = [ 63426. 113946.  45230. 119346. 256776. 327623.      0.      0.      0.\n",
            "      0.      0.      0.      0.]\n",
            "Y[0] = algeria adopts #### finance bill with oil put at ## dollars a barrel \n",
            "Y1_indices[0] = [5.12450e+04 4.67900e+04 0.00000e+00 1.48365e+05 7.60840e+04 3.88711e+05\n",
            " 2.68641e+05 2.95763e+05 6.20650e+04 1.00000e+01 1.27119e+05 4.30100e+04\n",
            " 6.93030e+04]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "ae2f7b38b5174f5297b18575ceb771b4207bf9e5",
        "id": "q_-e2Yy4uu3D",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ede2bb75-c73e-4260-a42f-bb7fdf81a1e1"
      },
      "cell_type": "code",
      "source": [
        "print(index_to_word[341956])"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "state\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "5a82a5bfbe6cbc2fea60ff56a81c85a75f87f7e8",
        "id": "RUSGUPQhuu3H",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def convert_to_one_hot(X, word_to_index):\n",
        "    \n",
        "    m = X.shape[0]\n",
        "    dim = X.shape[1]\n",
        "    print(type(X))\n",
        "    length = len(word_to_index) + 1\n",
        "    print(length)\n",
        "    \n",
        "    Z = np.zeros((m, dim, length + 1))\n",
        "    \n",
        "    \n",
        "    for i in tqdm(range(m)):\n",
        "        for j in range(dim):\n",
        "            if(j > 0):\n",
        "                if(X[i,j] == -1):\n",
        "                    Z[i, j, length] = 1\n",
        "                else:\n",
        "                    idx = X[i,j]\n",
        "                    Z[i, j - 1, int(idx)] = 1\n",
        "                \n",
        "    return Z"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "b62099dd7797b287e190732711c251eb77e3e972",
        "scrolled": false,
        "id": "DYTTe2keuu3K",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "3c0ba365-cc56-4562-ec6c-604ae24a2560"
      },
      "cell_type": "code",
      "source": [
        "decoder_target_data = convert_to_one_hot(Y1, word_to_index)\n",
        "decoder_target_data[0]"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'numpy.ndarray'>\n",
            "400001\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|| 100/100 [00:00<00:00, 28480.37it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       ...,\n",
              "       [1., 0., 0., ..., 0., 0., 0.],\n",
              "       [1., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "0689fd7ea82293bae5c9286d7b72ad275265b4d4",
        "id": "r-2jtgHQuu3O",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "eadb80dec6592ce803d45afb40e5d12e78f83255",
        "scrolled": false,
        "id": "bLFLfqQnuu3R",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        },
        "outputId": "e28e11fb-44eb-4fc5-9542-c2ce7f32206d"
      },
      "cell_type": "code",
      "source": [
        "X2 = open(\"dataset/DUC/duc2002/data/test/summaries/Text.txt\").readlines()\n",
        "preprocess_data(X2)\n",
        "X3 = sentences_to_indices(X2, word_to_index, max_len = 1500)\n",
        "print(\"X2[0] =\", X2[1])\n",
        "print(\"X3_indices[0] =\", X3[1])"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-9d609b708e19>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"dataset/DUC/duc2002/data/test/summaries/Text.txt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mpreprocess_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mX3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msentences_to_indices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_to_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1500\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"X2[0] =\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"X3_indices[0] =\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX3\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'dataset/DUC/duc2002/data/test/summaries/Text.txt'"
          ]
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "fe401b4d9105962786b170ef10d274c9b6ddd667",
        "id": "F1VQc9-zuu3T",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(index_to_word[8])\n",
        "print(word_to_index[\"#\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "27d23f789b586bf6c87e7a38a2eac968a48eb4ed",
        "id": "a7VTiX6Puu3V",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# GRADED FUNCTION: pretrained_embedding_layer\n",
        "\n",
        "def pretrained_embedding_layer(word_to_vec_map, word_to_index):\n",
        "    \"\"\"\n",
        "    Creates a Keras Embedding() layer and loads in pre-trained GloVe 50-dimensional vectors.\n",
        "    \n",
        "    Arguments:\n",
        "    word_to_vec_map -- dictionary mapping words to their GloVe vector representation.\n",
        "    word_to_index -- dictionary mapping from words to their indices in the vocabulary (400,001 words)\n",
        "\n",
        "    Returns:\n",
        "    embedding_layer -- pretrained layer Keras instance\n",
        "    \"\"\"\n",
        "    \n",
        "    vocab_len = len(word_to_index) + 1                  # adding 1 to fit Keras embedding (requirement)\n",
        "    emb_dim = word_to_vec_map[\"cucumber\"].shape[0]      # define dimensionality of your GloVe word vectors (= 50)\n",
        "    \n",
        "    ### START CODE HERE ###\n",
        "    # Initialize the embedding matrix as a numpy array of zeros of shape (vocab_len, dimensions of word vectors = emb_dim)\n",
        "    emb_matrix = np.zeros((vocab_len, emb_dim))\n",
        "    \n",
        "    # Set each row \"index\" of the embedding matrix to be the word vector representation of the \"index\"th word of the vocabulary\n",
        "    for word, index in word_to_index.items():\n",
        "        emb_matrix[index, :] = word_to_vec_map[word]\n",
        "\n",
        "    # Define Keras embedding layer with the correct output/input sizes, make it trainable. Use Embedding(...). Make sure to set trainable=False. \n",
        "    embedding_layer = Embedding(vocab_len, emb_dim, trainable = True)\n",
        "    ### END CODE HERE ###\n",
        "\n",
        "    # Build the embedding layer, it is required before setting the weights of the embedding layer. Do not modify the \"None\".\n",
        "    embedding_layer.build((None,))\n",
        "    \n",
        "    # Set the weights of the embedding layer to the embedding matrix. Your layer is now pretrained.\n",
        "    embedding_layer.set_weights([emb_matrix])\n",
        "    \n",
        "    return embedding_layer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "80e0aa982cf969d29f8a21193e208eb02408da93",
        "id": "V51w9VkEuu3Y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "614f30fd-5205-4e77-bf38-b74913c252b9"
      },
      "cell_type": "code",
      "source": [
        "embedding_layer = pretrained_embedding_layer(word_to_vec_map, word_to_index)\n",
        "print(\"weights[0][1][3] =\", embedding_layer.get_weights()[0][1][3])"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "weights[0][1][3] = -0.3403\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "pstj80DxMNWJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def attention_3d_block(inputs):\n",
        "    # inputs.shape = (batch_size, time_steps, input_dim)\n",
        "    # if True, the attention vector is shared across the input_dimensions where the attention is applied.\n",
        "    SINGLE_ATTENTION_VECTOR = False\n",
        "    TIME_STEPS = inputs.shape[1]\n",
        "    vocab_len = len(word_to_index) + 1\n",
        "    emb_dim = word_to_vec_map[\"cucumber\"].shape[0]      # define dimensionality of your GloVe word vectors (= 50)\n",
        "    #print(TIME_STEPS)\n",
        "    input_dim = int(inputs.shape[2])\n",
        "    a = Permute((2, 1))(inputs)\n",
        "    #a = Reshape((input_dim, TIME_STEPS))(a) # this line is not useful. It's just to know which dimension is what.\n",
        "    a = Dense(41, activation='softmax')(a)\n",
        "    if SINGLE_ATTENTION_VECTOR:\n",
        "        a = Lambda(lambda x: K.mean(x, axis=1), name='dim_reduction')(a)\n",
        "        a = RepeatVector(input_dim)(a)\n",
        "    a_probs = Permute((2, 1), name='attention_vec')(a)\n",
        "    output_attention_mul = mul([inputs, a_probs])\n",
        "    return output_attention_mul"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YcN0FuErMO7o",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "f9059859daa58dddf01024342d95f0a77f3e26e1",
        "id": "x_2zVNB9uu3c",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def Model_Text(encoder_input_shape, word_to_vec_map, word_to_index, decoder_input_shape):\n",
        "    \n",
        "    vocab_len = len(word_to_index) + 1                  # adding 1 to fit Keras embedding (requirement)\n",
        "    emb_dim = word_to_vec_map[\"cucumber\"].shape[0]      # define dimensionality of GloVe word vectors (= 50)\n",
        "    \n",
        "    sentence_indices = Input(encoder_input_shape, dtype='int32')\n",
        "    \n",
        "    embedding_layer = pretrained_embedding_layer(word_to_vec_map, word_to_index)\n",
        "    \n",
        "    embeddings = embedding_layer(sentence_indices) \n",
        "    \n",
        "    attention_mul = attention_3d_block(embeddings)\n",
        "    \n",
        "    X, state_h, state_c = LSTM(emb_dim, return_state=True)(attention_mul)\n",
        "    \n",
        "    \n",
        "    encoder_states = [state_h, state_c]\n",
        "    \n",
        "    \n",
        "    #Set up the decoder, using `encoder_states` as initial state.\n",
        "\n",
        "    decoder_inputs = Input(decoder_input_shape, dtype = 'int32')\n",
        "    \n",
        "    \n",
        "    embedding_layer = pretrained_embedding_layer(word_to_vec_map, word_to_index)\n",
        "    embeddings = embedding_layer(decoder_inputs) \n",
        "    \n",
        "    x = LSTM(emb_dim, return_sequences = True)(embeddings, initial_state=encoder_states)\n",
        "    \n",
        "    decoder_outputs = Dense(vocab_len + 1, activation='softmax')(x)\n",
        "\n",
        "    \n",
        "    model = Model(inputs=[sentence_indices, decoder_inputs], outputs= decoder_outputs)\n",
        "    \n",
        "    \n",
        "    # Encoder Model for Inference\n",
        "    encoder_model = Model(sentence_indices, encoder_states)\n",
        "\n",
        "    \n",
        "    # Decoder Model for Inference\n",
        "    decoder_state_input_h = Input(shape=(emb_dim,), name = \"Inference_decoder_input_hidden_state\")\n",
        "    decoder_state_input_c = Input(shape=(emb_dim,), name = \"Inference_decoder_input_cell_state\")\n",
        "    decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "    \n",
        "    decoder_outputs_1, state_h, state_c = LSTM(emb_dim,\n",
        "                                               return_sequences=True, \n",
        "                                               return_state=True,\n",
        "                                               name = \"Inference_decoder_LSTM\")(embeddings, initial_state=decoder_states_inputs)\n",
        "    \n",
        "    decoder_states = [state_h, state_c]\n",
        "    \n",
        "    decoder_outputs_1 = Dense(vocab_len + 1, \n",
        "                              activation='softmax',\n",
        "                              kernel_initializer= glorot_uniform(seed = 0),\n",
        "                              bias_initializer='zeros',\n",
        "                              name = \"decoder_Dense_Output\")(decoder_outputs_1)\n",
        "    decoder_model = Model([decoder_inputs] + decoder_states_inputs,\n",
        "                          [decoder_outputs_1] + decoder_states)\n",
        "\n",
        "    return model, encoder_model, decoder_model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "028c7323e0aec568b1a613e7750a9f8c6604a676",
        "id": "DloNf-kpuu3f",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "concatenator = Concatenate(axis = 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "ec781ef1d05b557fe5a0d6cdb2700c24edd5a552",
        "id": "LLZ6FDVKuu3h",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def Model_Text3(encoder_input_shape, word_to_vec_map, word_to_index, decoder_input_shape):\n",
        "    \n",
        "    vocab_len = len(word_to_index) + 1                  # adding 1 to fit Keras embedding (requirement)\n",
        "    emb_dim = word_to_vec_map[\"cucumber\"].shape[0]      # define dimensionality of GloVe word vectors (= 50)\n",
        "    \n",
        "    sentence_indices = Input(encoder_input_shape, dtype='int32')\n",
        "    \n",
        "    embedding_layer = pretrained_embedding_layer(word_to_vec_map, word_to_index)\n",
        "    \n",
        "    embeddings = embedding_layer(sentence_indices) \n",
        "    \n",
        "    X, _, _ = LSTM(emb_dim, return_state=True)(embeddings)\n",
        "    \n",
        "    \n",
        "    \n",
        "    #Set up the decoder, using `encoder_states` as initial state.\n",
        "    print(decoder_input_shape)\n",
        "    print(encoder_input_shape)\n",
        "    decoder_inputs = Input(decoder_input_shape, dtype='int32')\n",
        "    \n",
        "    \n",
        "    embedding_layer = pretrained_embedding_layer(word_to_vec_map, word_to_index)\n",
        "    \n",
        "    embeddings = embedding_layer(decoder_inputs)\n",
        "    \n",
        "    x, _, _ = LSTM(emb_dim, return_state=True)(embeddings)\n",
        "    \n",
        "    \n",
        "    decoder1 = concatenator([X, x])\n",
        "    \n",
        "    decoder_outputs = Dense(vocab_len + 1, activation='softmax')(decoder1)\n",
        "\n",
        "    \n",
        "    model = Model(inputs=[sentence_indices, decoder_inputs], outputs= decoder_outputs)\n",
        "    \n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "55b6b67f02384d0de0d1bf1c7152975aed9073ce",
        "id": "NJwHBo8Cuu3j",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def Model_Text2(encoder_input_shape, word_to_vec_map, word_to_index, decoder_input_shape):\n",
        "    \n",
        "    vocab_len = len(word_to_index) + 1                  # adding 1 to fit Keras embedding (requirement)\n",
        "    emb_dim = word_to_vec_map[\"cucumber\"].shape[0]      # define dimensionality of GloVe word vectors (= 50)\n",
        "    \n",
        "    sentence_indices = Input(encoder_input_shape, dtype='int32')\n",
        "    \n",
        "    embedding_layer = pretrained_embedding_layer(word_to_vec_map, word_to_index)\n",
        "    \n",
        "    embeddings = embedding_layer(sentence_indices) \n",
        "    \n",
        "    X, _, _ = LSTM(emb_dim, return_state=True)(embeddings)\n",
        "    \n",
        "    \n",
        "    #x = LSTM(emb_dim)(X)\n",
        "    \n",
        "    decoder_outputs = Dense(vocab_len + 1, activation='softmax')(X)\n",
        "\n",
        "    \n",
        "    model = Model(inputs=sentence_indices, outputs= decoder_outputs)\n",
        "    \n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "1ef6d5abc51408698e05f2d65fb77e619c4d0aa2",
        "scrolled": false,
        "id": "0j_ns1a0uu3m",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1404
        },
        "outputId": "d4054a13-20a0-44bb-8fd6-26d7bd21d010"
      },
      "cell_type": "code",
      "source": [
        "model, encoder_model, decoder_model = Model_Text((maxLen,), word_to_vec_map, word_to_index, (maxLen_Y,))\n",
        "model.summary()\n",
        "# Print Model Summary\n",
        "plot_model(model, to_file='model_word_attention.png')\n",
        "SVG(model_to_dot(model).create(prog='dot', format='svg'))\n"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_3 (InputLayer)            (None, 41)           0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_4 (Embedding)         (None, 41, 50)       20000050    input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "permute_2 (Permute)             (None, 50, 41)       0           embedding_4[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dense_3 (Dense)                 (None, 50, 41)       1722        permute_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "attention_vec (Permute)         (None, 41, 50)       0           dense_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "input_4 (InputLayer)            (None, 13)           0                                            \n",
            "__________________________________________________________________________________________________\n",
            "multiply_2 (Multiply)           (None, 41, 50)       0           embedding_4[0][0]                \n",
            "                                                                 attention_vec[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "embedding_5 (Embedding)         (None, 13, 50)       20000050    input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lstm_3 (LSTM)                   [(None, 50), (None,  20200       multiply_2[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lstm_4 (LSTM)                   (None, 13, 50)       20200       embedding_5[0][0]                \n",
            "                                                                 lstm_3[0][1]                     \n",
            "                                                                 lstm_3[0][2]                     \n",
            "__________________________________________________________________________________________________\n",
            "dense_4 (Dense)                 (None, 13, 400002)   20400102    lstm_4[0][0]                     \n",
            "==================================================================================================\n",
            "Total params: 60,442,324\n",
            "Trainable params: 60,442,324\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.SVG object>"
            ],
            "image/svg+xml": "<svg height=\"629pt\" viewBox=\"0.00 0.00 373.50 629.00\" width=\"374pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g class=\"graph\" id=\"graph0\" transform=\"scale(1 1) rotate(0) translate(4 625)\">\n<title>G</title>\n<polygon fill=\"#ffffff\" points=\"-4,4 -4,-625 369.5,-625 369.5,4 -4,4\" stroke=\"transparent\"/>\n<!-- 139820497008624 -->\n<g class=\"node\" id=\"node1\">\n<title>139820497008624</title>\n<polygon fill=\"none\" points=\"59.5,-584.5 59.5,-620.5 192.5,-620.5 192.5,-584.5 59.5,-584.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"126\" y=\"-598.8\">input_3: InputLayer</text>\n</g>\n<!-- 139820497007728 -->\n<g class=\"node\" id=\"node2\">\n<title>139820497007728</title>\n<polygon fill=\"none\" points=\"40.5,-511.5 40.5,-547.5 211.5,-547.5 211.5,-511.5 40.5,-511.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"126\" y=\"-525.8\">embedding_4: Embedding</text>\n</g>\n<!-- 139820497008624&#45;&gt;139820497007728 -->\n<g class=\"edge\" id=\"edge1\">\n<title>139820497008624-&gt;139820497007728</title>\n<path d=\"M126,-584.4551C126,-576.3828 126,-566.6764 126,-557.6817\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"129.5001,-557.5903 126,-547.5904 122.5001,-557.5904 129.5001,-557.5903\" stroke=\"#000000\"/>\n</g>\n<!-- 139820497322784 -->\n<g class=\"node\" id=\"node3\">\n<title>139820497322784</title>\n<polygon fill=\"none\" points=\"12.5,-438.5 12.5,-474.5 145.5,-474.5 145.5,-438.5 12.5,-438.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"79\" y=\"-452.8\">permute_2: Permute</text>\n</g>\n<!-- 139820497007728&#45;&gt;139820497322784 -->\n<g class=\"edge\" id=\"edge2\">\n<title>139820497007728-&gt;139820497322784</title>\n<path d=\"M114.382,-511.4551C108.9024,-502.9441 102.2531,-492.6165 96.2045,-483.2219\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"99.0035,-481.1037 90.6472,-474.5904 93.1178,-484.8931 99.0035,-481.1037\" stroke=\"#000000\"/>\n</g>\n<!-- 139820496895168 -->\n<g class=\"node\" id=\"node7\">\n<title>139820496895168</title>\n<polygon fill=\"none\" points=\"56,-219.5 56,-255.5 194,-255.5 194,-219.5 56,-219.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"125\" y=\"-233.8\">multiply_2: Multiply</text>\n</g>\n<!-- 139820497007728&#45;&gt;139820496895168 -->\n<g class=\"edge\" id=\"edge5\">\n<title>139820497007728-&gt;139820496895168</title>\n<path d=\"M138.0779,-511.1731C144.0973,-500.9747 150.7677,-487.7883 154,-475 173.9334,-396.1359 178.341,-370.4949 157,-292 154.3787,-282.3584 149.5732,-272.6801 144.4691,-264.2112\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"147.2554,-262.073 138.9027,-255.5551 141.3677,-265.8592 147.2554,-262.073\" stroke=\"#000000\"/>\n</g>\n<!-- 139820497007000 -->\n<g class=\"node\" id=\"node4\">\n<title>139820497007000</title>\n<polygon fill=\"none\" points=\"22.5,-365.5 22.5,-401.5 129.5,-401.5 129.5,-365.5 22.5,-365.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"76\" y=\"-379.8\">dense_3: Dense</text>\n</g>\n<!-- 139820497322784&#45;&gt;139820497007000 -->\n<g class=\"edge\" id=\"edge3\">\n<title>139820497322784-&gt;139820497007000</title>\n<path d=\"M78.2584,-438.4551C77.9267,-430.3828 77.5278,-420.6764 77.1582,-411.6817\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"80.6512,-411.4382 76.7434,-401.5904 73.6571,-411.7257 80.6512,-411.4382\" stroke=\"#000000\"/>\n</g>\n<!-- 139820496392768 -->\n<g class=\"node\" id=\"node5\">\n<title>139820496392768</title>\n<polygon fill=\"none\" points=\"0,-292.5 0,-328.5 148,-328.5 148,-292.5 0,-292.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"74\" y=\"-306.8\">attention_vec: Permute</text>\n</g>\n<!-- 139820497007000&#45;&gt;139820496392768 -->\n<g class=\"edge\" id=\"edge4\">\n<title>139820497007000-&gt;139820496392768</title>\n<path d=\"M75.5056,-365.4551C75.2845,-357.3828 75.0185,-347.6764 74.7721,-338.6817\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"78.2683,-338.4907 74.4956,-328.5904 71.2709,-338.6825 78.2683,-338.4907\" stroke=\"#000000\"/>\n</g>\n<!-- 139820496392768&#45;&gt;139820496895168 -->\n<g class=\"edge\" id=\"edge6\">\n<title>139820496392768-&gt;139820496895168</title>\n<path d=\"M86.6067,-292.4551C92.614,-283.8564 99.9168,-273.4034 106.5341,-263.9316\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"109.5036,-265.7925 112.3615,-255.5904 103.7652,-261.7835 109.5036,-265.7925\" stroke=\"#000000\"/>\n</g>\n<!-- 139820497065184 -->\n<g class=\"node\" id=\"node6\">\n<title>139820497065184</title>\n<polygon fill=\"none\" points=\"213.5,-219.5 213.5,-255.5 346.5,-255.5 346.5,-219.5 213.5,-219.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"280\" y=\"-233.8\">input_4: InputLayer</text>\n</g>\n<!-- 139820497611520 -->\n<g class=\"node\" id=\"node8\">\n<title>139820497611520</title>\n<polygon fill=\"none\" points=\"194.5,-146.5 194.5,-182.5 365.5,-182.5 365.5,-146.5 194.5,-146.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"280\" y=\"-160.8\">embedding_5: Embedding</text>\n</g>\n<!-- 139820497065184&#45;&gt;139820497611520 -->\n<g class=\"edge\" id=\"edge7\">\n<title>139820497065184-&gt;139820497611520</title>\n<path d=\"M280,-219.4551C280,-211.3828 280,-201.6764 280,-192.6817\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"283.5001,-192.5903 280,-182.5904 276.5001,-192.5904 283.5001,-192.5903\" stroke=\"#000000\"/>\n</g>\n<!-- 139820496893488 -->\n<g class=\"node\" id=\"node9\">\n<title>139820496893488</title>\n<polygon fill=\"none\" points=\"74,-146.5 74,-182.5 176,-182.5 176,-146.5 74,-146.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"125\" y=\"-160.8\">lstm_3: LSTM</text>\n</g>\n<!-- 139820496895168&#45;&gt;139820496893488 -->\n<g class=\"edge\" id=\"edge8\">\n<title>139820496895168-&gt;139820496893488</title>\n<path d=\"M125,-219.4551C125,-211.3828 125,-201.6764 125,-192.6817\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"128.5001,-192.5903 125,-182.5904 121.5001,-192.5904 128.5001,-192.5903\" stroke=\"#000000\"/>\n</g>\n<!-- 139820497611632 -->\n<g class=\"node\" id=\"node10\">\n<title>139820497611632</title>\n<polygon fill=\"none\" points=\"151,-73.5 151,-109.5 253,-109.5 253,-73.5 151,-73.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"202\" y=\"-87.8\">lstm_4: LSTM</text>\n</g>\n<!-- 139820497611520&#45;&gt;139820497611632 -->\n<g class=\"edge\" id=\"edge9\">\n<title>139820497611520-&gt;139820497611632</title>\n<path d=\"M260.7191,-146.4551C251.0627,-137.4177 239.2176,-126.3319 228.7016,-116.4899\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"231.0223,-113.8681 221.3294,-109.5904 226.239,-118.979 231.0223,-113.8681\" stroke=\"#000000\"/>\n</g>\n<!-- 139820496893488&#45;&gt;139820497611632 -->\n<g class=\"edge\" id=\"edge10\">\n<title>139820496893488-&gt;139820497611632</title>\n<path d=\"M144.0337,-146.4551C153.5663,-137.4177 165.2595,-126.3319 175.6408,-116.4899\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"178.0693,-119.0104 182.9184,-109.5904 173.2533,-113.9305 178.0693,-119.0104\" stroke=\"#000000\"/>\n</g>\n<!-- 139820497536952 -->\n<g class=\"node\" id=\"node11\">\n<title>139820497536952</title>\n<polygon fill=\"none\" points=\"148.5,-.5 148.5,-36.5 255.5,-36.5 255.5,-.5 148.5,-.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"202\" y=\"-14.8\">dense_4: Dense</text>\n</g>\n<!-- 139820497611632&#45;&gt;139820497536952 -->\n<g class=\"edge\" id=\"edge12\">\n<title>139820497611632-&gt;139820497536952</title>\n<path d=\"M202,-73.4551C202,-65.3828 202,-55.6764 202,-46.6817\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"205.5001,-46.5903 202,-36.5904 198.5001,-46.5904 205.5001,-46.5903\" stroke=\"#000000\"/>\n</g>\n</g>\n</svg>"
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "metadata": {
        "id": "yw9D4HIvuu3n",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1056
        },
        "outputId": "b4e49374-857b-4c8a-be67-3c317b1c1852"
      },
      "cell_type": "code",
      "source": [
        "print(encoder_model.summary())\n",
        "plot_model(model, to_file='encoder_model_word_attention.png')\n",
        "SVG(model_to_dot(encoder_model).create(prog='dot', format='svg'))"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_3 (InputLayer)            (None, 41)           0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_4 (Embedding)         (None, 41, 50)       20000050    input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "permute_2 (Permute)             (None, 50, 41)       0           embedding_4[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dense_3 (Dense)                 (None, 50, 41)       1722        permute_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "attention_vec (Permute)         (None, 41, 50)       0           dense_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "multiply_2 (Multiply)           (None, 41, 50)       0           embedding_4[0][0]                \n",
            "                                                                 attention_vec[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "lstm_3 (LSTM)                   [(None, 50), (None,  20200       multiply_2[0][0]                 \n",
            "==================================================================================================\n",
            "Total params: 20,021,972\n",
            "Trainable params: 20,021,972\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.SVG object>"
            ],
            "image/svg+xml": "<svg height=\"483pt\" viewBox=\"0.00 0.00 219.50 483.00\" width=\"220pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g class=\"graph\" id=\"graph0\" transform=\"scale(1 1) rotate(0) translate(4 479)\">\n<title>G</title>\n<polygon fill=\"#ffffff\" points=\"-4,4 -4,-479 215.5,-479 215.5,4 -4,4\" stroke=\"transparent\"/>\n<!-- 139820497008624 -->\n<g class=\"node\" id=\"node1\">\n<title>139820497008624</title>\n<polygon fill=\"none\" points=\"59.5,-438.5 59.5,-474.5 192.5,-474.5 192.5,-438.5 59.5,-438.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"126\" y=\"-452.8\">input_3: InputLayer</text>\n</g>\n<!-- 139820497007728 -->\n<g class=\"node\" id=\"node2\">\n<title>139820497007728</title>\n<polygon fill=\"none\" points=\"40.5,-365.5 40.5,-401.5 211.5,-401.5 211.5,-365.5 40.5,-365.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"126\" y=\"-379.8\">embedding_4: Embedding</text>\n</g>\n<!-- 139820497008624&#45;&gt;139820497007728 -->\n<g class=\"edge\" id=\"edge1\">\n<title>139820497008624-&gt;139820497007728</title>\n<path d=\"M126,-438.4551C126,-430.3828 126,-420.6764 126,-411.6817\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"129.5001,-411.5903 126,-401.5904 122.5001,-411.5904 129.5001,-411.5903\" stroke=\"#000000\"/>\n</g>\n<!-- 139820497322784 -->\n<g class=\"node\" id=\"node3\">\n<title>139820497322784</title>\n<polygon fill=\"none\" points=\"12.5,-292.5 12.5,-328.5 145.5,-328.5 145.5,-292.5 12.5,-292.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"79\" y=\"-306.8\">permute_2: Permute</text>\n</g>\n<!-- 139820497007728&#45;&gt;139820497322784 -->\n<g class=\"edge\" id=\"edge2\">\n<title>139820497007728-&gt;139820497322784</title>\n<path d=\"M114.382,-365.4551C108.9024,-356.9441 102.2531,-346.6165 96.2045,-337.2219\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"99.0035,-335.1037 90.6472,-328.5904 93.1178,-338.8931 99.0035,-335.1037\" stroke=\"#000000\"/>\n</g>\n<!-- 139820496895168 -->\n<g class=\"node\" id=\"node6\">\n<title>139820496895168</title>\n<polygon fill=\"none\" points=\"56,-73.5 56,-109.5 194,-109.5 194,-73.5 56,-73.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"125\" y=\"-87.8\">multiply_2: Multiply</text>\n</g>\n<!-- 139820497007728&#45;&gt;139820496895168 -->\n<g class=\"edge\" id=\"edge5\">\n<title>139820497007728-&gt;139820496895168</title>\n<path d=\"M138.0779,-365.1731C144.0973,-354.9747 150.7677,-341.7883 154,-329 173.9334,-250.1359 178.341,-224.4949 157,-146 154.3787,-136.3584 149.5732,-126.6801 144.4691,-118.2112\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"147.2554,-116.073 138.9027,-109.5551 141.3677,-119.8592 147.2554,-116.073\" stroke=\"#000000\"/>\n</g>\n<!-- 139820497007000 -->\n<g class=\"node\" id=\"node4\">\n<title>139820497007000</title>\n<polygon fill=\"none\" points=\"22.5,-219.5 22.5,-255.5 129.5,-255.5 129.5,-219.5 22.5,-219.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"76\" y=\"-233.8\">dense_3: Dense</text>\n</g>\n<!-- 139820497322784&#45;&gt;139820497007000 -->\n<g class=\"edge\" id=\"edge3\">\n<title>139820497322784-&gt;139820497007000</title>\n<path d=\"M78.2584,-292.4551C77.9267,-284.3828 77.5278,-274.6764 77.1582,-265.6817\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"80.6512,-265.4382 76.7434,-255.5904 73.6571,-265.7257 80.6512,-265.4382\" stroke=\"#000000\"/>\n</g>\n<!-- 139820496392768 -->\n<g class=\"node\" id=\"node5\">\n<title>139820496392768</title>\n<polygon fill=\"none\" points=\"0,-146.5 0,-182.5 148,-182.5 148,-146.5 0,-146.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"74\" y=\"-160.8\">attention_vec: Permute</text>\n</g>\n<!-- 139820497007000&#45;&gt;139820496392768 -->\n<g class=\"edge\" id=\"edge4\">\n<title>139820497007000-&gt;139820496392768</title>\n<path d=\"M75.5056,-219.4551C75.2845,-211.3828 75.0185,-201.6764 74.7721,-192.6817\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"78.2683,-192.4907 74.4956,-182.5904 71.2709,-192.6825 78.2683,-192.4907\" stroke=\"#000000\"/>\n</g>\n<!-- 139820496392768&#45;&gt;139820496895168 -->\n<g class=\"edge\" id=\"edge6\">\n<title>139820496392768-&gt;139820496895168</title>\n<path d=\"M86.6067,-146.4551C92.614,-137.8564 99.9168,-127.4034 106.5341,-117.9316\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"109.5036,-119.7925 112.3615,-109.5904 103.7652,-115.7835 109.5036,-119.7925\" stroke=\"#000000\"/>\n</g>\n<!-- 139820496893488 -->\n<g class=\"node\" id=\"node7\">\n<title>139820496893488</title>\n<polygon fill=\"none\" points=\"74,-.5 74,-36.5 176,-36.5 176,-.5 74,-.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"125\" y=\"-14.8\">lstm_3: LSTM</text>\n</g>\n<!-- 139820496895168&#45;&gt;139820496893488 -->\n<g class=\"edge\" id=\"edge7\">\n<title>139820496895168-&gt;139820496893488</title>\n<path d=\"M125,-73.4551C125,-65.3828 125,-55.6764 125,-46.6817\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"128.5001,-46.5903 125,-36.5904 121.5001,-46.5904 128.5001,-46.5903\" stroke=\"#000000\"/>\n</g>\n</g>\n</svg>"
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "metadata": {
        "id": "kyuhPReSuu3t",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 747
        },
        "outputId": "aaea3680-b166-4c39-891e-056bd5339b12"
      },
      "cell_type": "code",
      "source": [
        "print(decoder_model.summary())\n",
        "plot_model(model, to_file='decoder_model_word_attention.png')\n",
        "SVG(model_to_dot(decoder_model).create(prog='dot', format='svg'))"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_4 (InputLayer)            (None, 13)           0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_5 (Embedding)         (None, 13, 50)       20000050    input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "Inference_decoder_input_hidden_ (None, 50)           0                                            \n",
            "__________________________________________________________________________________________________\n",
            "Inference_decoder_input_cell_st (None, 50)           0                                            \n",
            "__________________________________________________________________________________________________\n",
            "Inference_decoder_LSTM (LSTM)   [(None, 13, 50), (No 20200       embedding_5[0][0]                \n",
            "                                                                 Inference_decoder_input_hidden_st\n",
            "                                                                 Inference_decoder_input_cell_stat\n",
            "__________________________________________________________________________________________________\n",
            "decoder_Dense_Output (Dense)    (None, 13, 400002)   20400102    Inference_decoder_LSTM[0][0]     \n",
            "==================================================================================================\n",
            "Total params: 40,420,352\n",
            "Trainable params: 40,420,352\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.SVG object>"
            ],
            "image/svg+xml": "<svg height=\"264pt\" viewBox=\"0.00 0.00 815.50 264.00\" width=\"816pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g class=\"graph\" id=\"graph0\" transform=\"scale(1 1) rotate(0) translate(4 260)\">\n<title>G</title>\n<polygon fill=\"#ffffff\" points=\"-4,4 -4,-260 811.5,-260 811.5,4 -4,4\" stroke=\"transparent\"/>\n<!-- 139820497065184 -->\n<g class=\"node\" id=\"node1\">\n<title>139820497065184</title>\n<polygon fill=\"none\" points=\"19,-219.5 19,-255.5 152,-255.5 152,-219.5 19,-219.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"85.5\" y=\"-233.8\">input_4: InputLayer</text>\n</g>\n<!-- 139820497611520 -->\n<g class=\"node\" id=\"node2\">\n<title>139820497611520</title>\n<polygon fill=\"none\" points=\"0,-146.5 0,-182.5 171,-182.5 171,-146.5 0,-146.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"85.5\" y=\"-160.8\">embedding_5: Embedding</text>\n</g>\n<!-- 139820497065184&#45;&gt;139820497611520 -->\n<g class=\"edge\" id=\"edge1\">\n<title>139820497065184-&gt;139820497611520</title>\n<path d=\"M85.5,-219.4551C85.5,-211.3828 85.5,-201.6764 85.5,-192.6817\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"89.0001,-192.5903 85.5,-182.5904 82.0001,-192.5904 89.0001,-192.5903\" stroke=\"#000000\"/>\n</g>\n<!-- 139820496939888 -->\n<g class=\"node\" id=\"node5\">\n<title>139820496939888</title>\n<polygon fill=\"none\" points=\"237,-73.5 237,-109.5 452,-109.5 452,-73.5 237,-73.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"344.5\" y=\"-87.8\">Inference_decoder_LSTM: LSTM</text>\n</g>\n<!-- 139820497611520&#45;&gt;139820496939888 -->\n<g class=\"edge\" id=\"edge2\">\n<title>139820497611520-&gt;139820496939888</title>\n<path d=\"M149.5225,-146.4551C186.1005,-136.1454 232.1363,-123.1701 270.2894,-112.4165\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"271.6409,-115.672 280.3164,-109.5904 269.7419,-108.9345 271.6409,-115.672\" stroke=\"#000000\"/>\n</g>\n<!-- 139820496938432 -->\n<g class=\"node\" id=\"node3\">\n<title>139820496938432</title>\n<polygon fill=\"none\" points=\"189.5,-146.5 189.5,-182.5 499.5,-182.5 499.5,-146.5 189.5,-146.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"344.5\" y=\"-160.8\">Inference_decoder_input_hidden_state: InputLayer</text>\n</g>\n<!-- 139820496938432&#45;&gt;139820496939888 -->\n<g class=\"edge\" id=\"edge3\">\n<title>139820496938432-&gt;139820496939888</title>\n<path d=\"M344.5,-146.4551C344.5,-138.3828 344.5,-128.6764 344.5,-119.6817\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"348.0001,-119.5903 344.5,-109.5904 341.0001,-119.5904 348.0001,-119.5903\" stroke=\"#000000\"/>\n</g>\n<!-- 139820497524888 -->\n<g class=\"node\" id=\"node4\">\n<title>139820497524888</title>\n<polygon fill=\"none\" points=\"517.5,-146.5 517.5,-182.5 807.5,-182.5 807.5,-146.5 517.5,-146.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"662.5\" y=\"-160.8\">Inference_decoder_input_cell_state: InputLayer</text>\n</g>\n<!-- 139820497524888&#45;&gt;139820496939888 -->\n<g class=\"edge\" id=\"edge4\">\n<title>139820497524888-&gt;139820496939888</title>\n<path d=\"M583.8932,-146.4551C538.1325,-135.9502 480.3156,-122.6778 432.9656,-111.8081\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"433.5582,-108.3532 423.0286,-109.527 431.992,-115.1758 433.5582,-108.3532\" stroke=\"#000000\"/>\n</g>\n<!-- 139820496801520 -->\n<g class=\"node\" id=\"node6\">\n<title>139820496801520</title>\n<polygon fill=\"none\" points=\"247,-.5 247,-36.5 442,-36.5 442,-.5 247,-.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"344.5\" y=\"-14.8\">decoder_Dense_Output: Dense</text>\n</g>\n<!-- 139820496939888&#45;&gt;139820496801520 -->\n<g class=\"edge\" id=\"edge5\">\n<title>139820496939888-&gt;139820496801520</title>\n<path d=\"M344.5,-73.4551C344.5,-65.3828 344.5,-55.6764 344.5,-46.6817\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"348.0001,-46.5903 344.5,-36.5904 341.0001,-46.5904 348.0001,-46.5903\" stroke=\"#000000\"/>\n</g>\n</g>\n</svg>"
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "5e868e819218f82da6c032d9e22985d87dbf8f1c",
        "id": "DwCBgXA-uu3v",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "rmsprop = optimizers.RMSprop(lr=0.02)\n",
        "# Compile & run training\n",
        "model.compile(optimizer=rmsprop, loss='categorical_crossentropy', metrics = ['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "aabf7a6de1e2a003de21b7c076479c16e1c9c43c",
        "scrolled": true,
        "id": "fJUtQ6Vfuu3y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d1268ffb-93a0-4b86-d4b3-76c495448698"
      },
      "cell_type": "code",
      "source": [
        "history = model.fit([X1, Y1], decoder_target_data, batch_size = 6, epochs = 10, shuffle = True) "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "scrolled": true,
        "id": "RWFnNOlguu3z",
        "colab_type": "code",
        "colab": {},
        "outputId": "49df34a0-f820-4e97-9b23-cd364973919e"
      },
      "cell_type": "code",
      "source": [
        "history1 = model.fit([X1, Y1], decoder_target_data, batch_size = 6, epochs = 5, shuffle = True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "100/100 [==============================] - ETA: 2:10 - loss: 3.4168 - acc: 0.487 - ETA: 2:02 - loss: 3.5411 - acc: 0.467 - ETA: 1:53 - loss: 3.5712 - acc: 0.474 - ETA: 1:44 - loss: 3.6777 - acc: 0.458 - ETA: 1:36 - loss: 3.7065 - acc: 0.456 - ETA: 1:28 - loss: 3.7533 - acc: 0.450 - ETA: 1:20 - loss: 3.8413 - acc: 0.441 - ETA: 1:12 - loss: 3.8035 - acc: 0.447 - ETA: 1:04 - loss: 3.7928 - acc: 0.448 - ETA: 55s - loss: 3.7556 - acc: 0.451 - ETA: 47s - loss: 3.6839 - acc: 0.46 - ETA: 39s - loss: 3.6377 - acc: 0.47 - ETA: 30s - loss: 3.6064 - acc: 0.48 - ETA: 22s - loss: 3.6689 - acc: 0.47 - ETA: 13s - loss: 3.7224 - acc: 0.46 - ETA: 5s - loss: 3.7434 - acc: 0.4639 - 140s 1s/step - loss: 3.7408 - acc: 0.4646\n",
            "Epoch 2/5\n",
            "100/100 [==============================] - ETA: 2:10 - loss: 3.4598 - acc: 0.474 - ETA: 2:01 - loss: 3.5369 - acc: 0.461 - ETA: 1:53 - loss: 3.5034 - acc: 0.474 - ETA: 1:45 - loss: 3.5641 - acc: 0.464 - ETA: 1:36 - loss: 3.5255 - acc: 0.476 - ETA: 1:28 - loss: 3.4711 - acc: 0.489 - ETA: 1:19 - loss: 3.4153 - acc: 0.496 - ETA: 1:11 - loss: 3.5171 - acc: 0.482 - ETA: 1:03 - loss: 3.5301 - acc: 0.482 - ETA: 55s - loss: 3.5620 - acc: 0.479 - ETA: 46s - loss: 3.5669 - acc: 0.47 - ETA: 38s - loss: 3.5797 - acc: 0.47 - ETA: 30s - loss: 3.5984 - acc: 0.47 - ETA: 22s - loss: 3.5909 - acc: 0.46 - ETA: 13s - loss: 3.5883 - acc: 0.47 - ETA: 5s - loss: 3.6216 - acc: 0.4647 - 138s 1s/step - loss: 3.6165 - acc: 0.4646\n",
            "Epoch 3/5\n",
            "100/100 [==============================] - ETA: 2:04 - loss: 3.2554 - acc: 0.474 - ETA: 1:58 - loss: 3.2072 - acc: 0.487 - ETA: 1:51 - loss: 3.2536 - acc: 0.478 - ETA: 1:43 - loss: 3.1721 - acc: 0.487 - ETA: 1:35 - loss: 3.2303 - acc: 0.492 - ETA: 1:28 - loss: 3.2423 - acc: 0.489 - ETA: 1:19 - loss: 3.3339 - acc: 0.481 - ETA: 1:11 - loss: 3.3865 - acc: 0.477 - ETA: 1:03 - loss: 3.4035 - acc: 0.475 - ETA: 54s - loss: 3.4071 - acc: 0.478 - ETA: 46s - loss: 3.4019 - acc: 0.48 - ETA: 38s - loss: 3.4448 - acc: 0.47 - ETA: 30s - loss: 3.4644 - acc: 0.47 - ETA: 21s - loss: 3.4767 - acc: 0.47 - ETA: 13s - loss: 3.5070 - acc: 0.46 - ETA: 5s - loss: 3.5112 - acc: 0.4647 - 137s 1s/step - loss: 3.5169 - acc: 0.4646\n",
            "Epoch 4/5\n",
            "100/100 [==============================] - ETA: 2:11 - loss: 3.2928 - acc: 0.448 - ETA: 2:01 - loss: 3.5359 - acc: 0.423 - ETA: 1:53 - loss: 3.6215 - acc: 0.414 - ETA: 1:44 - loss: 3.5704 - acc: 0.426 - ETA: 1:36 - loss: 3.4577 - acc: 0.446 - ETA: 1:27 - loss: 3.3688 - acc: 0.461 - ETA: 1:18 - loss: 3.3533 - acc: 0.467 - ETA: 1:10 - loss: 3.4137 - acc: 0.459 - ETA: 1:03 - loss: 3.3887 - acc: 0.463 - ETA: 54s - loss: 3.4085 - acc: 0.465 - ETA: 46s - loss: 3.4317 - acc: 0.46 - ETA: 38s - loss: 3.4758 - acc: 0.45 - ETA: 30s - loss: 3.4643 - acc: 0.45 - ETA: 22s - loss: 3.4393 - acc: 0.46 - ETA: 13s - loss: 3.4585 - acc: 0.45 - ETA: 5s - loss: 3.4310 - acc: 0.4655 - 139s 1s/step - loss: 3.4322 - acc: 0.4646\n",
            "Epoch 5/5\n",
            "100/100 [==============================] - ETA: 2:09 - loss: 3.3168 - acc: 0.410 - ETA: 1:59 - loss: 3.4612 - acc: 0.429 - ETA: 1:51 - loss: 3.3822 - acc: 0.448 - ETA: 1:43 - loss: 3.3686 - acc: 0.448 - ETA: 1:35 - loss: 3.2933 - acc: 0.461 - ETA: 1:29 - loss: 3.2340 - acc: 0.476 - ETA: 1:20 - loss: 3.3435 - acc: 0.457 - ETA: 1:12 - loss: 3.3496 - acc: 0.461 - ETA: 1:03 - loss: 3.3868 - acc: 0.457 - ETA: 55s - loss: 3.3801 - acc: 0.460 - ETA: 47s - loss: 3.3256 - acc: 0.47 - ETA: 38s - loss: 3.3291 - acc: 0.46 - ETA: 30s - loss: 3.3470 - acc: 0.46 - ETA: 22s - loss: 3.3386 - acc: 0.46 - ETA: 13s - loss: 3.3716 - acc: 0.46 - ETA: 5s - loss: 3.3503 - acc: 0.4671 - 139s 1s/step - loss: 3.3693 - acc: 0.4646\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "zWw6JO6nuu31",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 735
        },
        "outputId": "97df2901-2334-476a-e590-ee78c248cabc"
      },
      "cell_type": "code",
      "source": [
        "# Plot training & validation accuracy values\n",
        "plt.plot(history.history['acc'])\n",
        "#plt.plot(history.history['val_acc'])\n",
        "plt.title('Model accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Test'], loc='upper left')\n",
        "plt.savefig('Model_Acc_Word_level_1-1_input_reversed')\n",
        "plt.show()\n",
        "\n",
        "# Plot training & validation loss values\n",
        "plt.plot(history.history['loss'])\n",
        "#plt.plot(history.history['val_loss'])\n",
        "plt.title('Model loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Test'], loc='upper left')\n",
        "plt.savefig('Model_Loss_Word_level_1-1_input_reversed')\n",
        "plt.show()"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAe8AAAFnCAYAAACPasF4AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmCC\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f7587f06ac8>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAecAAAFnCAYAAACcvYGMAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/wNmuUviYegQhAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f7587ec1a90>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "N4Wcud_Tuu35",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def decode_word_sequence(input_seq):\n",
        "    # Encode the input as state vectors.\n",
        "    states_value = encoder_model.predict(input_seq)\n",
        "\n",
        "    # Generate empty target sequence of length 1.\n",
        "    target_seq = np.zeros(( 1, 13))\n",
        "    # Populate the first character of target sequence with the start character.\n",
        "    #target_seq[0, 0] = word_to_index['\\t']\n",
        "    \n",
        "    # Sampling loop for a batch of sequences\n",
        "    # (to simplify, here we assume a batch of size 1).\n",
        "    stop_condition = False\n",
        "    decoded_sentence = ''\n",
        "    i = 0\n",
        "    while not stop_condition:\n",
        "        output_tokens, h, c = decoder_model.predict(\n",
        "            [target_seq] + states_value)\n",
        "\n",
        "        # Sample a token\n",
        "        sampled_token_index = np.argmax(output_tokens[0, 0, :])\n",
        "        sampled_word = index_to_word[sampled_token_index]\n",
        "        decoded_sentence += sampled_word + \" \"\n",
        "\n",
        "        # Exit condition: either hit max length\n",
        "        # or find stop character.\n",
        "        if (sampled_word == '\\n' or\n",
        "           len(decoded_sentence) > 41):\n",
        "            stop_condition = True\n",
        "\n",
        "        # Update the target sequence (of length 1).\n",
        "        target_seq = np.zeros((1, 13))\n",
        "        target_seq[0, i] = sampled_token_index\n",
        "        i += 1\n",
        "\n",
        "        # Update states\n",
        "        states_value = [h, c]\n",
        "\n",
        "    return decoded_sentence"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9SucUwNluu37",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 717
        },
        "outputId": "07fb892e-5ad4-45ed-a3bf-44786aa5af15"
      },
      "cell_type": "code",
      "source": [
        "for seq_index in range(10):\n",
        "\n",
        "    # Take one sequence (part of the training set) for trying out decoding.\n",
        "    input_seq = X1[seq_index: seq_index + 1]\n",
        "    print(input_seq.shape)\n",
        "    decoded_sentence = decode_word_sequence(input_seq)\n",
        "\n",
        "    print('-')\n",
        "    print('Input sentence:', X[seq_index])\n",
        "    print('Decoded sentence:', decoded_sentence)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1, 41)\n",
            "-\n",
            "Input sentence: australia 's current account deficit shrunk by a record # . ## billion dollars   # . ## billion us   in the june quarter due to soaring commodity prices , figures released monday showed . \n",
            "Decoded sentence: chaptal 248.1 incredible incredible incredible \n",
            "(1, 41)\n",
            "-\n",
            "Input sentence: at least two people were killed in a suspected bomb attack on a passenger bus in the strife - torn southern philippines on monday , the military said . \n",
            "Decoded sentence: 61.08 mdrv incredible incredible incredible \n",
            "(1, 41)\n",
            "-\n",
            "Input sentence: australian shares closed down # . # percent monday following a weak lead from the united states and lower commodity prices , dealers said . \n",
            "Decoded sentence: nexis x51mm incredible incredible incredible \n",
            "(1, 41)\n",
            "-\n",
            "Input sentence: south korea 's nuclear envoy kim sook urged north korea monday to restart work to disable its nuclear plants and stop its `` typical '' brinkmanship in negotiations . \n",
            "Decoded sentence: wisemen warangal incredible incredible incredible \n",
            "(1, 41)\n",
            "-\n",
            "Input sentence: south korea on monday announced sweeping tax reforms , including income and corporate tax cuts to boost growth by stimulating sluggish private consumption and business investment . \n",
            "Decoded sentence: shiozawa emotes incredible incredible incredible \n",
            "(1, 41)\n",
            "-\n",
            "Input sentence: taiwan share prices closed down # . ## percent monday on wall street weakness and lacklustre interim earnings from electronics manufacturing giant hon hai , dealers said . \n",
            "Decoded sentence: 13.36 apatosaurus incredible incredible incredible \n",
            "(1, 41)\n",
            "-\n",
            "Input sentence: australian shares closed down # . # percent monday following a weak lead from the united states and lower commodity prices , dealers said . \n",
            "Decoded sentence: nexis x51mm incredible incredible incredible \n",
            "(1, 41)\n",
            "-\n",
            "Input sentence: spanish property group colonial , struggling under huge debts , announced losses of # . ## billion euros for the first half of #### which it blamed on asset depreciation . \n",
            "Decoded sentence: daiyu disembarkation incredible incredible \n",
            "(1, 41)\n",
            "-\n",
            "Input sentence: libyan leader moamer kadhafi monday promised wide political and economic reforms that he said would see ministries dismantled and oil revenues going directly into the pockets of the people . \n",
            "Decoded sentence: vickerson katzl incredible incredible incredible \n",
            "(1, 41)\n",
            "-\n",
            "Input sentence: the united nations ' humanitarian chief john holmes arrived in ethiopia monday to tour regions affected by drought , which has left some eight million people in need of urgent food aid . \n",
            "Decoded sentence: daiyu disembarkation incredible incredible \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "c718df170fa508d6e5649cc4de3b7c728e5bad1e",
        "id": "RlZ9S7ajuu3-",
        "colab_type": "code",
        "colab": {},
        "outputId": "b5262081-a8a4-4725-8ba0-330ec2f8283e"
      },
      "cell_type": "code",
      "source": [
        "decoder_target_data[0]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       ...,\n",
              "       [1., 0., 0., ..., 0., 0., 0.],\n",
              "       [1., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 121
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "2895c7fbf13b49003db453dcea6d88df70ed5c90",
        "id": "2hiN0Uo7uu4A",
        "colab_type": "code",
        "colab": {},
        "outputId": "bf8921d6-097a-43f1-c1be-bff68433eac9"
      },
      "cell_type": "code",
      "source": [
        "loss, acc = model.evaluate([X1 , Y1], decoder_target_data[:,:,:])\n",
        "print(\"Accuracy\" + str(acc))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100/100 [==============================] - ETA: 2: - ETA: 50s - ETA: 4 - 117s 1s/step\n",
            "Accuracy0.46461538195610047\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "a73d4970b7c0b2a43da9887a763603e1d287aa0e",
        "id": "WoSdCYViuu4E",
        "colab_type": "code",
        "colab": {},
        "outputId": "4e1de3f5-d313-4598-f6dd-320977ebf5d4"
      },
      "cell_type": "code",
      "source": [
        "acc"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.46461538195610047"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 124
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "84e550cfe4bbb00ebe457f06e9475b4153b333da",
        "scrolled": true,
        "id": "BXXSWrbfuu4G",
        "colab_type": "code",
        "colab": {},
        "outputId": "a2cea8e0-d86e-4058-efb3-d58f6fed6352"
      },
      "cell_type": "code",
      "source": [
        "idx = model.predict([X1[:10,:], Y1[:10,:]], verbose = 0)\n",
        "print(idx.shape)\n",
        "print(idx)\n",
        "id1 = {}\n",
        "for i in range(idx.shape[0]):\n",
        "    id1[str(i)] = []\n",
        "    for j in range(idx.shape[1]):\n",
        "        try:\n",
        "            id1[str(i)].append(index_to_word[np.argmax(idx[i,j,:])])\n",
        "        except Exception as e:\n",
        "            id1[str(i)].append(index_to_word[np.argmax(idx[i,j,:]) + 1])\n",
        "            pass\n",
        "print(id1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(10, 13, 400002)\n",
            "[[[3.9995812e-02 1.9187546e-06 1.9409463e-06 ... 1.9284203e-06\n",
            "   1.9194865e-06 1.9712222e-06]\n",
            "  [3.9996184e-02 1.9187485e-06 1.9409422e-06 ... 1.9284159e-06\n",
            "   1.9194804e-06 1.9712161e-06]\n",
            "  [3.9996240e-02 1.9187494e-06 1.9409392e-06 ... 1.9284153e-06\n",
            "   1.9194797e-06 1.9712152e-06]\n",
            "  ...\n",
            "  [3.9996248e-02 1.9187478e-06 1.9409397e-06 ... 1.9284155e-06\n",
            "   1.9194799e-06 1.9712154e-06]\n",
            "  [3.9996248e-02 1.9187496e-06 1.9409397e-06 ... 1.9284155e-06\n",
            "   1.9194799e-06 1.9712154e-06]\n",
            "  [3.9996248e-02 1.9187496e-06 1.9409397e-06 ... 1.9284155e-06\n",
            "   1.9194817e-06 1.9712154e-06]]\n",
            "\n",
            " [[3.9995898e-02 1.9187530e-06 1.9409429e-06 ... 1.9284189e-06\n",
            "   1.9194813e-06 1.9712188e-06]\n",
            "  [3.9996196e-02 1.9187489e-06 1.9409408e-06 ... 1.9284166e-06\n",
            "   1.9194811e-06 1.9712165e-06]\n",
            "  [3.9996248e-02 1.9187478e-06 1.9409397e-06 ... 1.9284155e-06\n",
            "   1.9194799e-06 1.9712154e-06]\n",
            "  ...\n",
            "  [3.9996266e-02 1.9187489e-06 1.9409406e-06 ... 1.9284166e-06\n",
            "   1.9194811e-06 1.9712165e-06]\n",
            "  [3.9996266e-02 1.9187505e-06 1.9409406e-06 ... 1.9284164e-06\n",
            "   1.9194808e-06 1.9712163e-06]\n",
            "  [3.9996266e-02 1.9187505e-06 1.9409406e-06 ... 1.9284164e-06\n",
            "   1.9194808e-06 1.9712163e-06]]\n",
            "\n",
            " [[3.9994713e-02 1.9187621e-06 1.9409540e-06 ... 1.9284334e-06\n",
            "   1.9194940e-06 1.9712320e-06]\n",
            "  [3.9995976e-02 1.9187496e-06 1.9409413e-06 ... 1.9284189e-06\n",
            "   1.9194815e-06 1.9712172e-06]\n",
            "  [3.9996218e-02 1.9187501e-06 1.9409420e-06 ... 1.9284178e-06\n",
            "   1.9194820e-06 1.9712177e-06]\n",
            "  ...\n",
            "  [3.9996266e-02 1.9187489e-06 1.9409406e-06 ... 1.9284166e-06\n",
            "   1.9194811e-06 1.9712165e-06]\n",
            "  [3.9996266e-02 1.9187487e-06 1.9409406e-06 ... 1.9284164e-06\n",
            "   1.9194808e-06 1.9712163e-06]\n",
            "  [3.9996259e-02 1.9187503e-06 1.9409401e-06 ... 1.9284162e-06\n",
            "   1.9194806e-06 1.9712161e-06]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[3.9995257e-02 1.9187551e-06 1.9409490e-06 ... 1.9284248e-06\n",
            "   1.9194872e-06 1.9712229e-06]\n",
            "  [3.9996110e-02 1.9187485e-06 1.9409422e-06 ... 1.9284162e-06\n",
            "   1.9194806e-06 1.9712161e-06]\n",
            "  [3.9978333e-02 1.9188526e-06 1.9410436e-06 ... 1.9285023e-06\n",
            "   1.9195718e-06 1.9713043e-06]\n",
            "  ...\n",
            "  [3.9996147e-02 1.9187503e-06 1.9409385e-06 ... 1.9284180e-06\n",
            "   1.9194806e-06 1.9712163e-06]\n",
            "  [3.9996114e-02 1.9187487e-06 1.9409406e-06 ... 1.9284164e-06\n",
            "   1.9194811e-06 1.9712165e-06]\n",
            "  [3.9996099e-02 1.9187480e-06 1.9409397e-06 ... 1.9284155e-06\n",
            "   1.9194802e-06 1.9712156e-06]]\n",
            "\n",
            " [[3.9643925e-02 1.9195245e-06 1.9422253e-06 ... 1.9297847e-06\n",
            "   1.9206486e-06 1.9725983e-06]\n",
            "  [3.9996084e-02 1.9187491e-06 1.9409429e-06 ... 1.9284187e-06\n",
            "   1.9194813e-06 1.9712188e-06]\n",
            "  [3.9501268e-02 1.9200388e-06 1.9421213e-06 ... 1.9299173e-06\n",
            "   1.9210515e-06 1.9727411e-06]\n",
            "  ...\n",
            "  [3.9996248e-02 1.9187496e-06 1.9409397e-06 ... 1.9284155e-06\n",
            "   1.9194799e-06 1.9712154e-06]\n",
            "  [3.9996248e-02 1.9187496e-06 1.9409415e-06 ... 1.9284155e-06\n",
            "   1.9194817e-06 1.9712174e-06]\n",
            "  [3.9996210e-02 1.9187496e-06 1.9409415e-06 ... 1.9284173e-06\n",
            "   1.9194817e-06 1.9712174e-06]]\n",
            "\n",
            " [[3.9995976e-02 1.9187512e-06 1.9409431e-06 ... 1.9284189e-06\n",
            "   1.9194833e-06 1.9712172e-06]\n",
            "  [3.9996218e-02 1.9187501e-06 1.9409420e-06 ... 1.9284178e-06\n",
            "   1.9194820e-06 1.9712177e-06]\n",
            "  [3.9996266e-02 1.9187489e-06 1.9409406e-06 ... 1.9284166e-06\n",
            "   1.9194811e-06 1.9712165e-06]\n",
            "  ...\n",
            "  [3.9996248e-02 1.9187496e-06 1.9409397e-06 ... 1.9284173e-06\n",
            "   1.9194799e-06 1.9712154e-06]\n",
            "  [3.9996248e-02 1.9187514e-06 1.9409397e-06 ... 1.9284173e-06\n",
            "   1.9194799e-06 1.9712174e-06]\n",
            "  [3.9996222e-02 1.9187503e-06 1.9409404e-06 ... 1.9284180e-06\n",
            "   1.9194806e-06 1.9712181e-06]]]\n",
            "{'0': ['!', '!', '!', '!', '!', '!', '!', '!', '!', '!', '!', '!', '!'], '1': ['!', '!', '!', '!', '!', '!', '!', '!', '!', '!', '!', '!', '!'], '2': ['!', '!', '!', '!', '!', '!', '!', '!', '!', '!', '!', '!', '!'], '3': ['!', '!', '!', '!', '!', '!', '!', '!', '!', '!', '!', '!', '!'], '4': ['!', '!', '!', '!', '!', '!', '!', '!', '!', '!', '!', '!', '!'], '5': ['!', '!', '!', '!', '!', '!', '!', '!', '!', '!', '!', '!', '!'], '6': ['!', '!', '!', '!', '!', '!', '!', '!', '!', '!', '!', '!', '!'], '7': ['!', '!', '!', '!', '!', '!', '!', '!', '!', '!', '!', '!', '!'], '8': ['!', '!', '!', '!', '!', '!', '!', '!', '!', '!', '!', '!', '!'], '9': ['!', '!', '!', '!', '!', '!', '!', '!', '!', '!', '!', '!', '!']}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "qMmZgSVeuu4I",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mx3pGpo1uu4K",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "for seq_index in range(5):\n",
        "\n",
        "    # Take one sequence (part of the training set) for trying out decoding.\n",
        "    input_seq = encoder_input_data[seq_index: seq_index + 1]\n",
        "    decoded_sentence = decode_sequence(input_seq)\n",
        "\n",
        "    print('-')\n",
        "    print('Input sentence:', input_texts[seq_index])\n",
        "    print('Decoded sentence:', decoded_sentence)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "4c1aa4eaf7e7fc182a7a9cf72cf76712a81026fe",
        "id": "8D6zQ2Ckuu4L",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "idx = model.predict([X1, Y1], verbose = 0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "60360c9d31c4fc6d386d0890e09d58d1abcceb47",
        "id": "DrYIv7l0uu4M",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(idx.shape)\n",
        "id = []\n",
        "for i in range(idx.shape[0]):\n",
        "    id.append(index_to_word[np.argmax(idx[i,:])])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "937a3d33385299e14f9a5408ed8e5a1d4b6c5d8b",
        "id": "f1DQ-KHtuu4O",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "id"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "8c91b0b9469281cc380b296a7d8e9543182127af",
        "id": "iYL61dv-uu4Q",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}